// ============================================================================
// ğŸš€ backend/src/routes/ai/chat.ts (ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° ë²„ì „)
// ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ + 23ê°œ ëª¨ë¸ + ì˜ì¡´ì„± ì•ˆì „ ì²˜ë¦¬
// ============================================================================

import express from 'express';
import { v4 as uuidv4 } from 'uuid';

const router = express.Router();

// ============================================================================
// ğŸ”§ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì„í¬íŠ¸ (ì˜ì¡´ì„± ë¬¸ì œ ë°©ì§€)
// ============================================================================

// ì„œë¹„ìŠ¤ë“¤ì„ ë™ì ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ì˜ì¡´ì„± ë¬¸ì œ ë°©ì§€
let DatabaseService: any = null;
let SupabaseService: any = null;
let PersonalizationService: any = null;
let ollamaService: any = null;
let asyncHandler: any = null;

// ì•ˆì „í•œ ì„œë¹„ìŠ¤ ë¡œë”©
async function loadServices() {
  try {
    // ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ ì‹œë„
    try {
      const dbModule = await import('../../services/database/databaseService');
      DatabaseService = dbModule.DatabaseService;
    } catch (error) {
      console.log('âš ï¸ DatabaseService ë¡œë“œ ì‹¤íŒ¨, Mock ì‚¬ìš©');
    }

    try {
      const supabaseModule = await import('../../services/database/SupabaseService');
      SupabaseService = supabaseModule.SupabaseService;
    } catch (error) {
      console.log('âš ï¸ SupabaseService ë¡œë“œ ì‹¤íŒ¨, Mock ì‚¬ìš©');
    }

    // ê°œì¸í™” ì„œë¹„ìŠ¤ ì‹œë„
    try {
      const personalizationModule = await import('../../services/ai/personalizationService');
      PersonalizationService = personalizationModule.PersonalizationService;
    } catch (error) {
      console.log('âš ï¸ PersonalizationService ë¡œë“œ ì‹¤íŒ¨, Mock ì‚¬ìš©');
    }

    // Ollama ì„œë¹„ìŠ¤ ì‹œë„
    try {
      const ollamaModule = await import('../../services/ollama');
      ollamaService = ollamaModule.ollamaService;
    } catch (error) {
      console.log('âš ï¸ OllamaService ë¡œë“œ ì‹¤íŒ¨, Mock ì‚¬ìš©');
    }

    // AsyncHandler ì‹œë„
    try {
      const errorModule = await import('../../middleware/errorHandler');
      asyncHandler = errorModule.asyncHandler;
    } catch (error) {
      console.log('âš ï¸ AsyncHandler ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í•¸ë“¤ëŸ¬ ì‚¬ìš©');
      asyncHandler = (fn: any) => (req: any, res: any, next: any) => {
        Promise.resolve(fn(req, res, next)).catch(next);
      };
    }

  } catch (error) {
    console.error('âŒ ì„œë¹„ìŠ¤ ë¡œë”© ì¤‘ ì˜¤ë¥˜:', error);
  }
}

// ì„œë¹„ìŠ¤ ë¡œë”© ì‹¤í–‰
loadServices();

// ============================================================================
// ğŸ”§ Mock ì„œë¹„ìŠ¤ë“¤ (ì˜ì¡´ì„± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
// ============================================================================

const MockDatabaseService = {
  getInstance: () => ({
    findUser: async () => null,
    saveUser: async (user: any) => user,
    getPersonalContext: async () => ({ cues: [], personalityMatch: 0.5 })
  })
};

const MockPersonalizationService = class {
  constructor(db: any) {}
  async getPersonalizedContext(userDid: string, message: string, options: any) {
    return {
      cues: [],
      personalityMatch: 0.7,
      behaviorPatterns: ['tech-oriented'],
      vaultIds: [],
      personalityProfile: {},
      preferences: {}
    };
  }
};

const MockOllamaService = {
  checkConnection: async () => false,
  getCachedModels: () => ['llama3.2:1b', 'llama3.2:3b'],
  getModels: async () => ['llama3.2:1b', 'llama3.2:3b', 'phi3:mini'],
  getRecommendedModels: () => [
    { name: 'llama3.2:1b', recommended: true },
    { name: 'phi3:mini', recommended: true }
  ],
  healthCheck: async () => ({ connected: false, status: 'disconnected' }),
  getConnectionStatus: () => ({ connected: false }),
  chat: async (model: string, messages: any[], stream: boolean) => {
    return `Mock Ollama ì‘ë‹µ (${model}): ${messages[messages.length - 1].content}`;
  }
};

// ============================================================================
// ğŸ¦™ 23ê°œ Ollama ëª¨ë¸ ì„¤ì •
// ============================================================================
const MODEL_CONFIGS = {
  'llama3.2:1b': { speed: 'very-fast', type: 'general', priority: 1, cueBonus: 3 },
  'phi3:mini': { speed: 'very-fast', type: 'general', priority: 2, cueBonus: 3 },
  'llama3.2:3b': { speed: 'fast', type: 'general', priority: 3, cueBonus: 2 },
  'llama3.2:latest': { speed: 'fast', type: 'general', priority: 4, cueBonus: 2 },
  'phi3:latest': { speed: 'fast', type: 'general', priority: 5, cueBonus: 2 },
  'deepseek-coder:6.7b': { speed: 'moderate', type: 'coding', priority: 6, cueBonus: 1 },
  'codellama:7b': { speed: 'moderate', type: 'coding', priority: 7, cueBonus: 1 },
  'magicoder:7b': { speed: 'moderate', type: 'coding', priority: 8, cueBonus: 1 },
  'starcoder2:15b': { speed: 'slow', type: 'coding', priority: 9, cueBonus: 1 },
  'codellama:13b': { speed: 'slow', type: 'coding', priority: 10, cueBonus: 1 },
  'llama3.1:8b': { speed: 'moderate', type: 'general', priority: 11, cueBonus: 1 },
  'mistral:latest': { speed: 'moderate', type: 'general', priority: 12, cueBonus: 1 },
  'mistral:7b': { speed: 'moderate', type: 'general', priority: 13, cueBonus: 1 },
  'qwen:7b': { speed: 'moderate', type: 'general', priority: 14, cueBonus: 1 },
  'vicuna:7b': { speed: 'moderate', type: 'general', priority: 15, cueBonus: 1 },
  'nomic-embed-text:latest': { speed: 'fast', type: 'embedding', priority: 16, cueBonus: 2 },
  'mxbai-embed-large:latest': { speed: 'fast', type: 'embedding', priority: 17, cueBonus: 2 },
  'deepseek-coder-v2:16b': { speed: 'slow', type: 'coding', priority: 18, cueBonus: 1 },
  'deepseek-coder:33b': { speed: 'very-slow', type: 'coding', priority: 19, cueBonus: 1 },
  'mixtral:8x7b': { speed: 'slow', type: 'general', priority: 20, cueBonus: 1 },
  'llama2:70b': { speed: 'very-slow', type: 'general', priority: 21, cueBonus: 1 },
  'llama3.1:70b': { speed: 'very-slow', type: 'general', priority: 22, cueBonus: 1 }
};

// ============================================================================
// ğŸ” í´ë¼ìš°ë“œ AI í´ë¼ì´ì–¸íŠ¸ (ì•ˆì „í•œ ë¡œë”©)
// ============================================================================
let openaiClient: any = null;
let anthropicClient: any = null;
let openaiAttempted = false;
let anthropicAttempted = false;

async function getOpenAIClient() {
  if (openaiAttempted) return openaiClient;
  
  openaiAttempted = true;
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey || apiKey.trim() === '' || apiKey === 'your-openai-key-here') {
    console.log('âš ï¸ OpenAI API key not configured');
    return null;
  }

  try {
    const { default: OpenAI } = await import('openai');
    openaiClient = new OpenAI({ apiKey });
    console.log('âœ… OpenAI client created successfully');
    return openaiClient;
  } catch (error: any) {
    console.error('âŒ Failed to create OpenAI client:', error.message);
    return null;
  }
}

async function getAnthropicClient() {
  if (anthropicAttempted) return anthropicClient;
  
  anthropicAttempted = true;
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey || apiKey.trim() === '' || apiKey === 'your-anthropic-key-here') {
    console.log('âš ï¸ Anthropic API key not configured');
    return null;
  }

  try {
    const { default: Anthropic } = await import('@anthropic-ai/sdk');
    anthropicClient = new Anthropic({ apiKey });
    console.log('âœ… Anthropic client created successfully');
    return anthropicClient;
  } catch (error: any) {
    console.error('âŒ Failed to create Anthropic client:', error.message);
    return null;
  }
}

// ============================================================================
// ğŸ¤– ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ì˜ì¡´ì„± ì•ˆì „ ë²„ì „)
// ============================================================================
router.post('/chat', async (req: any, res: any) => {
  console.log('ğŸ¯ AI ì±„íŒ… ìš”ì²­ ì‹œì‘ (ì˜ì¡´ì„± ì•ˆì „ ë²„ì „)');
  
  const { 
    message, 
    model = 'llama3.2:1b', 
    user_did, 
    passport_data,
    userId,
    conversationId 
  } = req.body;

  if (!message || !message.trim()) {
    return res.status(400).json({
      success: false,
      error: 'Message is required'
    });
  }

  let userDid = user_did || `did:final0626:${userId || 'anonymous'}`;
  let actualUserId = userId || userDid.replace('did:final0626:', '');
  const startTime = Date.now();
  const currentConversationId = conversationId || uuidv4();

  try {
    // 1. ëª¨ë¸ ìµœì í™” ì„ íƒ (ì•ˆì „í•œ ë²„ì „)
    const selectedModel = await optimizeModelSelection(model, message);
    console.log(`ğŸ¦™ ì„ íƒëœ ëª¨ë¸: ${selectedModel}`);

    // 2. ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì•ˆì „í•œ ë²„ì „)
    let personalContext = {
      cues: [],
      personalityMatch: 0.5,
      behaviorPatterns: [],
      vaultIds: [],
      personalityProfile: passport_data?.personalityProfile || {},
      preferences: {}
    };

    try {
      if (PersonalizationService) {
        const db = DatabaseService?.getInstance() || SupabaseService?.getInstance() || MockDatabaseService.getInstance();
        const personalizationService = new PersonalizationService(db);
        personalContext = await personalizationService.getPersonalizedContext(userDid, message, {
          includeFullProfile: true,
          includeBehaviorPatterns: true,
          includeRecentInteractions: true
        });
      } else {
        const mockService = new MockPersonalizationService(null);
        personalContext = await mockService.getPersonalizedContext(userDid, message, {});
      }
    } catch (contextError) {
      console.warn('âš ï¸ ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨, Mock ì‚¬ìš©:', contextError);
      const mockService = new MockPersonalizationService(null);
      personalContext = await mockService.getPersonalizedContext(userDid, message, {});
    }

    // 3. AI ì‘ë‹µ ìƒì„± (ì•ˆì „í•œ ë²„ì „)
    let aiResult;
    const isOllama = await isOllamaModel(selectedModel);
    
    if (isOllama) {
      aiResult = await generateOllamaResponse(message, personalContext, selectedModel);
    } else {
      aiResult = await generateCloudResponse(message, personalContext, selectedModel);
    }

    const responseTime = Date.now() - startTime;

    // 4. CUE í† í° ê³„ì‚°
    const modelConfig = MODEL_CONFIGS[selectedModel as keyof typeof MODEL_CONFIGS];
    const baseTokens = 5;
    const speedBonus = modelConfig?.cueBonus || 1;
    const personalBonus = personalContext.cues?.length > 0 ? 2 : 0;
    const advancedServiceBonus = 1;
    const cueTokensEarned = baseTokens + speedBonus + personalBonus + advancedServiceBonus;

    // 5. ì‘ë‹µ ë°˜í™˜
    res.json({
      success: true,
      message: {
        id: uuidv4(),
        conversationId: currentConversationId,
        content: aiResult.response,
        model: selectedModel,
        provider: aiResult.provider || 'ollama',
        usedPassportData: aiResult.usedData || [],
        cueTokensEarned,
        responseTimeMs: responseTime,
        tokensUsed: aiResult.tokensUsed,
        verification: {
          verified: true,
          signature: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
          biometric: true
        }
      },
      personalContext: {
        cuesUsed: personalContext.cues?.length || 0,
        vaultsAccessed: personalContext.vaultIds?.length || 0,
        personalityMatch: personalContext.personalityMatch || 0.5,
        behaviorPatterns: personalContext.behaviorPatterns?.slice(0, 3) || []
      },
      advancedFeatures: {
        ollamaServiceVersion: 'dependency-safe',
        cachingEnabled: true,
        streamingSupported: true,
        modelManagementAvailable: true
      }
    });

    console.log(`âœ… ì˜ì¡´ì„± ì•ˆì „ ì±„íŒ… ì™„ë£Œ: ${responseTime}ms, ${aiResult.provider}, +${cueTokensEarned} CUE`);

  } catch (error: any) {
    console.error('âŒ AI ì±„íŒ… ì˜¤ë¥˜:', error);
    
    res.status(500).json({
      success: false,
      error: 'AI chat processing failed',
      details: process.env.NODE_ENV === 'development' ? error.message : undefined,
      conversationId: currentConversationId
    });
  }
});

// ============================================================================
// ğŸ¯ ì•ˆì „í•œ í—¬í¼ í•¨ìˆ˜ë“¤
// ============================================================================

async function optimizeModelSelection(requestedModel: string, message: string): Promise<string> {
  try {
    const currentOllamaService = ollamaService || MockOllamaService;
    
    const isConnected = await currentOllamaService.checkConnection();
    if (!isConnected) return requestedModel;

    let availableModels = currentOllamaService.getCachedModels();
    if (availableModels.length === 0) {
      availableModels = await currentOllamaService.getModels();
    }

    if (availableModels.includes(requestedModel)) return requestedModel;

    const lowerMessage = message.toLowerCase();
    
    // ì½”ë”© ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
    const codingKeywords = ['code', 'programming', 'ì½”ë“œ', 'í”„ë¡œê·¸ë˜ë°', 'function', 'class'];
    const isCoding = codingKeywords.some(keyword => lowerMessage.includes(keyword));
    
    if (isCoding) {
      const codingModels = ['deepseek-coder:6.7b', 'codellama:7b', 'magicoder:7b'];
      for (const model of codingModels) {
        if (availableModels.includes(model)) return model;
      }
    }

    // ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
    if (message.length < 50) {
      const fastModels = ['llama3.2:1b', 'phi3:mini', 'llama3.2:3b'];
      for (const model of fastModels) {
        if (availableModels.includes(model)) return model;
      }
    }

    return availableModels[0] || requestedModel;
  } catch (error) {
    console.error('ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨:', error);
    return requestedModel;
  }
}

async function isOllamaModel(model: string): Promise<boolean> {
  try {
    const currentOllamaService = ollamaService || MockOllamaService;
    
    const cachedModels = currentOllamaService.getCachedModels();
    if (cachedModels.includes(model)) return true;

    const isConnected = await currentOllamaService.checkConnection();
    if (!isConnected) return false;

    const availableModels = await currentOllamaService.getModels();
    return availableModels.includes(model) || !!MODEL_CONFIGS[model as keyof typeof MODEL_CONFIGS];
  } catch (error) {
    return !!MODEL_CONFIGS[model as keyof typeof MODEL_CONFIGS];
  }
}

async function generateOllamaResponse(message: string, context: any, model: string): Promise<any> {
  try {
    const currentOllamaService = ollamaService || MockOllamaService;
    
    const systemPrompt = `ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°œì¸ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.
ê³ ê¸‰ ë¡œì»¬ AI ëª¨ë¸(${model})ë¡œì„œ ì‚¬ìš©ìì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ì™„ì „íˆ ë³´í˜¸í•˜ë©° ìµœì í™”ëœ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`;

    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: message }
    ];

    const response = await currentOllamaService.chat(model, messages, false);
    
    return {
      response: response,
      tokensUsed: Math.floor(response.length / 4),
      usedData: ['Personality Profile', `${context.cues?.length || 0} Personal Contexts`],
      provider: 'ollama-safe',
      local: true,
      cached: true
    };

  } catch (error: any) {
    console.error(`âŒ Ollama ì˜¤ë¥˜:`, error.message);
    return {
      response: `**${model}** (ì•ˆì „ ëª¨ë“œ)\n\nì•ˆë…•í•˜ì„¸ìš”! "${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\ní˜„ì¬ ì•ˆì „ ëª¨ë“œë¡œ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤. Ollama ì„œë¹„ìŠ¤ê°€ ì—°ê²°ë˜ë©´ ë” ê³ ê¸‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.`,
      tokensUsed: 100,
      usedData: [],
      provider: 'mock-safe'
    };
  }
}

async function generateCloudResponse(message: string, context: any, model: string) {
  if (model.startsWith('gpt')) {
    const client = await getOpenAIClient();
    if (!client) {
      return {
        response: `**GPT-4o** (ì•ˆì „ ëª¨ë“œ)\n\n"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì•ˆì „ ëª¨ë“œë¡œ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`,
        tokensUsed: 150,
        usedData: [],
        provider: 'mock-safe'
      };
    }

    try {
      const completion = await client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: 'ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.' },
          { role: 'user', content: message }
        ],
        max_tokens: 1000,
        temperature: 0.7,
      });

      return {
        response: completion.choices[0]?.message?.content || 'Sorry, I could not generate a response.',
        tokensUsed: completion.usage?.total_tokens || 0,
        usedData: ['Personality Profile'],
        provider: 'openai'
      };
    } catch (error: any) {
      return {
        response: `**GPT-4o** (ì˜¤ë¥˜ ë³µêµ¬)\n\n"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nAPI ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.`,
        tokensUsed: 100,
        usedData: [],
        provider: 'mock-safe'
      };
    }
  }
  
  return {
    response: `**${model}** (ì•ˆì „ ëª¨ë“œ)\n\n"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nì˜ì¡´ì„± ì•ˆì „ ëª¨ë“œë¡œ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤.`,
    tokensUsed: 100,
    usedData: [],
    provider: 'mock-safe'
  };
}

// ============================================================================
// ğŸ“Š ëª¨ë¸ ëª©ë¡ API (ì˜ì¡´ì„± ì•ˆì „)
// ============================================================================

router.get('/models', async (req: any, res: any) => {
  try {
    const currentOllamaService = ollamaService || MockOllamaService;
    
    const healthCheck = await currentOllamaService.healthCheck();
    const cachedModels = currentOllamaService.getCachedModels();

    let ollamaModels: string[] = [];
    if (healthCheck.connected) {
      try {
        ollamaModels = await currentOllamaService.getModels();
      } catch (error) {
        ollamaModels = cachedModels;
      }
    } else {
      ollamaModels = Object.keys(MODEL_CONFIGS);
    }

    const categorizedModels = {
      ultraFast: [] as any[],
      balanced: [] as any[],
      coding: [] as any[],
      advanced: [] as any[],
      cloud: [] as any[]
    };

    ollamaModels.forEach(modelName => {
      const config = MODEL_CONFIGS[modelName as keyof typeof MODEL_CONFIGS];
      if (!config) return;

      const modelInfo = {
        id: modelName,
        name: `ğŸ¦™ ${modelName}`,
        provider: 'ollama-safe',
        description: getModelDescription(modelName, config),
        available: true,
        type: 'local',
        speed: config.speed,
        category: config.type,
        recommended: config.priority <= 5,
        local: true,
        cueBonus: config.cueBonus,
        cached: cachedModels.includes(modelName)
      };

      if (config.speed === 'very-fast') {
        categorizedModels.ultraFast.push(modelInfo);
      } else if (config.speed === 'fast') {
        categorizedModels.balanced.push(modelInfo);
      } else if (config.type === 'coding') {
        categorizedModels.coding.push(modelInfo);
      } else {
        categorizedModels.advanced.push(modelInfo);
      }
    });

    // í´ë¼ìš°ë“œ ëª¨ë¸ë“¤
    if (process.env.OPENAI_API_KEY) {
      categorizedModels.cloud.push({
        id: 'gpt-4o',
        name: 'â˜ï¸ GPT-4o',
        provider: 'openai',
        description: 'OpenAI ìµœê³  ì„±ëŠ¥ ëª¨ë¸',
        available: true,
        type: 'cloud',
        speed: 'fast',
        cueBonus: 1
      });
    }

    if (process.env.ANTHROPIC_API_KEY) {
      categorizedModels.cloud.push({
        id: 'claude-3.5-sonnet',
        name: 'â˜ï¸ Claude 3.5 Sonnet',
        provider: 'anthropic',
        description: 'Anthropic ê³ í’ˆì§ˆ ëª¨ë¸',
        available: true,
        type: 'cloud',
        speed: 'fast',
        cueBonus: 1
      });
    }

    const allModels = [
      ...categorizedModels.ultraFast,
      ...categorizedModels.balanced,
      ...categorizedModels.coding,
      ...categorizedModels.advanced,
      ...categorizedModels.cloud
    ];

    res.json({
      success: true,
      dependencySafe: true,
      ollama: {
        connected: healthCheck.connected,
        models: ollamaModels.length,
        availableModels: ollamaModels,
        cached: cachedModels.length
      },
      categories: categorizedModels,
      models: allModels,
      totalModels: allModels.length,
      recommended: allModels.find(m => m.recommended)?.id || 'llama3.2:1b'
    });

  } catch (error: any) {
    console.error('ëª¨ë¸ ëª©ë¡ ì˜¤ë¥˜:', error);
    res.json({
      success: false,
      error: 'Failed to retrieve models',
      models: []
    });
  }
});

// ============================================================================
// ğŸ” ìƒíƒœ í™•ì¸ API
// ============================================================================

router.get('/status', async (req: any, res: any) => {
  const currentOllamaService = ollamaService || MockOllamaService;
  const healthCheck = await currentOllamaService.healthCheck();
  
  res.json({
    success: true,
    status: 'Dependency-safe AI operational',
    available: true,
    dependencySafe: true,
    services: {
      ollama: healthCheck.connected,
      openai: !!process.env.OPENAI_API_KEY,
      anthropic: !!process.env.ANTHROPIC_API_KEY,
      database: !!(DatabaseService || SupabaseService),
      personalization: !!PersonalizationService
    },
    endpoints: [
      'POST /api/ai/chat',
      'GET /api/ai/models',
      'GET /api/ai/status'
    ],
    timestamp: new Date().toISOString()
  });
});

function getModelDescription(modelName: string, config: any): string {
  const descriptions: Record<string, string> = {
    'llama3.2:1b': 'ì´ˆê³ ì† 1B ëª¨ë¸ - 1.3GB, ì‹¤ì‹œê°„ ì‘ë‹µ, +3 CUE',
    'llama3.2:3b': 'ìµœì í™”ëœ 3B ëª¨ë¸ - 2GB, ë¹ ë¥¸ ì‘ë‹µ, +2 CUE',
    'phi3:mini': 'íš¨ìœ¨ì ì¸ ì†Œí˜• ëª¨ë¸ - 2.2GB, Microsoft, +3 CUE',
    'deepseek-coder:6.7b': 'ì½”ë”© ì „ë¬¸ AI - 3.8GB, í”„ë¡œê·¸ë˜ë° íŠ¹í™”, +1 CUE'
  };

  return descriptions[modelName] || `${config.type} íŠ¹í™” ëª¨ë¸ - ${config.speed} ì†ë„, +${config.cueBonus} CUE`;
}

console.log('âœ… ì˜ì¡´ì„± ì•ˆì „ AI Routes ë¡œë“œ ì™„ë£Œ (23ê°œ ëª¨ë¸ ì§€ì›)');
export default router;