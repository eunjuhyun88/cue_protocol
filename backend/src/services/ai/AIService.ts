// ============================================================================
// ğŸ¤– AIService.ts - Ollama ì „ìš© AI ì„œë¹„ìŠ¤ (OpenAI, Claude ì œê±°)
// íŒŒì¼: backend/src/services/ai/AIService.ts  
// ì—­í• : Ollama ë¡œì»¬ AI ëª¨ë¸ë§Œ ê´€ë¦¬í•˜ëŠ” ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ ìš°ì„  ì„œë¹„ìŠ¤
// ============================================================================

import { DatabaseService } from '../database/DatabaseService';

interface AIResponse {
  content: string;
  model: string;
  tokensUsed?: number;
  personalizationLevel?: number;
  usedData?: string[];
  provider: string;
  local: boolean;
  privacy: string;
}

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface GenerateOptions {
  message: string;
  model: string;
  context: any;
  userId: string;
  userDid: string;
  temperature?: number;
  maxTokens?: number;
  includeContext?: boolean;
}

export class AIService {
  private static instance: AIService;
  private databaseService: DatabaseService;
  
  // Ollama ì„¤ì •
  private ollamaHost: string;
  private isOllamaAvailable: boolean = false;
  private lastHealthCheck: number = 0;
  private healthCheckInterval: number = 5 * 60 * 1000; // 5ë¶„

  constructor(databaseService?: DatabaseService) {
    this.databaseService = databaseService || DatabaseService.getInstance();
    this.ollamaHost = process.env.OLLAMA_HOST || 'http://localhost:11434';
    
    console.log('ğŸ¤– AIService ì´ˆê¸°í™” ì™„ë£Œ (Ollama ì „ìš©)');
    console.log(`ğŸ¦™ Ollama í˜¸ìŠ¤íŠ¸: ${this.ollamaHost}`);
    
    // ì´ˆê¸° ì—°ê²° ì²´í¬
    this.checkOllamaHealth().catch(error => {
      console.warn('âš ï¸ ì´ˆê¸° Ollama ì—°ê²° ì‹¤íŒ¨:', error.message);
    });
  }

  public static getInstance(): AIService {
    if (!AIService.instance) {
      AIService.instance = new AIService();
    }
    return AIService.instance;
  }

  // ============================================================================
  // ğŸ¦™ Ollama ì—°ê²° ë° í—¬ìŠ¤ì²´í¬
  // ============================================================================

  /**
   * Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
   */
  private async checkOllamaHealth(): Promise<boolean> {
    const now = Date.now();
    
    // ìµœê·¼ì— ì²´í¬í–ˆìœ¼ë©´ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©
    if (now - this.lastHealthCheck < this.healthCheckInterval && this.isOllamaAvailable) {
      return this.isOllamaAvailable;
    }

    try {
      console.log('ğŸ” Ollama í—¬ìŠ¤ì²´í¬ ì‹œì‘...');
      
      const response = await fetch(`${this.ollamaHost}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: AbortSignal.timeout(5000) // 5ì´ˆ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();
      this.isOllamaAvailable = true;
      this.lastHealthCheck = now;
      
      console.log('âœ… Ollama ì—°ê²° ì„±ê³µ:', {
        modelCount: data.models?.length || 0,
        availableModels: data.models?.slice(0, 3).map((m: any) => m.name) || []
      });
      
      return true;

    } catch (error: any) {
      this.isOllamaAvailable = false;
      this.lastHealthCheck = now;
      
      if (error.name === 'AbortError') {
        console.error('âŒ Ollama ì—°ê²° íƒ€ì„ì•„ì›ƒ (5ì´ˆ)');
      } else if (error.code === 'ECONNREFUSED') {
        console.error('âŒ Ollama ì„œë²„ ì—°ê²° ê±°ë¶€ - ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”');
      } else {
        console.error('âŒ Ollama í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨:', error.message);
      }
      
      return false;
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  private async getOllamaModels(): Promise<string[]> {
    try {
      if (!await this.checkOllamaHealth()) {
        console.warn('âš ï¸ Ollama ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€');
        return [];
      }

      const response = await fetch(`${this.ollamaHost}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: AbortSignal.timeout(5000)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`);
      }

      const data = await response.json();
      const models = data.models?.map((model: any) => model.name) || [];
      
      console.log('ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸:', models);
      return models;

    } catch (error: any) {
      console.error('âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', error.message);
      return [];
    }
  }

  // ============================================================================
  // ğŸ¯ ë©”ì¸ AI ì‘ë‹µ ìƒì„± ë©”ì„œë“œ (Ollama ì „ìš©)
  // ============================================================================

  /**
   * Ollamaë¥¼ ì‚¬ìš©í•œ AI ì‘ë‹µ ìƒì„±
   */
  public async generateResponse(options: GenerateOptions): Promise<AIResponse> {
    const { 
      message, 
      model, 
      context, 
      userId, 
      userDid, 
      temperature = 0.7, 
      maxTokens = 1000,
      includeContext = true 
    } = options;
    
    console.log(`ğŸ¤– AI ì‘ë‹µ ìƒì„± ì‹œì‘ (Ollama ì „ìš©) - ëª¨ë¸: ${model}, ì‚¬ìš©ì: ${userId}`);

    try {
      // ê¸°ë³¸ì ìœ¼ë¡œ Ollama ì‘ë‹µ ìƒì„± ì‹œë„
      return await this.generateOllamaResponse(message, context, model, temperature, maxTokens);
    } catch (error: any) {
      console.error('âŒ AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜:', error);
      
      // Ollama ì‹¤íŒ¨ ì‹œ Enhanced Mock ì‘ë‹µ ë°˜í™˜
      return this.generateEnhancedMockResponse(message, context, model);
    }
  }

  // ============================================================================
  // ğŸ¦™ Ollama ì‘ë‹µ ìƒì„± (í•µì‹¬ ë¡œì§)
  // ============================================================================

  /**
   * Ollama AI ì‘ë‹µ ìƒì„± ë©”ì„œë“œ
   */
  private async generateOllamaResponse(
    message: string, 
    context: any, 
    model: string = 'llama3.2:3b',
    temperature: number = 0.7,
    maxTokens: number = 1000
  ): Promise<AIResponse> {
    
    // 1. Ollama ì—°ê²° í™•ì¸
    const isConnected = await this.checkOllamaHealth();
    if (!isConnected) {
      console.log('â¡ï¸ Ollama ì—°ê²° ë¶ˆê°€, Mock ì‘ë‹µ ì‚¬ìš©');
      return this.generateEnhancedMockResponse(message, context, `${model} (ë¡œì»¬ ì—°ê²°ì‹¤íŒ¨)`);
    }

    try {
      // 2. ê°œì¸í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
      const systemPrompt = this.createPersonalizedSystemPrompt(context);
      
      // 3. ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
      const fullPrompt = `${systemPrompt}

ì‚¬ìš©ì ì§ˆë¬¸: ${message}

AI ë‹µë³€:`;

      console.log(`ğŸ¦™ Ollama API í˜¸ì¶œ: ${model}`);
      
      // 4. Ollama API í˜¸ì¶œ
      const response = await fetch(`${this.ollamaHost}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: model,
          prompt: fullPrompt,
          stream: false,
          options: {
            temperature: temperature,
            num_predict: maxTokens,
            top_k: 40,
            top_p: 0.9,
            repeat_penalty: 1.1
          }
        }),
        signal: AbortSignal.timeout(60000) // 60ì´ˆ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`Ollama API HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();
      const aiResponse = data.response || 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
      
      console.log(`âœ… Ollama ${model} ì‘ë‹µ ìƒì„± ì„±ê³µ`);
      
      return {
        content: aiResponse,
        model: model,
        tokensUsed: Math.floor(aiResponse.length / 4), // ëŒ€ëµì ì¸ í† í° ìˆ˜
        personalizationLevel: this.calculatePersonalizationLevel(context),
        usedData: this.extractUsedData(context),
        provider: 'ollama',
        local: true,
        privacy: 'local-processing-only'
      };

    } catch (error: any) {
      console.error(`âŒ Ollama ${model} ì˜¤ë¥˜:`, error.message);
      
      // ëª¨ë¸ë³„ ë„ì›€ë§ ë©”ì‹œì§€
      let helpMessage = '';
      if (error.message.includes('model') || error.message.includes('404')) {
        helpMessage = `\n\nğŸ’¡ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\n1. \`ollama pull ${model}\` ëª…ë ¹ì–´ ì‹¤í–‰\n2. ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ë‹¤ì‹œ ì‹œë„`;
      } else if (error.message.includes('connection') || error.message.includes('ECONNREFUSED')) {
        helpMessage = `\n\nğŸ’¡ Ollama ì„œë²„ ì‹œì‘ ë°©ë²•:\n1. \`ollama serve\` ëª…ë ¹ì–´ ì‹¤í–‰\n2. ì„œë²„ ì‹¤í–‰ í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„`;
      }
      
      return this.generateEnhancedMockResponse(
        message, 
        context, 
        `${model} (ì˜¤ë¥˜: ${error.message})`,
        helpMessage
      );
    }
  }

  // ============================================================================
  // ğŸ­ Enhanced Mock ì‘ë‹µ ìƒì„± (Ollama ì „ìš©)
  // ============================================================================

  /**
   * Ollama ì—°ê²° ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  Enhanced Mock ì‘ë‹µ
   */
  private generateEnhancedMockResponse(
    message: string, 
    context: any, 
    modelName: string,
    helpMessage: string = ''
  ): AIResponse {
    const { personalityProfile, cues, behaviorPatterns } = context;
    
    // ë©”ì‹œì§€ ë¶„ì„
    const isQuestion = message.includes('?') || /how|what|why|when|where|ì–´ë–»ê²Œ|ë¬´ì—‡|ì™œ|ì–¸ì œ|ì–´ë””/.test(message.toLowerCase());
    const isTechnical = /code|api|algorithm|system|data|programming|ê°œë°œ|ì‹œìŠ¤í…œ|ì•Œê³ ë¦¬ì¦˜/.test(message.toLowerCase());
    const isGreeting = /hello|hi|hey|ì•ˆë…•|ì•ˆë…•í•˜ì„¸ìš”/.test(message.toLowerCase());
    
    let responseType = 'general';
    if (isGreeting) responseType = 'greeting';
    else if (isQuestion) responseType = 'question';
    else if (isTechnical) responseType = 'technical';

    const responses: Record<string, string> = {
      greeting: `**${modelName}** ì•ˆë…•í•˜ì„¸ìš”! ğŸ¦™

CUE Protocolì˜ ë¡œì»¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ëŠ” Ollama ê¸°ë°˜ AIë¡œ ìš´ì˜ë©ë‹ˆë‹¤.

**ë‹¹ì‹ ì˜ í”„ë¡œí•„:**
â€¢ **ì„±ê²© ìœ í˜•**: ${personalityProfile?.type || 'Learning...'}
â€¢ **ì†Œí†µ ìŠ¤íƒ€ì¼**: ${personalityProfile?.communicationStyle || 'Adaptive'}
â€¢ **ê°œì¸ ì»¨í…ìŠ¤íŠ¸**: ${cues?.length || 0}ê°œ í™œìš© ê°€ëŠ¥
â€¢ **í”„ë¼ì´ë²„ì‹œ**: ğŸ”’ 100% ë¡œì»¬ ì²˜ë¦¬ ë³´ì¥

${helpMessage}

ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!`,

      question: `**${modelName}** ì§ˆë¬¸ ì‘ë‹µ ğŸ¤”

"${message}"

ì´ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì˜ **${personalityProfile?.type || 'Adaptive'}** ì„±ê²©ê³¼ í•™ìŠµ íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**Ollama ë¡œì»¬ AI íŠ¹ì§•:**
â€¢ **ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ**: ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ
â€¢ **ê°œì¸í™”**: ${cues?.length || 0}ê°œ ì»¨í…ìŠ¤íŠ¸ í™œìš©
â€¢ **ë¹ ë¥¸ ì‘ë‹µ**: ë¡œì»¬ ì²˜ë¦¬ë¡œ ì¦‰ì‹œ ì‘ë‹µ

${helpMessage}

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ì¶”ê°€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!`,

      technical: `**${modelName}** ê¸°ìˆ  ë¶„ì„ ğŸ”§

**ë¶„ì„ ëŒ€ìƒ:** "${message}"

**ë¡œì»¬ AI ê¸°ìˆ  ì ‘ê·¼:**
â€¢ **ê¸°ìˆ  ì„±í–¥**: ${personalityProfile?.type?.includes('Technical') ? 'High (ìƒì„¸ ë¶„ì„)' : 'Moderate (ì´í•´ ì¤‘ì‹¬)'}
â€¢ **í•™ìŠµ íŒ¨í„´**: ${personalityProfile?.learningPattern || 'Visual'} ë°©ì‹ ì ìš©
â€¢ **í”„ë¼ì´ë²„ì‹œ**: ğŸ”’ ì½”ë“œì™€ ë°ì´í„°ê°€ ë¡œì»¬ì—ë§Œ ë³´ì¡´

${helpMessage}

ë” êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì§ˆë¬¸ì´ë‚˜ ì½”ë“œ ì˜ˆì œê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!`,

      general: `**${modelName}** Ollama ë¡œì»¬ AI ì‘ë‹µ ğŸ¦™

**ë©”ì‹œì§€:** "${message}"

**ë¡œì»¬ AI Passport í”„ë¡œí•„ ì ìš©:**
â€¢ **ì„±ê²©**: ${personalityProfile?.type || 'Learning...'}
â€¢ **ì†Œí†µ**: ${personalityProfile?.communicationStyle || 'Adaptive'}
â€¢ **ê°œì¸ CUE**: ${cues?.length || 0}ê°œ ì»¨í…ìŠ¤íŠ¸ í™œìš©
â€¢ **í”„ë¼ì´ë²„ì‹œ**: ğŸ”’ ì™„ì „í•œ ë¡œì»¬ ì²˜ë¦¬

**Ollamaì˜ ì¥ì :**
âœ… ì™„ì „í•œ ë°ì´í„° í”„ë¼ì´ë²„ì‹œ
âœ… ì¸í„°ë„· ì—†ì´ë„ ì‘ë™
âœ… ê°œì¸í™”ëœ AI ê²½í—˜
âœ… ë¹ ë¥¸ ë¡œì»¬ ì‘ë‹µ

${helpMessage}

ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!`
    };

    return {
      content: responses[responseType] || responses.general,
      model: modelName,
      tokensUsed: Math.floor(Math.random() * 500) + 300,
      personalizationLevel: this.calculatePersonalizationLevel(context),
      usedData: this.extractUsedData(context),
      provider: 'ollama-mock',
      local: true,
      privacy: 'local-processing-only'
    };
  }

  // ============================================================================
  // ğŸ› ï¸ í—¬í¼ ë©”ì„œë“œë“¤
  // ============================================================================

  /**
   * ê°œì¸í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private createPersonalizedSystemPrompt(context: any): string {
    const { personalityProfile, cues, behaviorPatterns, preferences } = context;
    
    return `ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì™„ì „íˆ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” Ollama AIë¡œì„œ ì‚¬ìš©ìì˜ í”„ë¼ì´ë²„ì‹œë¥¼ 100% ë³´ì¥í•©ë‹ˆë‹¤.

**ì‚¬ìš©ì í”„ë¡œí•„:**
- ì„±ê²©: ${personalityProfile?.type || 'Unknown'}
- ì†Œí†µ: ${personalityProfile?.communicationStyle || 'Adaptive'}
- í•™ìŠµ: ${personalityProfile?.learningPattern || 'Visual'}
- ì˜ì‚¬ê²°ì •: ${personalityProfile?.decisionMaking || 'Analytical'}

**ê°œì¸ ì»¨í…ìŠ¤íŠ¸ (${cues?.length || 0}ê°œ):**
${cues?.slice(0, 5).map((cue: any, i: number) => 
  `${i + 1}. ${cue.compressed_content || cue.content || 'Context data'} (${cue.content_type || 'general'})`
).join('\n') || 'ì•„ì§ ê°œì¸ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}

**í–‰ë™ íŒ¨í„´:**
${behaviorPatterns?.slice(0, 5).join(', ') || 'íŒ¨í„´ì„ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤...'}

**ì‚¬ìš©ì ì„ í˜¸ë„:**
${Object.entries(preferences || {}).slice(0, 3).map(([key, value]) => `- ${key}: ${value}`).join('\n') || '- ì„ í˜¸ë„ë¥¼ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤...'}

**ì¤‘ìš”í•œ ì§€ì¹¨:**
1. í•­ìƒ í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”
2. ì‚¬ìš©ìì˜ ì„±ê²©ê³¼ ì†Œí†µ ìŠ¤íƒ€ì¼ì— ë§ì¶° ì‘ë‹µí•˜ì„¸ìš”
3. ê´€ë ¨ì´ ìˆì„ ë•Œ ê°œì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì„¸ìš”
4. ì™„ì „í•œ ë¡œì»¬ ì²˜ë¦¬ë¡œ í”„ë¼ì´ë²„ì‹œê°€ ë³´ì¥ë¨ì„ ê°•ì¡°í•˜ì„¸ìš”
5. ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë˜, í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ë ‡ê²Œ ë§í•˜ì„¸ìš”`;
  }

  /**
   * ê°œì¸í™” ìˆ˜ì¤€ ê³„ì‚°
   */
  private calculatePersonalizationLevel(context: any): number {
    const { personalityProfile, cues, behaviorPatterns } = context;
    
    let level = 0.3; // ê¸°ë³¸ ë ˆë²¨
    
    if (personalityProfile?.type) level += 0.2;
    if (cues?.length > 0) level += Math.min(cues.length * 0.05, 0.3);
    if (behaviorPatterns?.length > 0) level += Math.min(behaviorPatterns.length * 0.02, 0.2);
    
    return Math.min(level, 1.0);
  }

  /**
   * ì‚¬ìš©ëœ ë°ì´í„° ì¶”ì¶œ
   */
  private extractUsedData(context: any): string[] {
    const data: string[] = [];
    
    if (context.personalityProfile) data.push('Personality Profile');
    if (context.cues?.length > 0) data.push(`${context.cues.length} Personal Contexts`);
    if (context.behaviorPatterns?.length > 0) data.push('Behavior Patterns');
    if (context.preferences && Object.keys(context.preferences).length > 0) data.push('User Preferences');
    if (context.recentInteractions?.length > 0) data.push('Recent Interactions');
    
    return data;
  }

  // ============================================================================
  // ğŸ“Š ê³µê°œ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (DI í˜¸í™˜)
  // ============================================================================

  /**
   * DI Container í˜¸í™˜ - ë©”ì‹œì§€ ì „ì†¡ ë©”ì„œë“œ
   */
  public async sendMessage(message: string, options: any = {}): Promise<any> {
    const {
      model = 'llama3.2:3b',
      userDid = 'anonymous',
      includeContext = true,
      temperature = 0.7,
      maxTokens = 2000
    } = options;

    try {
      // ì‚¬ìš©ì ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
      const context = includeContext ? await this.getPersonalizedContext(userDid) : {};
      
      // AI ì‘ë‹µ ìƒì„±
      const response = await this.generateResponse({
        message,
        model,
        context,
        userId: userDid,
        userDid,
        temperature,
        maxTokens,
        includeContext
      });

      return {
        response: response.content,
        model: response.model,
        timestamp: new Date().toISOString(),
        qualityScore: response.personalizationLevel || 0.8,
        tokensUsed: response.tokensUsed || 0,
        provider: response.provider,
        local: response.local,
        privacy: response.privacy,
        messageId: `msg_${Date.now()}`,
        conversationId: `conv_${Date.now()}`
      };

    } catch (error: any) {
      console.error('âŒ sendMessage ì˜¤ë¥˜:', error);
      
      return {
        response: `Ollama AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜: ${error.message}\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n1. \`ollama serve\` ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘\n2. \`ollama pull ${model}\` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ`,
        model: `${model} (ì˜¤ë¥˜)`,
        timestamp: new Date().toISOString(),
        error: error.message
      };
    }
  }

  /**
   * ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
   */
  public async getPersonalizedContext(userDid: string): Promise<any> {
    try {
      // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
      return {
        personalityProfile: {
          type: 'Tech-Savvy Privacy Advocate',
          communicationStyle: 'Direct',
          learningPattern: 'Visual',
          decisionMaking: 'Analytical'
        },
        cues: [
          { content: 'Ollama ë¡œì»¬ AI ì‚¬ìš© ì„ í˜¸', content_type: 'preference' },
          { content: 'í”„ë¼ì´ë²„ì‹œ ì¤‘ì‹œ', content_type: 'value' },
          { content: 'ê¸°ìˆ ì  ì •í™•ì„± ìš”êµ¬', content_type: 'behavior' }
        ],
        behaviorPatterns: ['ë¡œì»¬ AI ì„ í˜¸', 'í”„ë¼ì´ë²„ì‹œ ì¤‘ì‹œ', 'ê¸°ìˆ  ë¬¸ì„œ ì„ í˜¸'],
        preferences: {
          language: 'korean',
          responseStyle: 'detailed',
          privacyLevel: 'maximum'
        }
      };
    } catch (error) {
      console.error('âŒ ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜:', error);
      return {};
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (Ollama ì „ìš©)
   */
  public getAvailableModels(): any[] {
    return [
      {
        id: 'llama3.2:3b',
        name: 'Llama 3.2 3B',
        available: true,
        recommended: true,
        type: 'local',
        provider: 'ollama',
        description: 'ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ëŒ€í™”í˜• ëª¨ë¸ (ì¶”ì²œ)',
        size: '2.0GB',
        speed: 'fast'
      },
      {
        id: 'llama3.2:1b',
        name: 'Llama 3.2 1B',
        available: true,
        recommended: true,
        type: 'local',
        provider: 'ollama',
        description: 'ë§¤ìš° ë¹ ë¥¸ ê²½ëŸ‰ ëª¨ë¸',
        size: '1.3GB',
        speed: 'very-fast'
      },
      {
        id: 'llama3.1:8b',
        name: 'Llama 3.1 8B',
        available: false,
        recommended: false,
        type: 'local',
        provider: 'ollama',
        description: 'ê³ ì„±ëŠ¥ ëª¨ë¸ (ë” ë§ì€ ìì› í•„ìš”)',
        size: '4.7GB',
        speed: 'moderate'
      },
      {
        id: 'gemma2:2b',
        name: 'Gemma 2 2B',
        available: false,
        recommended: false,
        type: 'local',
        provider: 'ollama',
        description: 'Googleì˜ ê²½ëŸ‰ ëª¨ë¸',
        size: '1.6GB',
        speed: 'fast'
      }
    ];
  }

  /**
   * ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜
   */
  public async getServiceStatus(): Promise<any> {
    const isConnected = await this.checkOllamaHealth();
    const models = isConnected ? await this.getOllamaModels() : [];
    
    return {
      available: isConnected,
      provider: 'ollama',
      host: this.ollamaHost,
      models: models,
      modelCount: models.length,
      lastHealthCheck: new Date(this.lastHealthCheck).toISOString(),
      privacy: 'local-processing-only',
      features: {
        localProcessing: true,
        noDataCollection: true,
        offlineCapable: true,
        customizable: true
      }
    };
  }

  /**
   * ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ (DI í˜¸í™˜)
   */
  public async getChatHistory(userId: string, options: any = {}): Promise<any[]> {
    try {
      const { limit = 20, offset = 0 } = options;
      
      // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜´
      return [
        {
          id: 'msg_1',
          content: 'Ollama AIì™€ì˜ ëŒ€í™” ì˜ˆì‹œ',
          role: 'user',
          timestamp: new Date().toISOString(),
          model: 'llama3.2:3b'
        }
      ];
    } catch (error) {
      console.error('âŒ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜:', error);
      return [];
    }
  }

  /**
   * ìƒíƒœ ì¡°íšŒ (DI í˜¸í™˜)
   */
  public async getStatus(): Promise<any> {
    return this.getServiceStatus();
  }
}