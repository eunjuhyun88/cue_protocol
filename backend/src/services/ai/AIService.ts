// ============================================================================
// ğŸ¤– AI ì„œë¹„ìŠ¤ - í†µí•© AI ëª¨ë¸ ê´€ë¦¬
// íŒŒì¼: backend/src/services/ai/AIService.ts
// ì—­í• : OpenAI, Claude, Ollama ë“± ë‹¤ì–‘í•œ AI ëª¨ë¸ í†µí•© ê´€ë¦¬
// ============================================================================

import { DatabaseService } from '../database/DatabaseService';

interface AIResponse {
  content: string;
  model: string;
  tokensUsed?: number;
  personalizationLevel?: number;
  usedData?: string[];
  provider?: string;
  local?: boolean;
}

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface GenerateOptions {
  message: string;
  model: string;
  context: any;
  userId: string;
  userDid: string;
  temperature?: number;
  maxTokens?: number;
}

export class AIService {
  private static instance: AIService;
  private databaseService: DatabaseService;
  
  // AI í´ë¼ì´ì–¸íŠ¸ë“¤ (ì§€ì—° ë¡œë”©)
  private openaiClient: any = null;
  private anthropicClient: any = null;
  private openaiAttempted = false;
  private anthropicAttempted = false;

  private constructor() {
    this.databaseService = DatabaseService.getInstance();
    console.log('ğŸ¤– AIService ì´ˆê¸°í™” ì™„ë£Œ');
  }

  public static getInstance(): AIService {
    if (!AIService.instance) {
      AIService.instance = new AIService();
    }
    return AIService.instance;
  }

  // ============================================================================
  // ğŸ” OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì§€ì—° ë¡œë”©)
  // ============================================================================
  
  private async getOpenAIClient() {
    if (this.openaiAttempted) {
      return this.openaiClient;
    }

    this.openaiAttempted = true;

    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey || apiKey.trim() === '' || apiKey === 'your-openai-key-here') {
      console.log('âš ï¸ OpenAI API key not configured');
      return null;
    }

    try {
      console.log('ğŸ”„ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘...');
      const { default: OpenAI } = await import('openai');
      
      this.openaiClient = new OpenAI({ apiKey });
      console.log('âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ');
      return this.openaiClient;
    } catch (error: any) {
      console.error('âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨:', error.message);
      this.openaiClient = null;
      return null;
    }
  }

  // ============================================================================
  // ğŸ” Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì§€ì—° ë¡œë”©)
  // ============================================================================
  
  private async getAnthropicClient() {
    if (this.anthropicAttempted) {
      return this.anthropicClient;
    }

    this.anthropicAttempted = true;

    const apiKey = process.env.ANTHROPIC_API_KEY;
    if (!apiKey || apiKey.trim() === '' || apiKey === 'your-anthropic-key-here') {
      console.log('âš ï¸ Anthropic API key not configured');
      return null;
    }

    try {
      console.log('ğŸ”„ Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘...');
      const { default: Anthropic } = await import('@anthropic-ai/sdk');
      
      this.anthropicClient = new Anthropic({ apiKey });
      console.log('âœ… Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ');
      return this.anthropicClient;
    } catch (error: any) {
      console.error('âŒ Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨:', error.message);
      this.anthropicClient = null;
      return null;
    }
  }

  // ============================================================================
  // ğŸ¯ ë©”ì¸ AI ì‘ë‹µ ìƒì„± ë©”ì„œë“œ
  // ============================================================================
  
  public async generateResponse(options: GenerateOptions): Promise<AIResponse> {
    const { message, model, context, userId, userDid, temperature = 0.7, maxTokens = 1000 } = options;
    
    console.log(`ğŸ¤– AI ì‘ë‹µ ìƒì„± ì‹œì‘ - ëª¨ë¸: ${model}, ì‚¬ìš©ì: ${userId}`);

    try {
      // ëª¨ë¸ì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
      switch (model) {
        case 'gpt-4':
        case 'gpt-4o':
        case 'gpt-4o-mini':
          return await this.generateGPTResponse(message, context, model, temperature, maxTokens);
          
        case 'claude-3.5-sonnet':
        case 'claude-sonnet':
        case 'claude-3-sonnet':
          return await this.generateClaudeResponse(message, context, model, maxTokens);
          
        case 'llama3.2:3b':
        case 'llama3.2:1b':
        case 'llama3.1:8b':
        case 'gemma2:2b':
        case 'qwen2.5:3b':
          return await this.generateOllamaResponse(message, context, model);
          
        case 'personalized-agent':
        default:
          return await this.generatePersonalizedResponse(message, context, userDid);
      }
    } catch (error: any) {
      console.error('âŒ AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜:', error);
      
      // ì—ëŸ¬ ë°œìƒ ì‹œ Enhanced Mock ì‘ë‹µ ë°˜í™˜
      return this.generateEnhancedMockResponse(message, context, model);
    }
  }

  // ============================================================================
  // ğŸ”µ GPT ì‘ë‹µ ìƒì„±
  // ============================================================================
  
  private async generateGPTResponse(
    message: string, 
    context: any, 
    model: string = 'gpt-4o-mini',
    temperature: number = 0.7,
    maxTokens: number = 1000
  ): Promise<AIResponse> {
    const client = await this.getOpenAIClient();
    if (!client) {
      return this.generateEnhancedMockResponse(message, context, `${model} (API ë¯¸ì‚¬ìš©)`);
    }

    try {
      const systemPrompt = this.createPersonalizedSystemPrompt(context);
      
      const completion = await client.chat.completions.create({
        model: model === 'gpt-4' ? 'gpt-4o-mini' : model, // ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš©
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: message }
        ],
        max_tokens: maxTokens,
        temperature: temperature,
      });

      const responseContent = completion.choices[0]?.message?.content || 
                             'Sorry, I could not generate a response.';

      console.log('âœ… GPT ì‘ë‹µ ìƒì„± ì„±ê³µ');
      
      return {
        content: responseContent,
        model: model,
        tokensUsed: completion.usage?.total_tokens || 0,
        personalizationLevel: this.calculatePersonalizationLevel(context),
        usedData: this.extractUsedData(context),
        provider: 'openai'
      };
    } catch (error: any) {
      console.error('âŒ GPT API ì˜¤ë¥˜:', error.message);
      return this.generateEnhancedMockResponse(message, context, `${model} (API ì˜¤ë¥˜)`);
    }
  }

  // ============================================================================
  // ğŸŸ  Claude ì‘ë‹µ ìƒì„±
  // ============================================================================
  
  private async generateClaudeResponse(
    message: string, 
    context: any, 
    model: string = 'claude-3-5-sonnet-20241022',
    maxTokens: number = 1000
  ): Promise<AIResponse> {
    const client = await this.getAnthropicClient();
    if (!client) {
      return this.generateEnhancedMockResponse(message, context, `${model} (API ë¯¸ì‚¬ìš©)`);
    }

    try {
      const systemPrompt = this.createPersonalizedSystemPrompt(context);
      
      const response = await client.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: maxTokens,
        system: systemPrompt,
        messages: [{ role: 'user', content: message }]
      });

      const content = response.content[0];
      const responseText = content.type === 'text' ? content.text : 
                          'Sorry, I could not generate a response.';

      console.log('âœ… Claude ì‘ë‹µ ìƒì„± ì„±ê³µ');
      
      return {
        content: responseText,
        model: model,
        tokensUsed: (response.usage?.input_tokens || 0) + (response.usage?.output_tokens || 0),
        personalizationLevel: this.calculatePersonalizationLevel(context),
        usedData: this.extractUsedData(context),
        provider: 'anthropic'
      };
    } catch (error: any) {
      console.error('âŒ Claude API ì˜¤ë¥˜:', error.message);
      return this.generateEnhancedMockResponse(message, context, `${model} (API ì˜¤ë¥˜)`);
    }
  }

  // ============================================================================
  // ğŸ¦™ Ollama ì‘ë‹µ ìƒì„±
  // ============================================================================
  
  private async generateOllamaResponse(
    message: string, 
    context: any, 
    model: string = 'llama3.2:3b'
  ): Promise<AIResponse> {
    try {
      // Ollama ì„œë¹„ìŠ¤ ë™ì  ì„í¬íŠ¸
      const { ollamaService } = await import('../ollama');
      
      const isConnected = await ollamaService.checkConnection();
      if (!isConnected) {
        console.log('â¡ï¸ Ollama ì—°ê²° ë¶ˆê°€, Mock ì‘ë‹µ ì‚¬ìš©');
        return this.generateEnhancedMockResponse(message, context, `${model} (ë¡œì»¬)`);
      }

      const systemPrompt = this.createPersonalizedSystemPrompt(context);
      
      const messages = [
        {
          role: 'system' as const,
          content: `${systemPrompt}

ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ê°œì¸ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë¡œì»¬ AI ëª¨ë¸ë¡œì„œ ì‚¬ìš©ìì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ì™„ì „íˆ ë³´í˜¸í•˜ë©° ë¹ ë¥¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`
        },
        {
          role: 'user' as const,
          content: message
        }
      ];

      const response = await ollamaService.chat(model, messages, false);
      
      console.log(`âœ… Ollama ${model} ì‘ë‹µ ìƒì„± ì„±ê³µ`);
      
      return {
        content: response,
        model: model,
        tokensUsed: Math.floor(response.length / 4), // ëŒ€ëµì ì¸ í† í° ìˆ˜
        personalizationLevel: this.calculatePersonalizationLevel(context),
        usedData: this.extractUsedData(context),
        provider: 'ollama',
        local: true
      };

    } catch (error: any) {
      console.error(`âŒ Ollama ${model} ì˜¤ë¥˜:`, error.message);
      return this.generateEnhancedMockResponse(message, context, `${model} (ë¡œì»¬ ì˜¤ë¥˜)`);
    }
  }

  // ============================================================================
  // ğŸ§  ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ)
  // ============================================================================
  
  private async generatePersonalizedResponse(
    message: string, 
    context: any, 
    userDid: string
  ): Promise<AIResponse> {
    console.log('ğŸ§  ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„± ì¤‘...');
    
    const personalityType = context.personalityProfile?.type || '';
    
    // ì„±ê²© íƒ€ì…ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ
    if (personalityType.includes('Technical') || personalityType.includes('INTJ')) {
      // ê¸°ìˆ ì /ë¶„ì„ì  ì„±í–¥ â†’ Claude ì„ í˜¸
      return await this.generateClaudeResponse(message, context);
    } else if (personalityType.includes('Creative') || personalityType.includes('ENFP')) {
      // ì°½ì˜ì  ì„±í–¥ â†’ GPT ì„ í˜¸
      return await this.generateGPTResponse(message, context);
    } else {
      // ê¸°ë³¸ì ìœ¼ë¡œ ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš©
      return await this.generateGPTResponse(message, context);
    }
  }

  // ============================================================================
  // ğŸ­ Enhanced Mock ì‘ë‹µ ìƒì„±
  // ============================================================================
  
  private generateEnhancedMockResponse(
    message: string, 
    context: any, 
    modelName: string
  ): AIResponse {
    const { personalityProfile, cues, behaviorPatterns } = context;
    
    // ë©”ì‹œì§€ ë¶„ì„
    const isQuestion = message.includes('?') || /how|what|why|when|where|ì–´ë–»ê²Œ|ë¬´ì—‡|ì™œ|ì–¸ì œ|ì–´ë””/.test(message.toLowerCase());
    const isTechnical = /code|api|algorithm|system|data|programming|ê°œë°œ|ì‹œìŠ¤í…œ|ì•Œê³ ë¦¬ì¦˜/.test(message.toLowerCase());
    const isGreeting = /hello|hi|hey|ì•ˆë…•|ì•ˆë…•í•˜ì„¸ìš”/.test(message.toLowerCase());
    
    let responseType = 'general';
    if (isGreeting) responseType = 'greeting';
    else if (isQuestion) responseType = 'question';
    else if (isTechnical) responseType = 'technical';

    const responses: Record<string, string> = {
      greeting: `**${modelName}** ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹

ë°˜ê°‘ìŠµë‹ˆë‹¤! AI Passport ì‹œìŠ¤í…œì˜ ê°œì¸í™”ëœ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ë‹¹ì‹ ì˜ í”„ë¡œí•„:**
â€¢ **ì„±ê²© ìœ í˜•**: ${personalityProfile?.type || 'Learning...'}
â€¢ **ì†Œí†µ ìŠ¤íƒ€ì¼**: ${personalityProfile?.communicationStyle || 'Adaptive'}
â€¢ **ê°œì¸ ì»¨í…ìŠ¤íŠ¸**: ${cues?.length || 0}ê°œ í™œìš© ê°€ëŠ¥

ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!`,

      question: `**${modelName}** ì§ˆë¬¸ ì‘ë‹µ ğŸ¤”

"${message}"

ì´ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì˜ **${personalityProfile?.type || 'Adaptive'}** ì„±ê²©ê³¼ í•™ìŠµ íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ê°œì¸í™” ì ìš©:**
â€¢ **í•™ìŠµ ë°©ì‹**: ${personalityProfile?.learningPattern || 'Visual'} 
â€¢ **ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸**: ${cues?.length || 0}ê°œ í™œìš©

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ì¶”ê°€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!`,

      technical: `**${modelName}** ê¸°ìˆ  ë¶„ì„ ğŸ”§

**ë¶„ì„ ëŒ€ìƒ:** "${message}"

**ê°œì¸í™”ëœ ê¸°ìˆ  ì ‘ê·¼:**
â€¢ **ê¸°ìˆ  ì„±í–¥**: ${personalityProfile?.type?.includes('Technical') ? 'High (ìƒì„¸ ë¶„ì„)' : 'Moderate (ì´í•´ ì¤‘ì‹¬)'}
â€¢ **í•™ìŠµ íŒ¨í„´**: ${personalityProfile?.learningPattern || 'Visual'} ë°©ì‹ ì ìš©

ë” êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì§ˆë¬¸ì´ë‚˜ ì½”ë“œ ì˜ˆì œê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!`,

      general: `**${modelName}** ê°œì¸í™” ì‘ë‹µ ğŸ’«

**ë©”ì‹œì§€:** "${message}"

**ë‹¹ì‹ ì˜ AI Passport í”„ë¡œí•„ ì ìš©:**
â€¢ **ì„±ê²©**: ${personalityProfile?.type || 'Learning...'}
â€¢ **ì†Œí†µ**: ${personalityProfile?.communicationStyle || 'Adaptive'}
â€¢ **ê°œì¸ CUE**: ${cues?.length || 0}ê°œ ì»¨í…ìŠ¤íŠ¸ í™œìš©

ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!`
    };

    return {
      content: responses[responseType] || responses.general,
      model: modelName,
      tokensUsed: Math.floor(Math.random() * 500) + 300,
      personalizationLevel: this.calculatePersonalizationLevel(context),
      usedData: this.extractUsedData(context),
      provider: 'mock'
    };
  }

  // ============================================================================
  // ğŸ› ï¸ í—¬í¼ ë©”ì„œë“œë“¤
  // ============================================================================
  
  private createPersonalizedSystemPrompt(context: any): string {
    const { personalityProfile, cues, behaviorPatterns, preferences } = context;
    
    return `You are an AI assistant with deep knowledge of the user's personality and preferences.

**User Profile:**
- Personality: ${personalityProfile?.type || 'Unknown'}
- Communication: ${personalityProfile?.communicationStyle || 'Adaptive'}
- Learning: ${personalityProfile?.learningPattern || 'Visual'}
- Decision Making: ${personalityProfile?.decisionMaking || 'Analytical'}

**Personal Context (${cues?.length || 0} items):**
${cues?.slice(0, 5).map((cue: any, i: number) => 
  `${i + 1}. ${cue.compressed_content || cue.content || 'Context data'} (${cue.content_type || 'general'})`
).join('\n') || 'No personal context available yet.'}

**Behavioral Patterns:**
${behaviorPatterns?.slice(0, 5).join(', ') || 'Learning patterns...'}

**User Preferences:**
${Object.entries(preferences || {}).slice(0, 3).map(([key, value]) => `- ${key}: ${value}`).join('\n') || '- Preferences learning...'}

Respond in a way that matches their personality and communication style. Use their personal context when relevant. Be helpful, accurate, and personalized. Respond in Korean unless specified otherwise.`;
  }

  private calculatePersonalizationLevel(context: any): number {
    const { personalityProfile, cues, behaviorPatterns } = context;
    
    let level = 0.3; // ê¸°ë³¸ ë ˆë²¨
    
    if (personalityProfile?.type) level += 0.2;
    if (cues?.length > 0) level += Math.min(cues.length * 0.05, 0.3);
    if (behaviorPatterns?.length > 0) level += Math.min(behaviorPatterns.length * 0.02, 0.2);
    
    return Math.min(level, 1.0);
  }

  private extractUsedData(context: any): string[] {
    const data: string[] = [];
    
    if (context.personalityProfile) data.push('Personality Profile');
    if (context.cues?.length > 0) data.push(`${context.cues.length} Personal Contexts`);
    if (context.behaviorPatterns?.length > 0) data.push('Behavior Patterns');
    if (context.preferences && Object.keys(context.preferences).length > 0) data.push('User Preferences');
    if (context.recentInteractions?.length > 0) data.push('Recent Interactions');
    
    return data;
  }

  // ============================================================================
  // ğŸ“Š ê³µê°œ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  // ============================================================================
  
  public getAvailableModels(): any[] {
    return [
      {
        id: 'personalized-agent',
        name: 'Personalized Agent',
        available: true,
        recommended: true,
        type: 'hybrid',
        description: 'AI Passport ê¸°ë°˜ ê°œì¸í™” ëª¨ë¸'
      },
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        available: !!process.env.OPENAI_API_KEY,
        type: 'cloud',
        provider: 'openai'
      },
      {
        id: 'claude-3.5-sonnet',
        name: 'Claude 3.5 Sonnet',
        available: !!process.env.ANTHROPIC_API_KEY,
        type: 'cloud',
        provider: 'anthropic'
      },
      {
        id: 'llama3.2:3b',
        name: 'Llama 3.2 3B',
        available: true,
        type: 'local',
        provider: 'ollama',
        recommended: true
      }
    ];
  }

  public async getServiceStatus(): Promise<any> {
    return {
      openai: {
        configured: !!process.env.OPENAI_API_KEY,
        connected: !!this.openaiClient
      },
      anthropic: {
        configured: !!process.env.ANTHROPIC_API_KEY,
        connected: !!this.anthropicClient
      },
      ollama: {
        configured: true,
        connected: false // ì‹¤ì œ ì—°ê²° ìƒíƒœëŠ” ë³„ë„ ì²´í¬ í•„ìš”
      }
    };
  }

  public async getChatHistory(options: {
    userId: string;
    limit?: number;
    offset?: number;
  }): Promise<any[]> {
    try {
      return await this.databaseService.getChatHistory(
        options.userId,
        undefined,
        options.limit || 20,
        options.offset || 0
      );
    } catch (error) {
      console.error('âŒ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜:', error);
      return [];
    }
  }
}