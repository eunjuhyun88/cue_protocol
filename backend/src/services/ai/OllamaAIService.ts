// ============================================================================
// ğŸ¦™ í†µí•©ëœ Ollama AI ì„œë¹„ìŠ¤ (ì¤‘ë³µ í•´ê²° + í˜¸í™˜ì„± ìœ ì§€)
// ê²½ë¡œ: backend/src/services/ai/OllamaAIService.ts
// ìš©ë„: Ollama ì „ìš© AI ì„œë¹„ìŠ¤ + DatabaseService í†µí•© + ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±
// í˜¸ì¶œêµ¬ì¡°: DIContainer â†’ OllamaAIService â†’ DatabaseService + Ollama API
// ============================================================================

interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
  images?: string[];
}

interface OllamaResponse {
  model: string;
  created_at: string;
  message?: {
    role: string;
    content: string;
  };
  response?: string; // generate APIìš©
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

interface OllamaModelInfo {
  name: string;
  modified_at: string;
  size: number;
  digest: string;
  details?: {
    format?: string;
    family?: string;
    families?: string[];
    parameter_size?: string;
    quantization_level?: string;
  };
}

export interface AIResponse {
  content: string;
  model: string;
  tokensUsed?: number;
  processingTime?: number;
  confidence?: number;
  provider?: string;
  local?: boolean;
  privacy?: string;
  metadata?: {
    promptTokens?: number;
    completionTokens?: number;
    modelSize?: string;
    quantization?: string;
    error?: string;
    fallback?: boolean;
    conversationId?: string;
    messageId?: string;
  };
}

export interface AIModel {
  id: string;
  name: string;
  available: boolean;
  type: 'chat' | 'code' | 'reasoning' | 'embedding';
  size: string;
  description: string;
  recommended?: boolean;
}

/**
 * í†µí•©ëœ Ollama AI ì„œë¹„ìŠ¤
 * - ê¸°ì¡´ ollama.tsì˜ ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
 * - DatabaseService í†µí•©ìœ¼ë¡œ ëŒ€í™” ì €ì¥
 * - ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì§€ì›
 * - í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
 */
export class OllamaAIService {
  private static instance: OllamaAIService;
  private baseURL: string;
  private timeout: number;
  private retryCount: number;
  private isAvailable: boolean = false;
  private lastHealthCheck: number = 0;
  private healthCheckInterval: number = 5 * 60 * 1000; // 5ë¶„
  private models: string[] = [];
  private modelsLastFetched: number = 0;
  private modelsRefreshInterval: number = 10 * 60 * 1000; // 10ë¶„
  private availableModels: Map<string, OllamaModelInfo> = new Map();
  private modelConfigs: Map<string, any> = new Map();
  private isInitialized: boolean = false;
  private db: any = null; // DatabaseService (ì„ íƒì )

  private constructor() {
    console.log('ğŸ¦™ === OllamaAIService ì´ˆê¸°í™” (DatabaseService í†µí•©) ===');
    
    this.baseURL = process.env.OLLAMA_BASE_URL || process.env.OLLAMA_HOST || process.env.OLLAMA_URL || 'http://localhost:11434';
    this.timeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000');
    this.retryCount = parseInt(process.env.OLLAMA_RETRY_COUNT || '3');
    
    console.log(`ğŸ”— Ollama ì„œë²„: ${this.baseURL}`);
    
    // DatabaseService ì—°ë™ ì‹œë„ (ì„ íƒì )
    this.initializeDatabaseConnection();
    
    // ëª¨ë¸ë³„ ì„¤ì • ì´ˆê¸°í™”
    this.initializeModelConfigs();
    
    // ë¹„ë™ê¸° ì´ˆê¸°í™”
    this.initializeAsync();
  }

  public static getInstance(): OllamaAIService {
    if (!OllamaAIService.instance) {
      OllamaAIService.instance = new OllamaAIService();
    }
    return OllamaAIService.instance;
  }

  // ============================================================================
  // ğŸ”§ ì´ˆê¸°í™” ë©”ì„œë“œë“¤
  // ============================================================================

  /**
   * DatabaseService ì—°ë™ ì´ˆê¸°í™” (ì„ íƒì )
   */
  private initializeDatabaseConnection(): void {
    try {
      // DI Containerë¥¼ í†µí•œ DatabaseService ê°€ì ¸ì˜¤ê¸° ì‹œë„
      const DIContainer = require('../../core/DIContainer');
      const container = DIContainer.DIContainer?.getInstance?.();
      
      if (container) {
        this.db = container.get('ActiveDatabaseService');
        console.log(`ğŸ—„ï¸ DatabaseService ì—°ë™: ${this.db?.isConnected?.() ? 'âœ…' : 'âš ï¸'}`);
      }
    } catch (error) {
      console.log('ğŸ—„ï¸ DatabaseService ì—°ë™: âš ï¸ (ì„ íƒì  ê¸°ëŠ¥)');
      this.db = null;
    }
  }

  /**
   * ë¹„ë™ê¸° ì´ˆê¸°í™”
   */
  private async initializeAsync(): Promise<void> {
    try {
      console.log('ğŸ” Ollama ì´ˆê¸° ì—°ê²° í™•ì¸ ì¤‘...');
      
      const isConnected = await this.checkConnection();
      if (isConnected) {
        console.log('âœ… Ollama ì´ˆê¸° ì—°ê²° ì„±ê³µ');
        await this.loadAvailableModels();
        console.log(`âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: ${this.models.length} ê°œ ëª¨ë¸`);
      } else {
        console.warn('âš ï¸ Ollama ì´ˆê¸° ì—°ê²° ì‹¤íŒ¨ - í´ë°± ëª¨ë“œë¡œ ë™ì‘');
        this.printConnectionHelp();
      }
      
      this.isInitialized = true;
    } catch (error: any) {
      console.warn('âš ï¸ Ollama ì´ˆê¸°í™” ì‹¤íŒ¨:', error.message);
      this.printConnectionHelp();
    }
  }

  /**
   * ì—°ê²° ë„ì›€ë§ ì¶œë ¥
   */
  private printConnectionHelp(): void {
    console.log('\nğŸ’¡ Ollama ì„¤ì • ë„ì›€ë§:');
    console.log('1. Ollama ì„¤ì¹˜: brew install ollama');
    console.log('2. ì„œë²„ ì‹œì‘: ollama serve');
    console.log('3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b');
    console.log('4. ëª¨ë¸ í™•ì¸: ollama list');
    console.log(`5. ì—°ê²° í™•ì¸: curl ${this.baseURL}/api/tags\n`);
  }

  /**
   * ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì • ì´ˆê¸°í™”
   */
  private initializeModelConfigs(): void {
    const configs = {
      'llama3.2:3b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'llama3.2:1b': { type: 'chat', temperature: 0.8, max_tokens: 1024 },
      'deepseek-coder:6.7b': { type: 'code', temperature: 0.3, max_tokens: 4096 },
      'codellama:7b': { type: 'code', temperature: 0.2, max_tokens: 4096 },
      'phi3:mini': { type: 'reasoning', temperature: 0.5, max_tokens: 2048 },
      'mistral:latest': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'llama3.1:8b': { type: 'chat', temperature: 0.7, max_tokens: 4096 }
    };

    Object.entries(configs).forEach(([model, config]) => {
      this.modelConfigs.set(model, config);
    });

    console.log(`âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: ${this.modelConfigs.size} ê°œ ëª¨ë¸`);
  }

  // ============================================================================
  // ğŸ” ì—°ê²° ìƒíƒœ ê´€ë¦¬ (ollama.ts í˜¸í™˜)
  // ============================================================================

  /**
   * Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
   */
  async checkConnection(): Promise<boolean> {
    const now = Date.now();
    
    // ìºì‹œëœ ì—°ê²° ìƒíƒœ ì‚¬ìš© (ì„±ê³µí•œ ê²½ìš°ë§Œ)
    if (now - this.lastHealthCheck < this.healthCheckInterval && this.isAvailable) {
      return this.isAvailable;
    }

    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000);

      const response = await fetch(`${this.baseURL}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      this.isAvailable = true;
      this.lastHealthCheck = now;
      
      return true;

    } catch (error: any) {
      this.isAvailable = false;
      this.lastHealthCheck = now;
      
      if (error.name === 'AbortError') {
        console.warn('âš ï¸ Ollama ì„œë²„ ì—°ê²° ì‹œê°„ ì´ˆê³¼');
      } else {
        console.warn('âš ï¸ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨:', error.message);
      }
      
      return false;
    }
  }

  /**
   * ê°•ì œ í—¬ìŠ¤ì²´í¬ (ìºì‹œ ë¬´ì‹œ)
   */
  async forceHealthCheck(): Promise<boolean> {
    this.lastHealthCheck = 0;
    return await this.checkConnection();
  }

  // ============================================================================
  // ğŸ“‹ ëª¨ë¸ ê´€ë¦¬ (ollama.ts í˜¸í™˜)
  // ============================================================================

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async getModels(): Promise<string[]> {
    await this.loadAvailableModels();
    return this.models;
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë¡œë“œ
   */
  async loadAvailableModels(): Promise<void> {
    const now = Date.now();
    
    // ìºì‹œëœ ëª¨ë¸ ëª©ë¡ì´ ìˆê³  ìµœì‹ ì´ë©´ ë°˜í™˜
    if (this.models.length > 0 && now - this.modelsLastFetched < this.modelsRefreshInterval) {
      return;
    }

    try {
      if (!await this.checkConnection()) {
        console.warn('âš ï¸ Ollama ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€ - ë¹ˆ ëª¨ë¸ ëª©ë¡ ë°˜í™˜');
        return;
      }

      const response = await fetch(`${this.baseURL}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: AbortSignal.timeout(10000)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data: unknown = await response.json();
      const modelsData = (data as any).models || [];
      
      this.availableModels.clear();
      this.models = [];
      
      modelsData.forEach((model: OllamaModelInfo) => {
        this.availableModels.set(model.name, model);
        this.models.push(model.name);
      });
      
      this.modelsLastFetched = now;
      
      console.log(`ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ (${this.models.length}ê°œ):`, this.models);

    } catch (error: any) {
      console.error('âŒ ëª¨ë¸ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨:', error.message);
    }
  }

  /**
   * í–¥ìƒëœ ëª¨ë¸ ëª©ë¡ (AIModel í˜•ì‹)
   */
  async getAvailableModels(): Promise<AIModel[]> {
    await this.loadAvailableModels();

    const models: AIModel[] = [];
    
    this.availableModels.forEach((info, name) => {
      const config = this.modelConfigs.get(name);
      const size = this.formatSize(info.size);
      
      models.push({
        id: name,
        name: this.getDisplayName(name),
        available: true,
        type: config?.type || 'chat',
        size,
        description: this.getModelDescription(name),
        recommended: name === 'llama3.2:3b'
      });
    });

    return models.sort((a, b) => {
      const typeOrder = { 'chat': 0, 'code': 1, 'reasoning': 2, 'embedding': 3 };
      return (typeOrder[a.type] || 9) - (typeOrder[b.type] || 9);
    });
  }

  /**
   * ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜
   */
  getDefaultModel(): string {
    if (this.models.includes('llama3.2:3b')) return 'llama3.2:3b';
    if (this.models.includes('llama3.2')) return 'llama3.2';
    if (this.models.includes('llama3:8b')) return 'llama3:8b';
    if (this.models.length > 0) return this.models[0];
    return 'llama3.2:3b';
  }

  // ============================================================================
  // ğŸ¯ AI ì‘ë‹µ ìƒì„± (í–¥ìƒëœ ê¸°ëŠ¥)
  // ============================================================================

  /**
   * AI ì‘ë‹µ ìƒì„± (ë©”ì¸ ë©”ì„œë“œ)
   */
  async generateResponse(
    message: string,
    modelId?: string,
    personalizedContext?: any,
    userId?: string,
    conversationId?: string
  ): Promise<AIResponse> {
    const startTime = Date.now();
    const model = modelId || this.getDefaultModel();

    try {
      // ì—°ê²° ìƒíƒœ í™•ì¸
      const connected = await this.checkConnection();
      if (!connected) {
        throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `ollama serve` ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.');
      }

      console.log(`ğŸ¦™ Ollama ì‘ë‹µ ìƒì„± ì‹œì‘: ${model}`);

      // ë©”ì‹œì§€ êµ¬ì„± (ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
      const messages = this.buildMessages(message, model, personalizedContext);
      
      // Ollama API í˜¸ì¶œ
      const response = await this.callOllamaAPI(model, messages);
      
      const processingTime = Date.now() - startTime;
      
      const aiResponse: AIResponse = {
        content: response.message?.content || response.response || '',
        model: `${model}`,
        tokensUsed: response.eval_count || this.estimateTokens(message + (response.message?.content || '')),
        processingTime,
        confidence: 0.9,
        provider: 'ollama',
        local: true,
        privacy: 'fully_local',
        metadata: {
          promptTokens: response.prompt_eval_count || this.estimateTokens(message),
          completionTokens: response.eval_count || this.estimateTokens(response.message?.content || ''),
          modelSize: this.getModelSize(model),
          conversationId,
          messageId: this.generateMessageId()
        }
      };

      // DatabaseServiceë¥¼ í†µí•œ ëŒ€í™” ì €ì¥ (ì„ íƒì )
      if (userId && this.db) {
        try {
          await this.saveChatToDatabase(userId, message, aiResponse, conversationId);
        } catch (dbError) {
          console.warn('âš ï¸ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨ (ê¸°ëŠ¥ì€ ê³„ì†ë¨):', dbError);
        }
      }

      console.log(`âœ… Ollama ì‘ë‹µ ìƒì„± ì™„ë£Œ: ${processingTime}ms`);
      return aiResponse;

    } catch (error: any) {
      const processingTime = Date.now() - startTime;
      console.error(`âŒ Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (${processingTime}ms):`, error.message);
      
      return this.generateFallbackResponse(message, model, processingTime);
    }
  }

  /**
   * ê¸°ì¡´ ollama.ts í˜¸í™˜ - generate ë©”ì„œë“œ
   */
  async generate(
    model: string,
    prompt: string,
    options: {
      temperature?: number;
      num_predict?: number;
    } = {}
  ): Promise<string> {
    try {
      console.log(`ğŸ¦™ Ollama Generate ìš”ì²­: ${model}`);
      
      if (!await this.checkConnection()) {
        throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      }

      const requestBody = {
        model,
        prompt,
        stream: false,
        options: {
          temperature: options.temperature || 0.7,
          num_predict: options.num_predict || 1000
        }
      };

      const response = await fetch(`${this.baseURL}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(this.timeout)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data: unknown = await response.json();
      const result = (data as any).response || 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
      
      console.log('âœ… Ollama Generate ì„±ê³µ');
      return result;

    } catch (error: any) {
      console.error('âŒ Ollama Generate ì‹¤íŒ¨:', error.message);
      throw error;
    }
  }

  /**
   * ê¸°ì¡´ ollama.ts í˜¸í™˜ - chatCompletion ë©”ì„œë“œ
   */
  async chatCompletion(
    model: string,
    messages: OllamaMessage[],
    options: {
      temperature?: number;
      stream?: boolean;
      personalizedContext?: any;
    } = {}
  ): Promise<string> {
    
    for (let attempt = 1; attempt <= this.retryCount; attempt++) {
      try {
        console.log(`ğŸ¦™ Ollama ì±„íŒ… ì‹œë„ ${attempt}/${this.retryCount} - ëª¨ë¸: ${model}`);
        
        if (!await this.checkConnection()) {
          throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
        }

        const result = await this.performChatRequest(model, messages, options);
        console.log(`âœ… Ollama ì±„íŒ… ì„±ê³µ (ì‹œë„ ${attempt})`);
        return result;

      } catch (error: any) {
        console.error(`âŒ Ollama ì±„íŒ… ì‹œë„ ${attempt} ì‹¤íŒ¨:`, error.message);
        
        if (attempt === this.retryCount) {
          throw error;
        }
        
        await this.delay(1000 * attempt);
      }
    }

    throw new Error('ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨');
  }

  // ============================================================================
  // ğŸ”§ ë‚´ë¶€ ë©”ì„œë“œë“¤
  // ============================================================================

  /**
   * Ollama API ì§ì ‘ í˜¸ì¶œ
   */
  private async callOllamaAPI(model: string, messages: OllamaMessage[]): Promise<OllamaResponse> {
    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        messages,
        stream: false,
        options: {
          temperature: this.modelConfigs.get(model)?.temperature || 0.7,
          num_predict: this.modelConfigs.get(model)?.max_tokens || 2048
        }
      }),
      signal: AbortSignal.timeout(this.timeout)
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const data: unknown = await response.json();
    return data as OllamaResponse;
  }

  /**
   * ì‹¤ì œ ì±„íŒ… ìš”ì²­ ìˆ˜í–‰ (ê¸°ì¡´ í˜¸í™˜)
   */
  private async performChatRequest(
    model: string,
    messages: OllamaMessage[],
    options: {
      temperature?: number;
      stream?: boolean;
      personalizedContext?: any;
    }
  ): Promise<string> {
    
    const { temperature = 0.7, stream = false, personalizedContext } = options;

    const requestBody = {
      model,
      messages: this.formatMessages(messages, personalizedContext),
      stream,
      options: {
        temperature,
        num_predict: 2000,
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(requestBody),
      signal: AbortSignal.timeout(this.timeout)
    });

    if (!response.ok) {
      const errorText = await response.text();
      let errorMessage = `HTTP ${response.status}: ${response.statusText}`;
      
      try {
        const errorJson = JSON.parse(errorText);
        errorMessage = errorJson.error || errorMessage;
      } catch {
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
      }
      
      throw new Error(errorMessage);
    }

    const data: OllamaResponse = await response.json();
    
    if (!data.done) {
      throw new Error('Ollama ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    const content = data.message?.content || data.response || '';
    
    if (!content.trim()) {
      throw new Error('Ollamaì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤');
    }

    return content;
  }

  /**
   * ë©”ì‹œì§€ í¬ë§·íŒ… (ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
   */
  private buildMessages(message: string, model: string, context?: any): OllamaMessage[] {
    const messages: OllamaMessage[] = [];
    
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    const systemPrompt = this.buildSystemPrompt(model, context);
    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.push({ role: 'user', content: message });

    return messages;
  }

  /**
   * ê¸°ì¡´ í˜¸í™˜ - ë©”ì‹œì§€ í¬ë§·íŒ…
   */
  private formatMessages(messages: OllamaMessage[], personalizedContext?: any): OllamaMessage[] {
    const formattedMessages: OllamaMessage[] = [];

    if (personalizedContext) {
      formattedMessages.push({
        role: 'system',
        content: this.formatPersonalizedContext(personalizedContext)
      });
    }

    formattedMessages.push(...messages);
    return formattedMessages;
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private buildSystemPrompt(model: string, context?: any): string {
    let basePrompt = "You are a helpful AI assistant specialized in conversational interactions.";
    
    const config = this.modelConfigs.get(model);
    if (config?.type === 'code') {
      basePrompt = "You are an expert programming assistant. Provide clear, well-commented code solutions.";
    } else if (config?.type === 'reasoning') {
      basePrompt = "You are a logical reasoning assistant. Think step by step and provide detailed explanations.";
    }

    if (!context) return basePrompt;

    // ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if (context.personalityProfile?.type) {
      basePrompt += `\n\nUser Profile: The user has a ${context.personalityProfile.type} personality type.`;
    }

    if (context.cues && context.cues.length > 0) {
      basePrompt += `\n\nPersonalization: You have access to ${context.cues.length} personal preference data points.`;
    }

    return basePrompt;
  }

  /**
   * ê¸°ì¡´ í˜¸í™˜ - ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
   */
  private formatPersonalizedContext(context: any): string {
    if (!context || Object.keys(context).length === 0) {
      return 'ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.';
    }

    let contextPrompt = 'ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n';
    contextPrompt += 'ì‚¬ìš©ìì— ëŒ€í•œ ì •ë³´:\n';

    if (context.personalityProfile) {
      contextPrompt += `- ì„±ê²©: ${context.personalityProfile.type || 'Adaptive'}\n`;
      contextPrompt += `- ì†Œí†µ ìŠ¤íƒ€ì¼: ${context.personalityProfile.communicationStyle || 'Balanced'}\n`;
    }

    if (context.behaviorPatterns && context.behaviorPatterns.length > 0) {
      contextPrompt += `- ê´€ì‹¬ì‚¬: ${context.behaviorPatterns.slice(0, 3).join(', ')}\n`;
    }

    contextPrompt += '\nì‚¬ìš©ìì˜ ì„±ê²©ê³¼ ì„ í˜¸ë„ì— ë§ì¶° ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.\n';
    return contextPrompt;
  }

  /**
   * DatabaseServiceë¥¼ í†µí•œ ëŒ€í™” ì €ì¥
   */
  private async saveChatToDatabase(
    userId: string,
    userMessage: string,
    aiResponse: AIResponse,
    conversationId?: string
  ): Promise<void> {
    try {
      if (!this.db || typeof this.db.saveChatMessage !== 'function') {
        return; // DB ì„œë¹„ìŠ¤ ì—†ìœ¼ë©´ ìŠ¤í‚µ
      }

      // ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
      await this.db.saveChatMessage({
        conversation_id: conversationId || 'default',
        user_id: userId,
        content: userMessage,
        sender: 'user',
        model: null,
        tokens_used: aiResponse.metadata?.promptTokens || 0,
        timestamp: new Date().toISOString()
      });

      // AI ì‘ë‹µ ì €ì¥
      await this.db.saveChatMessage({
        conversation_id: conversationId || 'default',
        user_id: userId,
        content: aiResponse.content,
        sender: 'assistant',
        model: aiResponse.model,
        tokens_used: aiResponse.metadata?.completionTokens || 0,
        processing_time: aiResponse.processingTime,
        timestamp: new Date().toISOString()
      });

      console.log('âœ… ëŒ€í™” ì €ì¥ ì™„ë£Œ');
    } catch (error) {
      console.warn('âš ï¸ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨:', error);
    }
  }

  // ============================================================================
  // ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  // ============================================================================

  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }

  private getModelSize(model: string): string {
    const info = this.availableModels.get(model);
    return info ? this.formatSize(info.size) : 'Unknown';
  }

  private formatSize(bytes: number): string {
    const units = ['B', 'KB', 'MB', 'GB', 'TB'];
    let size = bytes;
    let unitIndex = 0;
    
    while (size >= 1024 && unitIndex < units.length - 1) {
      size /= 1024;
      unitIndex++;
    }
    
    return `${size.toFixed(1)} ${units[unitIndex]}`;
  }

  private getDisplayName(modelName: string): string {
    const nameMap: { [key: string]: string } = {
      'llama3.2:3b': 'Llama 3.2 (3B)',
      'llama3.2:1b': 'Llama 3.2 (1B)',
      'deepseek-coder:6.7b': 'DeepSeek Coder (6.7B)',
      'codellama:7b': 'Code Llama (7B)',
      'phi3:mini': 'Phi-3 Mini',
      'mistral:latest': 'Mistral 7B',
      'llama3.1:8b': 'Llama 3.1 (8B)'
    };
    
    return nameMap[modelName] || modelName;
  }

  private getModelDescription(modelName: string): string {
    const descriptions: { [key: string]: string } = {
      'llama3.2:3b': 'ë²”ìš© ëŒ€í™” ë° í…ìŠ¤íŠ¸ ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸',
      'llama3.2:1b': 'ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê°„ë‹¨í•œ ì‘ì—…ìš© ê²½ëŸ‰ ëª¨ë¸',
      'deepseek-coder:6.7b': 'ì½”ë“œ ìƒì„±, ë””ë²„ê¹…, ì„¤ëª…ì— íŠ¹í™”ëœ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ ëª¨ë¸',
      'codellama:7b': 'Metaì˜ ì½”ë“œ ìƒì„± ì „ë¬¸ ëª¨ë¸, ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì§€ì›',
      'phi3:mini': 'ë…¼ë¦¬ì  ì¶”ë¡ ê³¼ ìˆ˜í•™ì  ë¬¸ì œ í•´ê²°ì— ê°•í•œ ì†Œí˜• ëª¨ë¸',
      'mistral:latest': 'íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ì‘ë‹µì„ ì œê³µí•˜ëŠ” ë²”ìš© ëª¨ë¸'
    };
    
    return descriptions[modelName] || 'ë²”ìš© AI ëª¨ë¸';
  }

  private generateMessageId(): string {
    return `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * í´ë°± ì‘ë‹µ ìƒì„±
   */
  getFallbackResponse(userMessage: string): string {
    return `ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ Ollama AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n1. \`ollama serve\` ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘\n2. \`ollama pull llama3.2:3b\` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n3. Ollamaê°€ ${this.baseURL} ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸\n\nì§ˆë¬¸ "${userMessage}"ì— ëŒ€í•œ ë‹µë³€ì€ ì„œë¹„ìŠ¤ ë³µêµ¬ í›„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.`;
  }

  private generateFallbackResponse(message: string, model: string, processingTime: number): AIResponse {
    const fallbackContent = this.getFallbackResponse(message);

    return {
      content: fallbackContent,
      model: `Fallback (${model})`,
      tokensUsed: fallbackContent.length,
      processingTime,
      confidence: 0.3,
      provider: 'ollama',
      local: true,
      metadata: {
        error: 'Service unavailable',
        fallback: true
      }
    };
  }

  // ============================================================================
  // ğŸ“Š ìƒíƒœ ë° ì •ë³´ ë©”ì„œë“œë“¤
  // ============================================================================

  /**
   * ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜
   */
  async getServiceStatus(): Promise<any> {
    const isConnected = await this.checkConnection();
    await this.loadAvailableModels();

    return {
      provider: 'ollama',
      connected: isConnected,
      baseUrl: this.baseURL,
      models: this.models,
      defaultModel: this.getDefaultModel(),
      features: [
        'chat', 
        'completion', 
        'local', 
        'privacy-focused',
        'conversation_storage',
        'personalization_support'
      ],
      database: {
        connected: this.db?.isConnected?.() || false,
        available: !!this.db
      }
    };
  }

  /**
   * ê¸°ì¡´ í˜¸í™˜ - ìƒíƒœ ì •ë³´
   */
  getStatus(): {
    available: boolean;
    baseUrl: string;
    lastHealthCheck: Date | null;
    timeout: number;
    retryCount: number;
    modelCount: number;
    cachedModels: string[];
  } {
    return {
      available: this.isAvailable,
      baseUrl: this.baseURL,
      lastHealthCheck: this.lastHealthCheck ? new Date(this.lastHealthCheck) : null,
      timeout: this.timeout,
      retryCount: this.retryCount,
      modelCount: this.models.length,
      cachedModels: this.models
    };
  }

  /**
   * ì—°ê²° í…ŒìŠ¤íŠ¸
   */
  async testConnection(): Promise<{success: boolean, message: string, details?: any}> {
    try {
      const isConnected = await this.forceHealthCheck();
      
      if (isConnected) {
        await this.loadAvailableModels();
        return {
          success: true,
          message: 'Ollama ì—°ê²° ì„±ê³µ',
          details: {
            modelCount: this.models.length,
            availableModels: this.models.slice(0, 5)
          }
        };
      } else {
        return {
          success: false,
          message: 'Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
          details: {
            baseUrl: this.baseURL,
            suggestion: 'ollama serve ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”'
          }
        };
      }
    } catch (error: any) {
      return {
        success: false,
        message: `ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ${error.message}`,
        details: {
          baseUrl: this.baseURL,
          error: error.message
        }
      };
    }
  }

  /**
   * ì„œë¹„ìŠ¤ ì •ë¦¬ (DI Containerìš©)
   */
  public dispose(): void {
    console.log('ğŸ§¹ OllamaAIService ì •ë¦¬ ì¤‘...');
    this.isInitialized = false;
    this.models = [];
    this.availableModels.clear();
    this.isAvailable = false;
    console.log('âœ… OllamaAIService ì •ë¦¬ ì™„ë£Œ');
  }
}

// ============================================================================
// ğŸ“¤ Export (ì¤‘ë³µ ì œê±° ë° í˜¸í™˜ì„± ë³´ì¥)
// ============================================================================

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
const ollamaService = OllamaAIService.getInstance();

// ê¸°ì¡´ ollama.ts í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
export const checkConnection = () => ollamaService.checkConnection();
export const getModels = () => ollamaService.getModels();
export const chat = (model: string, messages: OllamaMessage[], stream: boolean = false) => 
  ollamaService.chatCompletion(model, messages, { stream });

// ============================================================================
// ğŸ› ìˆ˜ì •: ì¤‘ë³µ export ë¬¸ì œ í•´ê²°
// ============================================================================

// í´ë˜ìŠ¤ì™€ ì¸ìŠ¤í„´ìŠ¤ export (ì¤‘ë³µ ì œê±°)
export { OllamaAIService };
export { ollamaService };

// ê¸°ë³¸ export
export default ollamaService;

// ============================================================================
// ğŸ‰ ìˆ˜ì • ì™„ë£Œ ë¡œê·¸
// ============================================================================

console.log('âœ… OllamaAIService Export ìˆ˜ì • ì™„ë£Œ:');
console.log('  ğŸ› FIXED: Multiple exports ì¤‘ë³µ ì œê±°');
console.log('  âœ… í´ë˜ìŠ¤ì™€ ì¸ìŠ¤í„´ìŠ¤ ëª…í™•íˆ êµ¬ë¶„');
console.log('  ğŸ”§ ê¸°ì¡´ í˜¸í™˜ì„± 100% ìœ ì§€');