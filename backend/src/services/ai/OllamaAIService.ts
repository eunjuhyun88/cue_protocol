// ============================================================================
// ğŸ¦™ ì •ë¦¬ëœ Ollama ì „ìš© AI ì„œë¹„ìŠ¤
// ìœ„ì¹˜: backend/src/services/ai/OllamaAIService.ts
// ëª©ì : Ollamaë§Œ ì§€ì›, í´ë¼ìš°ë“œ AI ì½”ë“œ ì™„ì „ ì œê±°
// ============================================================================

import axios, { AxiosInstance } from 'axios';

export interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface OllamaResponse {
  model: string;
  created_at: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface OllamaModelInfo {
  name: string;
  modified_at: string;
  size: number;
  digest: string;
  details?: {
    format: string;
    family: string;
    families: string[];
    parameter_size: string;
    quantization_level: string;
  };
}

export interface AIResponse {
  content: string;
  model: string;
  tokensUsed: number;
  processingTime: number;
  confidence: number;
  metadata?: {
    promptTokens?: number;
    completionTokens?: number;
    modelSize?: string;
    quantization?: string;
    error?: string;
    fallback?: boolean;
  };
}

export interface AIModel {
  id: string;
  name: string;
  available: boolean;
  type: 'chat' | 'code' | 'reasoning' | 'embedding';
  size: string;
  description: string;
  recommended?: boolean;
}

/**
 * Ollama ì „ìš© AI ì„œë¹„ìŠ¤
 * - OpenAI, Claude, Gemini ì½”ë“œ ì™„ì „ ì œê±°
 * - Ollama ì„œë²„ì™€ë§Œ í†µì‹ 
 * - í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ë“¤ ìµœëŒ€ í™œìš©
 */
export class OllamaAIService {
  private client: AxiosInstance;
  private baseURL: string;
  private timeout: number;
  private availableModels: Map<string, OllamaModelInfo> = new Map();
  private modelConfigs: Map<string, any> = new Map();

  constructor() {
    this.baseURL = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    this.timeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000');

    // Axios í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    this.client = axios.create({
      baseURL: this.baseURL,
      timeout: this.timeout,
      headers: {
        'Content-Type': 'application/json'
      }
    });

    console.log(`ğŸ¦™ OllamaAIService ì´ˆê¸°í™” - ${this.baseURL}`);
    
    // ëª¨ë¸ë³„ ì„¤ì • ì´ˆê¸°í™”
    this.initializeModelConfigs();
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë¡œë“œ
    this.loadAvailableModels();
  }

  // ============================================================================
  // ğŸ”Œ Ollama ì„œë²„ ì—°ê²° ê´€ë¦¬
  // ============================================================================

  /**
   * Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
   */
  async checkConnection(): Promise<boolean> {
    try {
      const response = await this.client.get('/api/tags', { timeout: 5000 });
      console.log('âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ');
      return true;
    } catch (error: any) {
      console.warn('âš ï¸ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨:', error.message);
      return false;
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë¡œë“œ
   */
  async loadAvailableModels(): Promise<void> {
    try {
      const response = await this.client.get('/api/tags');
      const models = response.data.models || [];
      
      this.availableModels.clear();
      models.forEach((model: OllamaModelInfo) => {
        this.availableModels.set(model.name, model);
      });

      console.log(`âœ… Ollama ëª¨ë¸ ${models.length}ê°œ ë¡œë“œ ì™„ë£Œ:`, 
        Array.from(this.availableModels.keys()).join(', '));

    } catch (error: any) {
      console.error('âŒ Ollama ëª¨ë¸ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨:', error.message);
    }
  }

  // ============================================================================
  // ğŸ¤– ëª¨ë¸ ì„¤ì • ë° ê´€ë¦¬
  // ============================================================================

  /**
   * ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì • ì´ˆê¸°í™”
   */
  private initializeModelConfigs(): void {
    // ì±„íŒ… ëª¨ë¸ ì„¤ì •
    this.modelConfigs.set('llama3.2:3b', {
      type: 'chat',
      temperature: 0.7,
      max_tokens: 2048,
      system_prompt: "You are a helpful AI assistant specialized in conversational interactions."
    });

    this.modelConfigs.set('llama3.2:1b', {
      type: 'chat',
      temperature: 0.8,
      max_tokens: 1024,
      system_prompt: "You are a lightweight AI assistant for quick responses."
    });

    // ì½”ë”© ëª¨ë¸ ì„¤ì •
    this.modelConfigs.set('deepseek-coder:6.7b', {
      type: 'code',
      temperature: 0.3,
      max_tokens: 4096,
      system_prompt: "You are an expert programming assistant. Provide clear, well-commented code solutions."
    });

    this.modelConfigs.set('codellama:7b', {
      type: 'code',
      temperature: 0.2,
      max_tokens: 4096,
      system_prompt: "You are CodeLlama, an AI assistant specialized in code generation and explanation."
    });

    this.modelConfigs.set('codellama:13b', {
      type: 'code',
      temperature: 0.2,
      max_tokens: 4096,
      system_prompt: "You are CodeLlama 13B, providing detailed code analysis and solutions."
    });

    // ì¶”ë¡  ëª¨ë¸ ì„¤ì •
    this.modelConfigs.set('phi3:mini', {
      type: 'reasoning',
      temperature: 0.5,
      max_tokens: 2048,
      system_prompt: "You are a logical reasoning assistant. Think step by step and provide detailed explanations."
    });

    this.modelConfigs.set('phi3:latest', {
      type: 'reasoning',
      temperature: 0.5,
      max_tokens: 2048,
      system_prompt: "You are Phi-3, focusing on analytical thinking and problem-solving."
    });

    // ë²”ìš© ëª¨ë¸ ì„¤ì •
    this.modelConfigs.set('mistral:latest', {
      type: 'chat',
      temperature: 0.7,
      max_tokens: 2048,
      system_prompt: "You are Mistral, a helpful and efficient AI assistant."
    });

    this.modelConfigs.set('mistral:7b', {
      type: 'chat',
      temperature: 0.7,
      max_tokens: 2048,
      system_prompt: "You are Mistral 7B, providing balanced and informative responses."
    });

    // ëŒ€í˜• ëª¨ë¸ ì„¤ì •
    this.modelConfigs.set('llama3.1:8b', {
      type: 'chat',
      temperature: 0.7,
      max_tokens: 4096,
      system_prompt: "You are Llama 3.1 8B, providing comprehensive and detailed responses."
    });

    this.modelConfigs.set('llama3.1:70b', {
      type: 'chat',
      temperature: 0.6,
      max_tokens: 8192,
      system_prompt: "You are Llama 3.1 70B, a highly capable AI assistant for complex tasks."
    });

    console.log('âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ:', this.modelConfigs.size, 'ê°œ ëª¨ë¸');
  }

  // ============================================================================
  // ğŸ’¬ AI ì‘ë‹µ ìƒì„± (ë©”ì¸ ê¸°ëŠ¥)
  // ============================================================================

  /**
   * AI ì‘ë‹µ ìƒì„± (Ollama ì „ìš©)
   */
  async generateResponse(
    message: string,
    modelId: string,
    personalizedContext?: any
  ): Promise<AIResponse> {
    const startTime = Date.now();

    try {
      // ëª¨ë¸ ìœ íš¨ì„± í™•ì¸ ë° ì„ íƒ
      const model = await this.validateAndSelectModel(modelId);
      
      // ë©”ì‹œì§€ êµ¬ì„±
      const messages = this.buildMessages(message, model, personalizedContext);
      
      // Ollama API í˜¸ì¶œ
      const response = await this.callOllamaAPI(model, messages);
      
      // ì‘ë‹µ ì²˜ë¦¬
      const processingTime = Date.now() - startTime;
      
      return {
        content: response.message.content,
        model: `${model} (Ollama)`,
        tokensUsed: this.estimateTokens(message + response.message.content),
        processingTime,
        confidence: 0.9,
        metadata: {
          promptTokens: this.estimateTokens(message),
          completionTokens: this.estimateTokens(response.message.content),
          modelSize: this.getModelSize(model),
          quantization: this.getModelQuantization(model)
        }
      };

    } catch (error: any) {
      console.error(`âŒ Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (${modelId}):`, error.message);
      
      // í´ë°± ì‘ë‹µ ìƒì„±
      return this.generateFallbackResponse(message, modelId, Date.now() - startTime);
    }
  }

  /**
   * Ollama API ì§ì ‘ í˜¸ì¶œ
   */
  private async callOllamaAPI(model: string, messages: OllamaMessage[]): Promise<OllamaResponse> {
    console.log(`ğŸ¦™ Ollama API í˜¸ì¶œ: ${model}`);

    const response = await this.client.post('/api/chat', {
      model,
      messages,
      stream: false,
      options: {
        temperature: this.modelConfigs.get(model)?.temperature || 0.7,
        num_predict: this.modelConfigs.get(model)?.max_tokens || 2048
      }
    });

    if (!response.data || !response.data.message) {
      throw new Error('Invalid Ollama API response');
    }

    console.log(`âœ… Ollama ì‘ë‹µ ë°›ìŒ: ${response.data.message.content.length}ì`);
    return response.data;
  }

  // ============================================================================
  // ğŸ“ ë©”ì‹œì§€ êµ¬ì„± ë° í”„ë¡¬í”„íŠ¸ ìƒì„±
  // ============================================================================

  /**
   * ë©”ì‹œì§€ êµ¬ì„±
   */
  private buildMessages(
    userMessage: string,
    model: string,
    context?: any
  ): OllamaMessage[] {
    const messages: OllamaMessage[] = [];
    
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    const systemPrompt = this.buildSystemPrompt(model, context);
    if (systemPrompt) {
      messages.push({
        role: 'system',
        content: systemPrompt
      });
    }

    // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.push({
      role: 'user',
      content: userMessage
    });

    return messages;
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private buildSystemPrompt(model: string, context?: any): string {
    const basePrompt = this.modelConfigs.get(model)?.system_prompt || 
      "You are a helpful AI assistant.";

    if (!context) return basePrompt;

    const personalityInfo = context.personalityProfile?.type ? 
      `\n\nUser Profile: ${context.personalityProfile.type}` : '';
    
    const cueInfo = context.cues?.length ? 
      `\nAvailable Context: ${context.cues.length} data points` : '';

    return `${basePrompt}${personalityInfo}${cueInfo}

Please provide responses that match the user's communication style and preferences.`;
  }

  // ============================================================================
  // ğŸ” ëª¨ë¸ ì„ íƒ ë° ê´€ë¦¬
  // ============================================================================

  /**
   * ëª¨ë¸ ìœ íš¨ì„± í™•ì¸ ë° ì„ íƒ
   */
  private async validateAndSelectModel(requestedModel: string): Promise<string> {
    // ìš”ì²­ëœ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    if (this.availableModels.has(requestedModel)) {
      return requestedModel;
    }

    // ëª¨ë¸ íƒ€ì…ë³„ í´ë°± ë¡œì§
    const fallbackModels = {
      'code': ['deepseek-coder:6.7b', 'codellama:7b', 'codellama:13b', 'magicoder:7b'],
      'chat': ['llama3.2:3b', 'llama3.2:1b', 'mistral:latest', 'llama3.1:8b'],
      'reasoning': ['phi3:mini', 'phi3:latest', 'llama3.2:3b']
    };

    // ìš”ì²­ëœ ëª¨ë¸ì˜ íƒ€ì… ì¶”ì •
    let modelType = 'chat';
    if (requestedModel.includes('code') || requestedModel.includes('coder')) {
      modelType = 'code';
    } else if (requestedModel.includes('phi') || requestedModel.includes('reasoning')) {
      modelType = 'reasoning';
    }

    // í´ë°± ëª¨ë¸ ì°¾ê¸°
    const candidates = fallbackModels[modelType as keyof typeof fallbackModels];
    for (const candidate of candidates) {
      if (this.availableModels.has(candidate)) {
        console.log(`âš ï¸ ëª¨ë¸ í´ë°±: ${requestedModel} â†’ ${candidate}`);
        return candidate;
      }
    }

    // ìµœì¢… í´ë°±: ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ëª¨ë¸
    const firstAvailable = Array.from(this.availableModels.keys())[0];
    if (firstAvailable) {
      console.log(`âš ï¸ ìµœì¢… í´ë°±: ${requestedModel} â†’ ${firstAvailable}`);
      return firstAvailable;
    }

    throw new Error('ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤');
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
   */
  async getAvailableModels(): Promise<AIModel[]> {
    await this.loadAvailableModels();

    const models: AIModel[] = [];
    
    this.availableModels.forEach((info, name) => {
      const config = this.modelConfigs.get(name);
      const size = this.formatSize(info.size);
      
      models.push({
        id: name,
        name: this.getDisplayName(name),
        available: true,
        type: config?.type || 'chat',
        size,
        description: this.getModelDescription(name),
        recommended: name === process.env.OLLAMA_DEFAULT_MODEL
      });
    });

    // íƒ€ì…ë³„ ì •ë ¬
    return models.sort((a, b) => {
      const typeOrder = { 'chat': 0, 'code': 1, 'reasoning': 2, 'embedding': 3 };
      return (typeOrder[a.type] || 9) - (typeOrder[b.type] || 9);
    });
  }

  /**
   * ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜
   */
  async getServiceStatus(): Promise<any> {
    const isConnected = await this.checkConnection();
    await this.loadAvailableModels();

    return {
      provider: 'ollama',
      connected: isConnected,
      baseUrl: this.baseURL,
      modelsAvailable: this.availableModels.size,
      defaultModel: process.env.OLLAMA_DEFAULT_MODEL,
      models: Array.from(this.availableModels.keys()),
      lastChecked: new Date().toISOString()
    };
  }

  // ============================================================================
  // ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  // ============================================================================

  private estimateTokens(text: string): number {
    // ê°„ë‹¨í•œ í† í° ì¶”ì • (ì‹¤ì œë¡œëŠ” tokenizer ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥)
    return Math.ceil(text.length / 4);
  }

  private getModelSize(model: string): string {
    const info = this.availableModels.get(model);
    return info ? this.formatSize(info.size) : 'Unknown';
  }

  private getModelQuantization(model: string): string {
    const info = this.availableModels.get(model);
    return info?.details?.quantization_level || 'Unknown';
  }

  private formatSize(bytes: number): string {
    const units = ['B', 'KB', 'MB', 'GB', 'TB'];
    let size = bytes;
    let unitIndex = 0;
    
    while (size >= 1024 && unitIndex < units.length - 1) {
      size /= 1024;
      unitIndex++;
    }
    
    return `${size.toFixed(1)} ${units[unitIndex]}`;
  }

  private getDisplayName(modelName: string): string {
    const nameMap: { [key: string]: string } = {
      'llama3.2:3b': 'Llama 3.2 (3B)',
      'llama3.2:1b': 'Llama 3.2 (1B)',
      'llama3.2:latest': 'Llama 3.2 (3B)',
      'deepseek-coder:6.7b': 'DeepSeek Coder (6.7B)',
      'deepseek-coder:33b': 'DeepSeek Coder (33B)',
      'deepseek-coder-v2:16b': 'DeepSeek Coder v2 (16B)',
      'codellama:7b': 'Code Llama (7B)',
      'codellama:13b': 'Code Llama (13B)',
      'phi3:mini': 'Phi-3 Mini',
      'phi3:latest': 'Phi-3',
      'mistral:latest': 'Mistral 7B',
      'mistral:7b': 'Mistral 7B',
      'llama3.1:8b': 'Llama 3.1 (8B)',
      'llama3.1:70b': 'Llama 3.1 (70B)',
      'llama2:7b': 'Llama 2 (7B)',
      'llama2:13b': 'Llama 2 (13B)',
      'llama2:70b': 'Llama 2 (70B)',
      'mixtral:8x7b': 'Mixtral 8x7B',
      'magicoder:7b': 'MagiCoder (7B)',
      'starcoder2:15b': 'StarCoder2 (15B)',
      'qwen:7b': 'Qwen (7B)',
      'vicuna:7b': 'Vicuna (7B)',
      'nomic-embed-text:latest': 'Nomic Embed Text',
      'mxbai-embed-large:latest': 'MxBai Embed Large'
    };
    
    return nameMap[modelName] || modelName;
  }

  private getModelDescription(modelName: string): string {
    const descriptions: { [key: string]: string } = {
      'llama3.2:3b': 'ë²”ìš© ëŒ€í™” ë° í…ìŠ¤íŠ¸ ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸',
      'llama3.2:1b': 'ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê°„ë‹¨í•œ ì‘ì—…ìš© ê²½ëŸ‰ ëª¨ë¸',
      'deepseek-coder:6.7b': 'ì½”ë“œ ìƒì„±, ë””ë²„ê¹…, ì„¤ëª…ì— íŠ¹í™”ëœ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ ëª¨ë¸',
      'deepseek-coder:33b': 'ëŒ€ê·œëª¨ ì½”ë“œ í”„ë¡œì íŠ¸ì™€ ë³µì¡í•œ í”„ë¡œê·¸ë˜ë° ì‘ì—…ìš©',
      'codellama:7b': 'Metaì˜ ì½”ë“œ ìƒì„± ì „ë¬¸ ëª¨ë¸, ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì§€ì›',
      'codellama:13b': 'ë” í° ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ì™€ ë³µì¡í•œ ë¡œì§ ì²˜ë¦¬',
      'phi3:mini': 'ë…¼ë¦¬ì  ì¶”ë¡ ê³¼ ìˆ˜í•™ì  ë¬¸ì œ í•´ê²°ì— ê°•í•œ ì†Œí˜• ëª¨ë¸',
      'mistral:latest': 'íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ì‘ë‹µì„ ì œê³µí•˜ëŠ” ë²”ìš© ëª¨ë¸',
      'llama3.1:8b': 'í–¥ìƒëœ ì„±ëŠ¥ì˜ ì¤‘ê¸‰ ë²”ìš© ëª¨ë¸',
      'llama3.1:70b': 'ìµœê³  ì„±ëŠ¥ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ (ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì )',
      'mixtral:8x7b': 'ì „ë¬¸ê°€ í˜¼í•© ëª¨ë¸, ë‹¤ì–‘í•œ ì‘ì—…ì— íŠ¹í™”',
      'nomic-embed-text:latest': 'í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì „ìš© ëª¨ë¸',
      'mxbai-embed-large:latest': 'ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ëª¨ë¸'
    };
    
    return descriptions[modelName] || 'ë²”ìš© AI ëª¨ë¸';
  }

  /**
   * í´ë°± ì‘ë‹µ ìƒì„±
   */
  private generateFallbackResponse(
    message: string,
    model: string,
    processingTime: number
  ): AIResponse {
    const fallbackContent = `ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ${model} ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ìš”ì²­í•˜ì‹  ë©”ì‹œì§€:** "${message}"

**ìƒí™©:**
- Ollama ì„œë²„ ì—°ê²° í™•ì¸ í•„ìš”
- ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ
- ì„œë²„ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê°€ëŠ¥ì„±

**í•´ê²° ë°©ë²•:**
1. Ollama ì„œë²„ ìƒíƒœ í™•ì¸: \`ollama list\`
2. ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ: \`ollama run ${model}\`
3. ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„: llama3.2:3b ì¶”ì²œ

ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`;

    return {
      content: fallbackContent,
      model: `Fallback (${model})`,
      tokensUsed: fallbackContent.length,
      processingTime,
      confidence: 0.3,
      metadata: {
        error: 'Service unavailable',
        fallback: true
      }
    };
  }
}