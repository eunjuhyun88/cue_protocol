// ============================================================================
// ğŸ¦™ ì‹¤ì œ í”„ë¡œì íŠ¸ í˜¸í™˜ OllamaAIService - chat.tsì™€ ì™„ë²½ í˜¸í™˜
// íŒŒì¼: backend/src/services/ai/OllamaAIService.ts  
// íŠ¹ì§•: ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜ + generateOllamaResponse í•¨ìˆ˜ ì œê³µ
// ============================================================================

interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
  images?: string[];
}

interface OllamaResponse {
  model: string;
  created_at: string;
  message?: {
    role: string;
    content: string;
  };
  response?: string; // generate APIìš©
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

interface OllamaModelInfo {
  name: string;
  modified_at: string;
  size: number;
  digest: string;
  details?: {
    format?: string;
    family?: string;
    families?: string[];
    parameter_size?: string;
    quantization_level?: string;
  };
}

/**
 * ğŸ¦™ ì‹¤ì œ í”„ë¡œì íŠ¸ í˜¸í™˜ OllamaAIService
 * - ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜ (OllamaService í´ë˜ìŠ¤)
 * - chat.tsì—ì„œ ì‚¬ìš©í•˜ëŠ” generateOllamaResponse í•¨ìˆ˜ ì œê³µ
 * - ë¬´í•œë£¨í”„ ë°©ì§€ ì‹œìŠ¤í…œ
 * - DatabaseService ì•ˆì „í•œ ì—°ë™
 * - app.ts generateResponse ë©”ì„œë“œ ì§€ì›
 */
export class OllamaAIService {
  private static instance: OllamaAIService;
  private baseURL: string;
  private timeout: number;
  private retryCount: number;
  private defaultModel: string;
  private operationCount: number = 0;
  
  // ë¬´í•œë£¨í”„ ë°©ì§€ ì‹œìŠ¤í…œ
  private isConnecting: boolean = false;
  private isLoadingModels: boolean = false;
  private lastConnectionCheck: number = 0;
  private lastModelsCheck: number = 0;
  private connectionCooldown: number = 5000; // 5ì´ˆ
  private modelsCooldown: number = 10000; // 10ì´ˆ
  
  // ìƒíƒœ ê´€ë¦¬
  private isAvailable: boolean = false;
  private models: string[] = [];
  private availableModels: Map<string, OllamaModelInfo> = new Map();
  private modelConfigs: Map<string, any> = new Map();
  private isInitialized: boolean = false;
  private db: any = null; // DatabaseService (ì„ íƒì )
  private lastError: string | null = null;

  private constructor() {
    console.log('ğŸ¦™ === ì‹¤ì œ í”„ë¡œì íŠ¸ í˜¸í™˜ OllamaAIService ì´ˆê¸°í™” ===');
    
    this.baseURL = this.validateBaseURL();
    this.timeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000');
    this.retryCount = parseInt(process.env.OLLAMA_RETRY_COUNT || '3');
    this.defaultModel = process.env.OLLAMA_DEFAULT_MODEL || 'llama3.2:3b';
    
    console.log(`ğŸ”— Ollama ì„œë²„: ${this.baseURL}`);
    console.log(`âš™ï¸ ê¸°ë³¸ ëª¨ë¸: ${this.defaultModel}, íƒ€ì„ì•„ì›ƒ: ${this.timeout}ms`);
    
    // DatabaseService ì•ˆì „í•œ ì—°ë™
    this.initializeDatabaseConnection();
    
    // ëª¨ë¸ë³„ ì„¤ì • ì´ˆê¸°í™”
    this.initializeModelConfigs();
    
    // ë¹„ë™ê¸° ì´ˆê¸°í™” (ì•ˆì „í•˜ê²Œ)
    this.safeInitializeAsync();
  }

  public static getInstance(): OllamaAIService {
    if (!OllamaAIService.instance) {
      OllamaAIService.instance = new OllamaAIService();
    }
    return OllamaAIService.instance;
  }

  // ============================================================================
  // ğŸ”§ ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜ ë©”ì„œë“œë“¤
  // ============================================================================

  /**
   * ğŸ”§ Base URL ê²€ì¦ ë° ì„¤ì •
   */
  private validateBaseURL(): string {
    const possibleUrls = [
      process.env.OLLAMA_BASE_URL,
      process.env.OLLAMA_URL,
      process.env.OLLAMA_HOST,
      'http://localhost:11434'
    ];

    for (const url of possibleUrls) {
      if (url && url.trim() !== '') {
        const cleanUrl = url.trim().replace(/\/$/, ''); // ë§ˆì§€ë§‰ ìŠ¬ë˜ì‹œ ì œê±°
        console.log(`ğŸ” Ollama URL ì„¤ì •: ${cleanUrl}`);
        return cleanUrl;
      }
    }

    console.warn('âš ï¸ OLLAMA_BASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©');
    return 'http://localhost:11434';
  }

  /**
   * ğŸ”Œ ì—°ê²° ìƒíƒœ í™•ì¸ (ê¸°ì¡´ ollama.ts í˜¸í™˜)
   */
  async checkConnection(): Promise<boolean> {
    const now = Date.now();
    
    // ì¿¨ë‹¤ìš´ ì²´í¬
    if (now - this.lastConnectionCheck < this.connectionCooldown) {
      console.log('ğŸ”„ ì—°ê²° ì²´í¬ ì¿¨ë‹¤ìš´ ì¤‘... ìºì‹œëœ ê²°ê³¼ ë°˜í™˜');
      return this.isAvailable;
    }
    
    // ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
    if (this.isConnecting) {
      console.log('â³ ì´ë¯¸ ì—°ê²° ì²´í¬ ì¤‘... ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜');
      return this.isAvailable;
    }
    
    return await this.performSingleConnectionCheck();
  }

  /**
   * ğŸ’¬ ê¸°ë³¸ ì±„íŒ… ë©”ì„œë“œ (ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜)
   */
  async chat(
    model: string = 'llama3.2',
    messages: OllamaMessage[],
    stream: boolean = false
  ): Promise<string> {
    
    for (let attempt = 1; attempt <= this.retryCount; attempt++) {
      try {
        console.log(`ğŸ¦™ Ollama ì±„íŒ… ì‹œë„ ${attempt}/${this.retryCount} - ëª¨ë¸: ${model}`);
        
        if (!await this.checkConnection()) {
          throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
        }

        const result = await this.performChatRequest(model, messages, { stream });
        console.log(`âœ… Ollama ì±„íŒ… ì„±ê³µ (ì‹œë„ ${attempt})`);
        return result;

      } catch (error: any) {
        console.error(`âŒ Ollama ì±„íŒ… ì‹œë„ ${attempt} ì‹¤íŒ¨:`, error.message);
        
        if (attempt === this.retryCount) {
          throw error;
        }
        
        await this.delay(1000 * attempt);
      }
    }

    throw new Error('ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨');
  }

  /**
   * ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ê¸°ì¡´ ollama.ts í˜¸í™˜)
   */
  async getModels(): Promise<string[]> {
    const now = Date.now();
    
    // ì¿¨ë‹¤ìš´ ì²´í¬
    if (this.models.length > 0 && now - this.lastModelsCheck < this.modelsCooldown) {
      console.log('ğŸ”„ ëª¨ë¸ ëª©ë¡ ì¿¨ë‹¤ìš´ ì¤‘... ìºì‹œëœ ê²°ê³¼ ë°˜í™˜');
      return this.models;
    }
    
    // ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
    if (this.isLoadingModels) {
      console.log('â³ ì´ë¯¸ ëª¨ë¸ ë¡œë”© ì¤‘... ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜');
      return this.models;
    }
    
    await this.performSingleModelsLoad();
    return this.models;
  }

  /**
   * ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ ollama.ts í˜¸í™˜)
   */
  async pullModel(model: string): Promise<void> {
    try {
      console.log(`ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: ${model}`);
      
      const response = await fetch(`${this.baseURL}/api/pull`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name: model }),
        signal: AbortSignal.timeout(300000) // 5ë¶„ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`Failed to pull model: ${model}`);
      }

      console.log(`âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ${model}`);

    } catch (error: any) {
      const errorMessage = this.getErrorMessage(error);
      console.error(`âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ${model}`, errorMessage);
      throw new Error(`ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ${errorMessage}`);
    }
  }

  // ============================================================================
  // ğŸ¯ chat.tsì—ì„œ ì‚¬ìš©í•˜ëŠ” generateOllamaResponse í˜¸í™˜ ë©”ì„œë“œ
  // ============================================================================

  /**
   * ğŸ’¬ app.ts í˜¸í™˜ generateResponse ë©”ì„œë“œ (app.tsì—ì„œ í˜¸ì¶œ)
   * âœ… app.tsì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ì‹œê·¸ë‹ˆì²˜
   */
  async generateResponse(
    message: string,
    model: string = this.defaultModel,
    personalizedContext: any = {},
    userDid: string = 'anonymous',
    conversationId: string = `conv_${Date.now()}`
  ): Promise<{
    content: string;
    model: string;
    tokensUsed: number;
    processingTime: number;
    conversationId: string;
    metadata: any;
  }> {
    const startTime = Date.now();
    this.operationCount++;

    console.log(`ğŸ¦™ AI ì‘ë‹µ ìƒì„± ì‹œì‘ [${this.operationCount}]: ${model}`);
    console.log(`ğŸ“ ë©”ì‹œì§€: "${message.substring(0, 100)}..."`);
    console.log(`ğŸ‘¤ ì‚¬ìš©ì: ${userDid}, ëŒ€í™”: ${conversationId}`);

    try {
      // ë‹¨ì¼ ì—°ê²° í™•ì¸ (ë¬´í•œë£¨í”„ ë°©ì§€)
      const connected = await this.checkConnection();
      if (!connected) {
        throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `ollama serve` ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.');
      }

      // ê°œì¸í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
      const systemPrompt = this.createPersonalizedSystemPrompt(personalizedContext, userDid);
      
      // ë©”ì‹œì§€ êµ¬ì„±
      const messages: OllamaMessage[] = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: message }
      ];
      
      // Ollama API í˜¸ì¶œ (ê¸°ì¡´ chat ë©”ì„œë“œ í™œìš©)
      const aiResponseContent = await this.chat(model, messages, false);
      const processingTime = Date.now() - startTime;
      
      // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
      const estimatedTokens = this.estimateTokenUsage(message, aiResponseContent);

      // app.ts í˜¸í™˜ ì‘ë‹µ ê°ì²´ ìƒì„±
      const result = {
        content: aiResponseContent,
        model,
        tokensUsed: estimatedTokens,
        processingTime,
        conversationId,
        metadata: {
          userDid,
          systemPromptUsed: !!systemPrompt,
          personalizedContext: !!personalizedContext.personalityProfile,
          cuesUsed: personalizedContext.cues?.length || 0,
          operationId: this.operationCount,
          timestamp: new Date().toISOString(),
          promptTokens: this.estimateTokensFromText(message),
          completionTokens: this.estimateTokensFromText(aiResponseContent),
          provider: 'ollama',
          local: true,
          privacy: 'fully_local'
        }
      };

      // DatabaseServiceë¥¼ í†µí•œ ëŒ€í™” ì €ì¥ (ì•ˆì „í•˜ê²Œ)
      if (userDid && this.db) {
        try {
          await this.saveChatToDatabase(userDid, message, result, conversationId);
        } catch (dbError) {
          console.warn('âš ï¸ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨ (ê¸°ëŠ¥ì€ ê³„ì†ë¨):', dbError);
        }
      }

      console.log(`âœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ [${processingTime}ms]: ${estimatedTokens} tokens`);
      return result;

    } catch (error: any) {
      const processingTime = Date.now() - startTime;
      const errorMessage = this.getErrorMessage(error);
      
      console.error(`âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ [${processingTime}ms]:`, errorMessage);
      this.lastError = errorMessage;

      // ì—ëŸ¬ ì‹œì—ë„ êµ¬ì¡°ì  ì‘ë‹µ ë°˜í™˜ (app.ts í˜¸í™˜)
      return {
        content: `ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (${errorMessage})`,
        model,
        tokensUsed: 0,
        processingTime,
        conversationId,
        metadata: {
          userDid,
          error: errorMessage,
          operationId: this.operationCount,
          timestamp: new Date().toISOString(),
          fallback: true
        }
      };
    }
  }

  // ============================================================================
  // ğŸ“Š chat.ts í˜¸í™˜ ì „ìš© í•¨ìˆ˜ (generateOllamaResponseì™€ ê°™ì€ í˜•íƒœ)
  // ============================================================================

  /**
   * ğŸ”§ ê°œì¸í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (chat.ts í˜¸í™˜)
   */
  private createPersonalizedSystemPrompt(context: any = {}, userDid: string): string {
    const { personalityProfile, cues, behaviorPatterns } = context;
    
    let prompt = `ë‹¹ì‹ ì€ AI Personal Assistantì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê°œì¸ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.

ê¸°ë³¸ ì§€ì¹¨:
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”
- ì‚¬ìš©ìì˜ ê°œì¸ ë°ì´í„°ì™€ íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ê°œì¸í™”ëœ ì‘ë‹µì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”`;

    // ê°œì„± í”„ë¡œí•„ ì¶”ê°€
    if (personalityProfile?.type) {
      prompt += `\n\nì‚¬ìš©ì ì„±ê²© íƒ€ì…: ${personalityProfile.type}`;
      if (personalityProfile.traits?.length > 0) {
        prompt += `\nì£¼ìš” íŠ¹ì„±: ${personalityProfile.traits.join(', ')}`;
      }
    }

    // ê°œì¸ í ë°ì´í„° ì¶”ê°€
    if (cues?.length > 0) {
      const recentCues = cues.slice(0, 5); // ìµœê·¼ 5ê°œë§Œ ì‚¬ìš©
      prompt += `\n\nì‚¬ìš©ìì˜ ìµœê·¼ ê´€ì‹¬ì‚¬ ë° íŒ¨í„´:`;
      recentCues.forEach((cue: any, index: number) => {
        prompt += `\n${index + 1}. ${cue.content || cue.text}`;
      });
    }

    // í–‰ë™ íŒ¨í„´ ì¶”ê°€
    if (behaviorPatterns?.length > 0) {
      prompt += `\n\nì‚¬ìš©ì í–‰ë™ íŒ¨í„´: ${behaviorPatterns.join(', ')}`;
    }

    prompt += `\n\nì‚¬ìš©ì ID: ${userDid}`;
    
    return prompt;
  }

  // ============================================================================
  // ğŸ”§ ì•ˆì „í•œ ì´ˆê¸°í™” ë©”ì„œë“œë“¤ (ë¬´í•œë£¨í”„ ë°©ì§€)
  // ============================================================================

  /**
   * DatabaseService ì•ˆì „í•œ ì—°ë™ (DI Container í†µí•©)
   */
  private initializeDatabaseConnection(): void {
    try {
      // í™˜ê²½ ë³€ìˆ˜ë¡œ DI Container ì‚¬ìš© ì—¬ë¶€ í™•ì¸
      if (process.env.USE_DI_CONTAINER !== 'false') {
        // DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì—°ê²° ì‹œë„
        const DIContainer = require('../../core/DIContainer');
        const container = DIContainer.DIContainer?.getInstance?.();
        
        if (container && typeof container.get === 'function') {
          try {
            this.db = container.get('DatabaseService');
            console.log(`ğŸ—„ï¸ DatabaseService ì—°ë™: ${this.db?.isConnected?.() ? 'âœ…' : 'âš ï¸'} (DI)`);
          } catch (diError) {
            console.log('ğŸ—„ï¸ DatabaseService ì—°ë™: âš ï¸ (DI ì‹¤íŒ¨, ì„ íƒì  ê¸°ëŠ¥)');
            this.db = null;
          }
        }
      }
    } catch (error) {
      console.log('ğŸ—„ï¸ DatabaseService ì—°ë™: âš ï¸ (ì„ íƒì  ê¸°ëŠ¥)');
      this.db = null;
    }
  }

  /**
   * ì•ˆì „í•œ ë¹„ë™ê¸° ì´ˆê¸°í™” (ë¬´í•œë£¨í”„ ë°©ì§€)
   */
  private async safeInitializeAsync(): Promise<void> {
    try {
      console.log('ğŸ” Ollama ì´ˆê¸° ì—°ê²° í™•ì¸ ì¤‘...');
      
      // ë‹¨ì¼ ì—°ê²° ì‹œë„ (ì¬ê·€ í˜¸ì¶œ ì—†ìŒ)
      const isConnected = await this.performSingleConnectionCheck();
      
      if (isConnected) {
        console.log('âœ… Ollama ì´ˆê¸° ì—°ê²° ì„±ê³µ');
        
        // ëª¨ë¸ ë¡œë”©ë„ ë‹¨ì¼ ì‹œë„
        await this.performSingleModelsLoad();
        console.log(`âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: ${this.models.length} ê°œ ëª¨ë¸`);
        
        this.isAvailable = true;
      } else {
        console.warn('âš ï¸ Ollama ì´ˆê¸° ì—°ê²° ì‹¤íŒ¨ - í´ë°± ëª¨ë“œë¡œ ë™ì‘');
        this.printConnectionHelp();
        this.isAvailable = false;
      }
      
      this.isInitialized = true;
    } catch (error: any) {
      console.warn('âš ï¸ Ollama ì´ˆê¸°í™” ì‹¤íŒ¨:', error.message);
      this.printConnectionHelp();
      this.isAvailable = false;
      this.isInitialized = true; // ì´ˆê¸°í™” ì™„ë£Œë¡œ í‘œì‹œ (ë¬´í•œ ëŒ€ê¸° ë°©ì§€)
    }
  }

  /**
   * ì—°ê²° ë„ì›€ë§ ì¶œë ¥
   */
  private printConnectionHelp(): void {
    console.log('\nğŸ’¡ Ollama ì„¤ì • ë„ì›€ë§:');
    console.log('1. Ollama ì„¤ì¹˜: brew install ollama');
    console.log('2. ì„œë²„ ì‹œì‘: ollama serve');
    console.log('3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b');
    console.log('4. ëª¨ë¸ í™•ì¸: ollama list');
    console.log(`5. ì—°ê²° í™•ì¸: curl ${this.baseURL}/api/tags\n`);
  }

  /**
   * ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì • ì´ˆê¸°í™” (ì „ì²´ Ollama ëª¨ë¸ ì§€ì›)
   */
  private initializeModelConfigs(): void {
    const configs = {
      // ğŸ¦™ Llama ëª¨ë¸êµ°
      'llama3.2:3b': { type: 'chat', temperature: 0.7, max_tokens: 2048, recommended: true },
      'llama3.2:1b': { type: 'chat', temperature: 0.8, max_tokens: 1024 },
      'llama3.2:latest': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'llama3.1:8b': { type: 'chat', temperature: 0.7, max_tokens: 4096 },
      'llama3.1:70b': { type: 'chat', temperature: 0.6, max_tokens: 8192 },
      'llama2:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'llama2:13b': { type: 'chat', temperature: 0.6, max_tokens: 4096 },
      'llama2:70b': { type: 'chat', temperature: 0.5, max_tokens: 8192 },

      // ğŸ’» ì½”ë”© ì „ë¬¸ ëª¨ë¸êµ°
      'deepseek-coder:6.7b': { type: 'code', temperature: 0.3, max_tokens: 4096, recommended: true },
      'deepseek-coder:33b': { type: 'code', temperature: 0.2, max_tokens: 8192 },
      'deepseek-coder-v2:16b': { type: 'code', temperature: 0.3, max_tokens: 6144 },
      'codellama:7b': { type: 'code', temperature: 0.2, max_tokens: 4096 },
      'codellama:13b': { type: 'code', temperature: 0.2, max_tokens: 6144 },
      'magicoder:7b': { type: 'code', temperature: 0.3, max_tokens: 4096 },
      'starcoder2:15b': { type: 'code', temperature: 0.2, max_tokens: 8192 },

      // ğŸ§  ì¶”ë¡ /ë…¼ë¦¬ ëª¨ë¸êµ°
      'phi3:mini': { type: 'reasoning', temperature: 0.5, max_tokens: 2048, recommended: true },
      'phi3:latest': { type: 'reasoning', temperature: 0.5, max_tokens: 2048 },
      'phi:2.7b': { type: 'reasoning', temperature: 0.6, max_tokens: 2048 },

      // ğŸ­ ë²”ìš© ëŒ€í™” ëª¨ë¸êµ°
      'mistral:latest': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'mistral:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'mixtral:8x7b': { type: 'chat', temperature: 0.6, max_tokens: 4096 },
      'vicuna:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'qwen:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },

      // ğŸ”— ì„ë² ë”© ëª¨ë¸êµ°
      'nomic-embed-text:latest': { type: 'embedding', temperature: 0.0, max_tokens: 512 },
      'mxbai-embed-large:latest': { type: 'embedding', temperature: 0.0, max_tokens: 512 }
    };

    Object.entries(configs).forEach(([model, config]) => {
      this.modelConfigs.set(model, config);
    });

    console.log(`âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: ${this.modelConfigs.size} ê°œ ëª¨ë¸ (ì „ì²´ Ollama ì§€ì›)`);
    console.log(`ğŸ¯ ì¶”ì²œ ëª¨ë¸: ${Object.entries(configs).filter(([_, config]) => config.recommended).map(([name]) => name).join(', ')}`);
  }

  // ============================================================================
  // ğŸ”§ ë‚´ë¶€ ë©”ì„œë“œë“¤ (ì•ˆì „í•œ êµ¬í˜„)
  // ============================================================================

  /**
   * ë‹¨ì¼ ì—°ê²° í™•ì¸ ìˆ˜í–‰ (ì¬ê·€ í˜¸ì¶œ ì—†ìŒ)
   */
  private async performSingleConnectionCheck(): Promise<boolean> {
    this.isConnecting = true;
    this.lastConnectionCheck = Date.now();
    
    try {
      console.log(`ğŸ” Ollama ì—°ê²° í™•ì¸ ì¤‘: ${this.baseURL}`);
      
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000);
      
      const response = await fetch(`${this.baseURL}/api/tags`, {
        signal: controller.signal,
        headers: { 'Content-Type': 'application/json' }
      });
      
      clearTimeout(timeoutId);
      
      if (!response.ok) {
        console.warn(`âš ï¸ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: ${response.status}`);
        this.isAvailable = false;
        return false;
      }
      
      const data = await response.json();
      console.log(`âœ… Ollama ì—°ê²° ì„±ê³µ, ëª¨ë¸ ìˆ˜: ${data.models?.length || 0}`);
      this.isAvailable = true;
      this.lastError = null;
      return true;
      
    } catch (error: any) {
      if (error.name === 'AbortError') {
        console.warn('âš ï¸ Ollama ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ (5ì´ˆ)');
      } else if (error.message?.includes('ECONNREFUSED')) {
        console.warn('âš ï¸ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ. `ollama serve` ëª…ë ¹ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.');
      } else {
        console.warn(`âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: ${error.message}`);
      }
      this.isAvailable = false;
      this.lastError = this.getErrorMessage(error);
      return false;
      
    } finally {
      this.isConnecting = false;
    }
  }

  /**
   * ë‹¨ì¼ ëª¨ë¸ ë¡œë”© ìˆ˜í–‰ (ì¬ê·€ í˜¸ì¶œ ì—†ìŒ)
   */
  private async performSingleModelsLoad(): Promise<void> {
    this.isLoadingModels = true;
    this.lastModelsCheck = Date.now();
    
    try {
      // ì—°ê²° ìƒíƒœê°€ í™•ì‹¤íˆ ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë°°ì—´ ë°˜í™˜
      if (!this.isAvailable) {
        console.warn('âš ï¸ Ollama ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€ - ë¹ˆ ëª¨ë¸ ëª©ë¡ ë°˜í™˜');
        this.models = [];
        return;
      }

      const response = await fetch(`${this.baseURL}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: AbortSignal.timeout(10000)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data: unknown = await response.json();
      const modelsData = (data as any).models || [];
      
      this.availableModels.clear();
      this.models = [];
      
      modelsData.forEach((model: OllamaModelInfo) => {
        this.availableModels.set(model.name, model);
        this.models.push(model.name);
      });
      
      console.log(`ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ (${this.models.length}ê°œ):`, this.models);

    } catch (error: any) {
      console.error('âŒ ëª¨ë¸ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨:', error.message);
      this.models = [];
    } finally {
      this.isLoadingModels = false;
    }
  }

  /**
   * ì‹¤ì œ ì±„íŒ… ìš”ì²­ ìˆ˜í–‰ (ê¸°ì¡´ í˜¸í™˜)
   */
  private async performChatRequest(
    model: string,
    messages: OllamaMessage[],
    options: {
      temperature?: number;
      stream?: boolean;
      personalizedContext?: any;
    }
  ): Promise<string> {
    
    const { temperature = 0.7, stream = false, personalizedContext } = options;

    const requestBody = {
      model,
      messages: this.formatMessages(messages, personalizedContext),
      stream,
      options: {
        temperature,
        num_predict: 2000,
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(requestBody),
      signal: AbortSignal.timeout(this.timeout)
    });

    if (!response.ok) {
      const errorText = await response.text();
      let errorMessage = `HTTP ${response.status}: ${response.statusText}`;
      
      try {
        const errorJson = JSON.parse(errorText);
        errorMessage = errorJson.error || errorMessage;
      } catch {
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
      }
      
      throw new Error(errorMessage);
    }

    const data: OllamaResponse = await response.json();
    
    if (!data.done) {
      throw new Error('Ollama ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    const content = data.message?.content || data.response || '';
    
    if (!content.trim()) {
      throw new Error('Ollamaì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤');
    }

    return content;
  }

  /**
   * DatabaseServiceë¥¼ í†µí•œ ì•ˆì „í•œ ëŒ€í™” ì €ì¥
   */
  private async saveChatToDatabase(
    userId: string,
    userMessage: string,
    aiResponse: any,
    conversationId?: string
  ): Promise<void> {
    try {
      if (!this.db || typeof this.db.saveChatMessage !== 'function') {
        return; // DB ì„œë¹„ìŠ¤ ì—†ìœ¼ë©´ ìŠ¤í‚µ
      }

      // ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
      await this.db.saveChatMessage({
        conversation_id: conversationId || 'default',
        user_id: userId,
        content: userMessage,
        sender: 'user',
        model: null,
        tokens_used: aiResponse.metadata?.promptTokens || 0,
        timestamp: new Date().toISOString()
      });

      // AI ì‘ë‹µ ì €ì¥
      await this.db.saveChatMessage({
        conversation_id: conversationId || 'default',
        user_id: userId,
        content: aiResponse.content,
        sender: 'assistant',
        model: aiResponse.model,
        tokens_used: aiResponse.metadata?.completionTokens || 0,
        processing_time: aiResponse.processingTime,
        timestamp: new Date().toISOString()
      });

      console.log('âœ… ëŒ€í™” ì €ì¥ ì™„ë£Œ');
    } catch (error) {
      console.warn('âš ï¸ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨:', error);
    }
  }

  // ============================================================================
  // ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  // ============================================================================

  /**
   * ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
   */
  private estimateTokenUsage(input: string, output: string): number {
    const inputTokens = this.estimateTokensFromText(input);
    const outputTokens = this.estimateTokensFromText(output);
    return inputTokens + outputTokens;
  }

  /**
   * ğŸ“ í…ìŠ¤íŠ¸ì—ì„œ í† í° ìˆ˜ ì¶”ì •
   */
  private estimateTokensFromText(text: string): number {
    if (!text) return 0;
    
    // ì˜ì–´ëŠ” 4ìë‹¹ 1í† í°, í•œêµ­ì–´ëŠ” 2ìë‹¹ 1í† í°ìœ¼ë¡œ ê·¼ì‚¬
    const englishChars = (text.match(/[a-zA-Z\s]/g) || []).length;
    const koreanChars = (text.match(/[ã„±-ã…ê°€-í£]/g) || []).length;
    const otherChars = text.length - englishChars - koreanChars;
    
    return Math.ceil(englishChars / 4) + Math.ceil(koreanChars / 2) + Math.ceil(otherChars / 3);
  }

  /**
   * ğŸ”§ ì—ëŸ¬ ë©”ì‹œì§€ ì •ë¦¬
   */
  private getErrorMessage(error: any): string {
    if (typeof error === 'string') return error;
    if (error?.message) return error.message;
    if (error?.response?.data?.message) return error.response.data.message;
    if (error?.response?.statusText) return error.response.statusText;
    return 'Unknown error occurred';
  }

  private formatMessages(messages: OllamaMessage[], personalizedContext?: any): OllamaMessage[] {
    const formattedMessages: OllamaMessage[] = [];

    if (personalizedContext) {
      formattedMessages.push({
        role: 'system',
        content: this.formatPersonalizedContext(personalizedContext)
      });
    }

    formattedMessages.push(...messages);
    return formattedMessages;
  }

  private formatPersonalizedContext(context: any): string {
    if (!context || Object.keys(context).length === 0) {
      return 'ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.';
    }

    let contextPrompt = 'ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n';
    contextPrompt += 'ì‚¬ìš©ìì— ëŒ€í•œ ì •ë³´:\n';

    if (context.personalityProfile) {
      contextPrompt += `- ì„±ê²©: ${context.personalityProfile.type || 'Adaptive'}\n`;
      contextPrompt += `- ì†Œí†µ ìŠ¤íƒ€ì¼: ${context.personalityProfile.communicationStyle || 'Balanced'}\n`;
    }

    if (context.behaviorPatterns && context.behaviorPatterns.length > 0) {
      contextPrompt += `- ê´€ì‹¬ì‚¬: ${context.behaviorPatterns.slice(0, 3).join(', ')}\n`;
    }

    contextPrompt += '\nì‚¬ìš©ìì˜ ì„±ê²©ê³¼ ì„ í˜¸ë„ì— ë§ì¶° ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.\n';
    return contextPrompt;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ
   */
  getStats(): {
    operationCount: number;
    baseURL: string;
    defaultModel: string;
    timeout: number;
    lastError: string | null;
    available: boolean;
    modelCount: number;
  } {
    return {
      operationCount: this.operationCount,
      baseURL: this.baseURL,
      defaultModel: this.defaultModel,
      timeout: this.timeout,
      lastError: this.lastError,
      available: this.isAvailable,
      modelCount: this.models.length
    };
  }

  /**
   * ğŸ“Š ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ (app.ts í˜¸í™˜)
   */
  async getServiceStatus(): Promise<any> {
    const timestamp = new Date().toISOString();
    
    try {
      const connected = await this.checkConnection();
      
      if (!connected) {
        return {
          connected: false,
          baseUrl: this.baseURL,
          models: [],
          status: 'offline',
          error: this.lastError || 'Connection failed',
          timestamp,
          database: {
            connected: this.db?.isConnected?.() || false,
            available: !!this.db
          }
        };
      }

      const models = await this.getModels();
      
      return {
        connected: true,
        baseUrl: this.baseURL,
        models,
        status: 'ready',
        timestamp,
        database: {
          connected: this.db?.isConnected?.() || false,
          available: !!this.db
        },
        features: [
          'chat', 
          'completion', 
          'local', 
          'privacy-focused',
          'conversation_storage',
          'personalization_support'
        ]
      };

    } catch (error: any) {
      const errorMessage = this.getErrorMessage(error);
      this.lastError = errorMessage;

      return {
        connected: false,
        baseUrl: this.baseURL,
        models: [],
        status: 'error',
        error: errorMessage,
        timestamp,
        database: {
          connected: false,
          available: !!this.db
        }
      };
    }
  }

  /**
   * ğŸ§¹ ì„œë¹„ìŠ¤ ì •ë¦¬ (DI Containerìš©)
   */
  public dispose(): void {
    console.log('ğŸ§¹ OllamaAIService ì •ë¦¬ ì¤‘...');
    this.isInitialized = false;
    this.models = [];
    this.availableModels.clear();
    this.isAvailable = false;
    this.isConnecting = false;
    this.isLoadingModels = false;
    this.lastConnectionCheck = 0;
    this.lastModelsCheck = 0;
    this.operationCount = 0;
    this.lastError = null;
    console.log('âœ… OllamaAIService ì •ë¦¬ ì™„ë£Œ');
  }
}

// ============================================================================
// ğŸ“¤ Export (ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜)
// ============================================================================

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¸°ì¡´ ollama.tsì™€ ë™ì¼í•œ ë°©ì‹)
const ollamaService = OllamaAIService.getInstance();

// ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜ í•¨ìˆ˜ë“¤
export const checkConnection = () => ollamaService.checkConnection();
export const getModels = () => ollamaService.getModels();

// âœ… chat.tsì—ì„œ í•„ìš”í•œ generateOllamaResponse í•¨ìˆ˜ ì œê³µ
export async function generateOllamaResponse(message: string, context: any): Promise<{
  response: string;
  tokensUsed: number;
  usedData: any[];
}> {
  try {
    console.log('ğŸ¦™ generateOllamaResponse í˜¸ì¶œë¨ (chat.ts í˜¸í™˜)');
    
    // ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
    const model = process.env.OLLAMA_DEFAULT_MODEL || 'llama3.2:3b';
    
    // OllamaAIServiceì˜ generateResponse ë©”ì„œë“œ í™œìš©
    const result = await ollamaService.generateResponse(
      message,
      model,
      context,
      context.userDid || 'anonymous',
      context.conversationId || `conv_${Date.now()}`
    );

    // chat.tsì—ì„œ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜
    return {
      response: result.content,
      tokensUsed: result.tokensUsed,
      usedData: context.cues || []
    };

  } catch (error: any) {
    console.error('âŒ generateOllamaResponse ì‹¤íŒ¨:', error.message);
    
    // ì—ëŸ¬ ì‹œ í´ë°± ì‘ë‹µ
    return {
      response: `ì£„ì†¡í•©ë‹ˆë‹¤. Ollama AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n1. \`ollama serve\` ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘\n2. \`ollama pull llama3.2:3b\` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n3. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸\n\në¬¸ì˜: "${message}"`,
      tokensUsed: 0,
      usedData: []
    };
  }
}

// ê¸°ì¡´ ollama.ts í˜¸í™˜ì„ ìœ„í•œ chat í•¨ìˆ˜
export const chat = (model: string, messages: OllamaMessage[], stream: boolean = false) => 
  ollamaService.chat(model, messages, stream);

// í´ë˜ìŠ¤ì™€ ì¸ìŠ¤í„´ìŠ¤ export
export { ollamaService, OllamaAIService };
export default OllamaAIService;

// ============================================================================
// ğŸ‰ ì‹¤ì œ í”„ë¡œì íŠ¸ í˜¸í™˜ ì™„ë£Œ ë¡œê·¸
// ============================================================================

console.log('âœ… ì‹¤ì œ í”„ë¡œì íŠ¸ í˜¸í™˜ Ollama AI ì„œë¹„ìŠ¤ ë¡œë“œë¨');
console.log('  ğŸ”— ê¸°ì¡´ ollama.ts ì™„ì „ í˜¸í™˜');
console.log('  ğŸ¯ chat.ts generateOllamaResponse í•¨ìˆ˜ ì œê³µ');
console.log('  âœ… app.ts generateResponse ë©”ì„œë“œ í˜¸í™˜');
console.log('  ğŸ› ë¬´í•œë£¨í”„ ì™„ì „ ë°©ì§€');
console.log('  ğŸ—„ï¸ DatabaseService ì•ˆì „í•œ ì—°ê²°');
console.log('  ğŸ’ª ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€');
console.log('  ğŸ›¡ï¸ ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬');