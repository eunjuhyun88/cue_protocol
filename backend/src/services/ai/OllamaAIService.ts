// ============================================================================
// ğŸ¦™ ì™„ì „íˆ ìˆ˜ì •ëœ Ollama AI ì„œë¹„ìŠ¤ (ë¬´í•œë£¨í”„ í•´ê²° + ì „ì²´ ê¸°ëŠ¥)
// ê²½ë¡œ: backend/src/services/ai/OllamaAIService.ts
// ìš©ë„: Ollama ì „ìš© AI ì„œë¹„ìŠ¤ + DatabaseService í†µí•© + ì•ˆì •ì„± ë³´ì¥
// ìˆ˜ì •ì‚¬í•­: ë¬´í•œë£¨í”„ ë°©ì§€, DatabaseService ì˜¬ë°”ë¥¸ ì—°ê²°, ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
// ============================================================================

interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
  images?: string[];
}

interface OllamaResponse {
  model: string;
  created_at: string;
  message?: {
    role: string;
    content: string;
  };
  response?: string; // generate APIìš©
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

interface OllamaModelInfo {
  name: string;
  modified_at: string;
  size: number;
  digest: string;
  details?: {
    format?: string;
    family?: string;
    families?: string[];
    parameter_size?: string;
    quantization_level?: string;
  };
}

export interface AIResponse {
  content: string;
  model: string;
  tokensUsed?: number;
  processingTime?: number;
  confidence?: number;
  provider?: string;
  local?: boolean;
  privacy?: string;
  metadata?: {
    promptTokens?: number;
    completionTokens?: number;
    modelSize?: string;
    quantization?: string;
    error?: string;
    fallback?: boolean;
    conversationId?: string;
    messageId?: string;
  };
}

export interface AIModel {
  id: string;
  name: string;
  available: boolean;
  type: 'chat' | 'code' | 'reasoning' | 'embedding';
  size: string;
  description: string;
  recommended?: boolean;
}

/**
 * ì™„ì „íˆ ìˆ˜ì •ëœ Ollama AI ì„œë¹„ìŠ¤
 * - ë¬´í•œë£¨í”„ ì™„ì „ ë°©ì§€
 * - DatabaseService ì•ˆì „í•œ ì—°ê²°
 * - ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ ì‹œìŠ¤í…œ
 * - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬
 * - ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€
 */
export class OllamaAIService {
  private static instance: OllamaAIService;
  private baseURL: string;
  private timeout: number;
  private retryCount: number;
  
  // ë¬´í•œë£¨í”„ ë°©ì§€ ì‹œìŠ¤í…œ
  private isConnecting: boolean = false;
  private isLoadingModels: boolean = false;
  private lastConnectionCheck: number = 0;
  private lastModelsCheck: number = 0;
  private connectionCooldown: number = 5000; // 5ì´ˆ
  private modelsCooldown: number = 10000; // 10ì´ˆ
  
  // ìƒíƒœ ê´€ë¦¬
  private isAvailable: boolean = false;
  private models: string[] = [];
  private availableModels: Map<string, OllamaModelInfo> = new Map();
  private modelConfigs: Map<string, any> = new Map();
  private isInitialized: boolean = false;
  private db: any = null; // DatabaseService (ì„ íƒì )

  private constructor() {
    console.log('ğŸ¦™ === OllamaAIService ì´ˆê¸°í™” (ë¬´í•œë£¨í”„ ë°©ì§€ + ì™„ì „ ê¸°ëŠ¥) ===');
    
    this.baseURL = process.env.OLLAMA_BASE_URL || 
                   process.env.OLLAMA_HOST || 
                   process.env.OLLAMA_URL || 
                   'http://localhost:11434';
    this.timeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000');
    this.retryCount = parseInt(process.env.OLLAMA_RETRY_COUNT || '3');
    
    console.log(`ğŸ”— Ollama ì„œë²„: ${this.baseURL}`);
    
    // DatabaseService ì•ˆì „í•œ ì—°ë™
    this.initializeDatabaseConnection();
    
    // ëª¨ë¸ë³„ ì„¤ì • ì´ˆê¸°í™”
    this.initializeModelConfigs();
    
    // ë¹„ë™ê¸° ì´ˆê¸°í™” (ì•ˆì „í•˜ê²Œ)
    this.safeInitializeAsync();
  }

  public static getInstance(): OllamaAIService {
    if (!OllamaAIService.instance) {
      OllamaAIService.instance = new OllamaAIService();
    }
    return OllamaAIService.instance;
  }

  // ============================================================================
  // ğŸ”§ ì•ˆì „í•œ ì´ˆê¸°í™” ë©”ì„œë“œë“¤ (ë¬´í•œë£¨í”„ ë°©ì§€)
  // ============================================================================

  /**
   * DatabaseService ì•ˆì „í•œ ì—°ë™ (DI Container í†µí•©)
   */
  private initializeDatabaseConnection(): void {
    try {
      // í™˜ê²½ ë³€ìˆ˜ë¡œ DI Container ì‚¬ìš© ì—¬ë¶€ í™•ì¸
      if (process.env.USE_DI_CONTAINER !== 'false') {
        // DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì—°ê²° ì‹œë„
        const DIContainer = require('../../core/DIContainer');
        const container = DIContainer.DIContainer?.getInstance?.();
        
        if (container && typeof container.get === 'function') {
          // DatabaseService ëŒ€ì‹  ActiveDatabaseServiceë¥¼ ì‹œë„í•˜ë˜, ì—†ìœ¼ë©´ DatabaseService ì‚¬ìš©
          try {
            this.db = container.get('DatabaseService'); // âœ… ìˆ˜ì •: ActiveDatabaseService â†’ DatabaseService
            console.log(`ğŸ—„ï¸ DatabaseService ì—°ë™: ${this.db?.isConnected?.() ? 'âœ…' : 'âš ï¸'} (DI)`);
          } catch (diError) {
            console.log('ğŸ—„ï¸ DatabaseService ì—°ë™: âš ï¸ (DI ì‹¤íŒ¨, ì„ íƒì  ê¸°ëŠ¥)');
            this.db = null;
          }
        }
      }
    } catch (error) {
      console.log('ğŸ—„ï¸ DatabaseService ì—°ë™: âš ï¸ (ì„ íƒì  ê¸°ëŠ¥)');
      this.db = null;
    }
  }

  /**
   * ì•ˆì „í•œ ë¹„ë™ê¸° ì´ˆê¸°í™” (ë¬´í•œë£¨í”„ ë°©ì§€)
   */
  private async safeInitializeAsync(): Promise<void> {
    try {
      console.log('ğŸ” Ollama ì´ˆê¸° ì—°ê²° í™•ì¸ ì¤‘...');
      
      // ë‹¨ì¼ ì—°ê²° ì‹œë„ (ì¬ê·€ í˜¸ì¶œ ì—†ìŒ)
      const isConnected = await this.performSingleConnectionCheck();
      
      if (isConnected) {
        console.log('âœ… Ollama ì´ˆê¸° ì—°ê²° ì„±ê³µ');
        
        // ëª¨ë¸ ë¡œë”©ë„ ë‹¨ì¼ ì‹œë„
        await this.performSingleModelsLoad();
        console.log(`âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: ${this.models.length} ê°œ ëª¨ë¸`);
        
        this.isAvailable = true;
      } else {
        console.warn('âš ï¸ Ollama ì´ˆê¸° ì—°ê²° ì‹¤íŒ¨ - í´ë°± ëª¨ë“œë¡œ ë™ì‘');
        this.printConnectionHelp();
        this.isAvailable = false;
      }
      
      this.isInitialized = true;
    } catch (error: any) {
      console.warn('âš ï¸ Ollama ì´ˆê¸°í™” ì‹¤íŒ¨:', error.message);
      this.printConnectionHelp();
      this.isAvailable = false;
      this.isInitialized = true; // ì´ˆê¸°í™” ì™„ë£Œë¡œ í‘œì‹œ (ë¬´í•œ ëŒ€ê¸° ë°©ì§€)
    }
  }

  /**
   * ì—°ê²° ë„ì›€ë§ ì¶œë ¥
   */
  private printConnectionHelp(): void {
    console.log('\nğŸ’¡ Ollama ì„¤ì • ë„ì›€ë§:');
    console.log('1. Ollama ì„¤ì¹˜: brew install ollama');
    console.log('2. ì„œë²„ ì‹œì‘: ollama serve');
    console.log('3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b');
    console.log('4. ëª¨ë¸ í™•ì¸: ollama list');
    console.log(`5. ì—°ê²° í™•ì¸: curl ${this.baseURL}/api/tags\n`);
  }

  /**
   * ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì • ì´ˆê¸°í™” (ì „ì²´ Ollama ëª¨ë¸ ì§€ì›)
   */
  private initializeModelConfigs(): void {
    const configs = {
      // ğŸ¦™ Llama ëª¨ë¸êµ°
      'llama3.2:3b': { type: 'chat', temperature: 0.7, max_tokens: 2048, recommended: true },
      'llama3.2:1b': { type: 'chat', temperature: 0.8, max_tokens: 1024 },
      'llama3.2:latest': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'llama3.1:8b': { type: 'chat', temperature: 0.7, max_tokens: 4096 },
      'llama3.1:70b': { type: 'chat', temperature: 0.6, max_tokens: 8192 },
      'llama2:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'llama2:13b': { type: 'chat', temperature: 0.6, max_tokens: 4096 },
      'llama2:70b': { type: 'chat', temperature: 0.5, max_tokens: 8192 },

      // ğŸ’» ì½”ë”© ì „ë¬¸ ëª¨ë¸êµ°
      'deepseek-coder:6.7b': { type: 'code', temperature: 0.3, max_tokens: 4096, recommended: true },
      'deepseek-coder:33b': { type: 'code', temperature: 0.2, max_tokens: 8192 },
      'deepseek-coder-v2:16b': { type: 'code', temperature: 0.3, max_tokens: 6144 },
      'codellama:7b': { type: 'code', temperature: 0.2, max_tokens: 4096 },
      'codellama:13b': { type: 'code', temperature: 0.2, max_tokens: 6144 },
      'magicoder:7b': { type: 'code', temperature: 0.3, max_tokens: 4096 },
      'starcoder2:15b': { type: 'code', temperature: 0.2, max_tokens: 8192 },

      // ğŸ§  ì¶”ë¡ /ë…¼ë¦¬ ëª¨ë¸êµ°
      'phi3:mini': { type: 'reasoning', temperature: 0.5, max_tokens: 2048, recommended: true },
      'phi3:latest': { type: 'reasoning', temperature: 0.5, max_tokens: 2048 },
      'phi:2.7b': { type: 'reasoning', temperature: 0.6, max_tokens: 2048 },

      // ğŸ­ ë²”ìš© ëŒ€í™” ëª¨ë¸êµ°
      'mistral:latest': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'mistral:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'mixtral:8x7b': { type: 'chat', temperature: 0.6, max_tokens: 4096 },
      'vicuna:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },
      'qwen:7b': { type: 'chat', temperature: 0.7, max_tokens: 2048 },

      // ğŸ”— ì„ë² ë”© ëª¨ë¸êµ°
      'nomic-embed-text:latest': { type: 'embedding', temperature: 0.0, max_tokens: 512 },
      'mxbai-embed-large:latest': { type: 'embedding', temperature: 0.0, max_tokens: 512 }
    };

    Object.entries(configs).forEach(([model, config]) => {
      this.modelConfigs.set(model, config);
    });

    console.log(`âœ… ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: ${this.modelConfigs.size} ê°œ ëª¨ë¸ (ì „ì²´ Ollama ì§€ì›)`);
    console.log(`ğŸ¯ ì¶”ì²œ ëª¨ë¸: ${Object.entries(configs).filter(([_, config]) => config.recommended).map(([name]) => name).join(', ')}`);
  }

  // ============================================================================
  // ğŸ” ë¬´í•œë£¨í”„ ë°©ì§€ ì—°ê²° ìƒíƒœ ê´€ë¦¬
  // ============================================================================

  /**
   * ì•ˆì „í•œ ì—°ê²° í™•ì¸ (ì¿¨ë‹¤ìš´ + ì¤‘ë³µ ë°©ì§€)
   */
  async checkConnection(): Promise<boolean> {
    const now = Date.now();
    
    // ì¿¨ë‹¤ìš´ ì²´í¬
    if (now - this.lastConnectionCheck < this.connectionCooldown) {
      console.log('ğŸ”„ ì—°ê²° ì²´í¬ ì¿¨ë‹¤ìš´ ì¤‘... ìºì‹œëœ ê²°ê³¼ ë°˜í™˜');
      return this.isAvailable;
    }
    
    // ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
    if (this.isConnecting) {
      console.log('â³ ì´ë¯¸ ì—°ê²° ì²´í¬ ì¤‘... ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜');
      return this.isAvailable;
    }
    
    return await this.performSingleConnectionCheck();
  }

  /**
   * ë‹¨ì¼ ì—°ê²° í™•ì¸ ìˆ˜í–‰ (ì¬ê·€ í˜¸ì¶œ ì—†ìŒ)
   */
  private async performSingleConnectionCheck(): Promise<boolean> {
    this.isConnecting = true;
    this.lastConnectionCheck = Date.now();
    
    try {
      console.log(`ğŸ” Ollama ì—°ê²° í™•ì¸ ì¤‘: ${this.baseURL}`);
      
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000);
      
      const response = await fetch(`${this.baseURL}/api/tags`, {
        signal: controller.signal,
        headers: { 'Content-Type': 'application/json' }
      });
      
      clearTimeout(timeoutId);
      
      if (!response.ok) {
        console.warn(`âš ï¸ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: ${response.status}`);
        this.isAvailable = false;
        return false;
      }
      
      const data = await response.json();
      console.log(`âœ… Ollama ì—°ê²° ì„±ê³µ, ëª¨ë¸ ìˆ˜: ${data.models?.length || 0}`);
      this.isAvailable = true;
      return true;
      
    } catch (error: any) {
      if (error.name === 'AbortError') {
        console.warn('âš ï¸ Ollama ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ (5ì´ˆ)');
      } else if (error.message?.includes('ECONNREFUSED')) {
        console.warn('âš ï¸ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ. `ollama serve` ëª…ë ¹ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.');
      } else {
        console.warn(`âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: ${error.message}`);
      }
      this.isAvailable = false;
      return false;
      
    } finally {
      this.isConnecting = false;
    }
  }

  /**
   * ê°•ì œ í—¬ìŠ¤ì²´í¬ (ìºì‹œ ë¬´ì‹œ)
   */
  async forceHealthCheck(): Promise<boolean> {
    this.lastConnectionCheck = 0;
    this.isConnecting = false;
    return await this.checkConnection();
  }

  // ============================================================================
  // ğŸ“‹ ë¬´í•œë£¨í”„ ë°©ì§€ ëª¨ë¸ ê´€ë¦¬
  // ============================================================================

  /**
   * ì•ˆì „í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ì¿¨ë‹¤ìš´ + ì¤‘ë³µ ë°©ì§€)
   */
  async getModels(): Promise<string[]> {
    const now = Date.now();
    
    // ì¿¨ë‹¤ìš´ ì²´í¬
    if (this.models.length > 0 && now - this.lastModelsCheck < this.modelsCooldown) {
      console.log('ğŸ”„ ëª¨ë¸ ëª©ë¡ ì¿¨ë‹¤ìš´ ì¤‘... ìºì‹œëœ ê²°ê³¼ ë°˜í™˜');
      return this.models;
    }
    
    // ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
    if (this.isLoadingModels) {
      console.log('â³ ì´ë¯¸ ëª¨ë¸ ë¡œë”© ì¤‘... ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜');
      return this.models;
    }
    
    await this.performSingleModelsLoad();
    return this.models;
  }

  /**
   * ë‹¨ì¼ ëª¨ë¸ ë¡œë”© ìˆ˜í–‰ (ì¬ê·€ í˜¸ì¶œ ì—†ìŒ)
   */
  private async performSingleModelsLoad(): Promise<void> {
    this.isLoadingModels = true;
    this.lastModelsCheck = Date.now();
    
    try {
      // ì—°ê²° ìƒíƒœê°€ í™•ì‹¤íˆ ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë°°ì—´ ë°˜í™˜
      if (!this.isAvailable) {
        console.warn('âš ï¸ Ollama ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€ - ë¹ˆ ëª¨ë¸ ëª©ë¡ ë°˜í™˜');
        this.models = [];
        return;
      }

      const response = await fetch(`${this.baseURL}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: AbortSignal.timeout(10000)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data: unknown = await response.json();
      const modelsData = (data as any).models || [];
      
      this.availableModels.clear();
      this.models = [];
      
      modelsData.forEach((model: OllamaModelInfo) => {
        this.availableModels.set(model.name, model);
        this.models.push(model.name);
      });
      
      console.log(`ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ (${this.models.length}ê°œ):`, this.models);

    } catch (error: any) {
      console.error('âŒ ëª¨ë¸ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨:', error.message);
      this.models = [];
    } finally {
      this.isLoadingModels = false;
    }
  }

  /**
   * í–¥ìƒëœ ëª¨ë¸ ëª©ë¡ (AIModel í˜•ì‹)
   */
  async getAvailableModels(): Promise<AIModel[]> {
    await this.getModels(); // ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©

    const models: AIModel[] = [];
    
    this.availableModels.forEach((info, name) => {
      const config = this.modelConfigs.get(name);
      const size = this.formatSize(info.size);
      
      models.push({
        id: name,
        name: this.getDisplayName(name),
        available: true,
        type: config?.type || 'chat',
        size,
        description: this.getModelDescription(name),
        recommended: config?.recommended || false
      });
    });

    // íƒ€ì…ë³„, ì¶”ì²œ ì—¬ë¶€ë³„ ì •ë ¬
    return models.sort((a, b) => {
      // ì¶”ì²œ ëª¨ë¸ì„ ë¨¼ì € ë°°ì¹˜
      if (a.recommended && !b.recommended) return -1;
      if (!a.recommended && b.recommended) return 1;
      
      // íƒ€ì…ë³„ ì •ë ¬
      const typeOrder = { 'chat': 0, 'code': 1, 'reasoning': 2, 'embedding': 3 };
      const typeCompare = (typeOrder[a.type] || 9) - (typeOrder[b.type] || 9);
      if (typeCompare !== 0) return typeCompare;
      
      // ê°™ì€ íƒ€ì… ë‚´ì—ì„œëŠ” ì´ë¦„ìˆœ
      return a.name.localeCompare(b.name);
    });
  }

  /**
   * ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜ (ì‚¬ìš©ì ëª¨ë¸ ëª©ë¡ ê¸°ë°˜)
   */
  getDefaultModel(): string {
    // ì‚¬ìš©ìê°€ ë³´ìœ í•œ ëª¨ë¸ ì¤‘ì—ì„œ ì¶”ì²œ ìˆœì„œëŒ€ë¡œ ì„ íƒ
    const preferredOrder = [
      'llama3.2:3b',       // ì¶”ì²œ 1ìˆœìœ„
      'llama3.2:latest',   
      'deepseek-coder:6.7b', // ì½”ë”©ìš© ì¶”ì²œ
      'phi3:mini',         // ì¶”ë¡ ìš© ì¶”ì²œ
      'llama3.1:8b',
      'mistral:latest',
      'llama2:7b',
      'codellama:7b',
      'vicuna:7b',
      'qwen:7b'
    ];

    // ì‚¬ìš©ìê°€ ë³´ìœ í•œ ëª¨ë¸ ì¤‘ ì²« ë²ˆì§¸ ì¶”ì²œ ëª¨ë¸ ì„ íƒ
    for (const model of preferredOrder) {
      if (this.models.includes(model)) {
        return model;
      }
    }

    // ì¶”ì²œ ëª©ë¡ì— ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
    if (this.models.length > 0) return this.models[0];
    
    // ê¸°ë³¸ê°’
    return 'llama3.2:3b';
  }

  // ============================================================================
  // ğŸ¯ AI ì‘ë‹µ ìƒì„± (í–¥ìƒëœ ê¸°ëŠ¥ + ì•ˆì „ì„±)
  // ============================================================================

  /**
   * AI ì‘ë‹µ ìƒì„± (ë©”ì¸ ë©”ì„œë“œ) - ì•ˆì „í•œ êµ¬í˜„
   */
  async generateResponse(
    message: string,
    modelId?: string,
    personalizedContext?: any,
    userId?: string,
    conversationId?: string
  ): Promise<AIResponse> {
    const startTime = Date.now();
    const model = modelId || this.getDefaultModel();

    try {
      // ë‹¨ì¼ ì—°ê²° í™•ì¸ (ë¬´í•œë£¨í”„ ë°©ì§€)
      const connected = await this.checkConnection();
      if (!connected) {
        throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `ollama serve` ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.');
      }

      console.log(`ğŸ¦™ Ollama ì‘ë‹µ ìƒì„± ì‹œì‘: ${model}`);

      // ë©”ì‹œì§€ êµ¬ì„±
      const messages = this.buildMessages(message, model, personalizedContext);
      
      // Ollama API í˜¸ì¶œ
      const response = await this.callOllamaAPI(model, messages);
      
      const processingTime = Date.now() - startTime;
      
      const aiResponse: AIResponse = {
        content: response.message?.content || response.response || '',
        model: `${model}`,
        tokensUsed: response.eval_count || this.estimateTokens(message + (response.message?.content || '')),
        processingTime,
        confidence: 0.9,
        provider: 'ollama',
        local: true,
        privacy: 'fully_local',
        metadata: {
          promptTokens: response.prompt_eval_count || this.estimateTokens(message),
          completionTokens: response.eval_count || this.estimateTokens(response.message?.content || ''),
          modelSize: this.getModelSize(model),
          conversationId,
          messageId: this.generateMessageId()
        }
      };

      // DatabaseServiceë¥¼ í†µí•œ ëŒ€í™” ì €ì¥ (ì•ˆì „í•˜ê²Œ)
      if (userId && this.db) {
        try {
          await this.saveChatToDatabase(userId, message, aiResponse, conversationId);
        } catch (dbError) {
          console.warn('âš ï¸ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨ (ê¸°ëŠ¥ì€ ê³„ì†ë¨):', dbError);
        }
      }

      console.log(`âœ… Ollama ì‘ë‹µ ìƒì„± ì™„ë£Œ: ${processingTime}ms`);
      return aiResponse;

    } catch (error: any) {
      const processingTime = Date.now() - startTime;
      console.error(`âŒ Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (${processingTime}ms):`, error.message);
      
      return this.generateFallbackResponse(message, model, processingTime);
    }
  }

  /**
   * ê¸°ì¡´ ollama.ts í˜¸í™˜ - generate ë©”ì„œë“œ
   */
  async generate(
    model: string,
    prompt: string,
    options: {
      temperature?: number;
      num_predict?: number;
    } = {}
  ): Promise<string> {
    try {
      console.log(`ğŸ¦™ Ollama Generate ìš”ì²­: ${model}`);
      
      if (!await this.checkConnection()) {
        throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      }

      const requestBody = {
        model,
        prompt,
        stream: false,
        options: {
          temperature: options.temperature || 0.7,
          num_predict: options.num_predict || 1000
        }
      };

      const response = await fetch(`${this.baseURL}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(this.timeout)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data: unknown = await response.json();
      const result = (data as any).response || 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
      
      console.log('âœ… Ollama Generate ì„±ê³µ');
      return result;

    } catch (error: any) {
      console.error('âŒ Ollama Generate ì‹¤íŒ¨:', error.message);
      throw error;
    }
  }

  /**
   * ê¸°ì¡´ ollama.ts í˜¸í™˜ - chatCompletion ë©”ì„œë“œ
   */
  async chatCompletion(
    model: string,
    messages: OllamaMessage[],
    options: {
      temperature?: number;
      stream?: boolean;
      personalizedContext?: any;
    } = {}
  ): Promise<string> {
    
    for (let attempt = 1; attempt <= this.retryCount; attempt++) {
      try {
        console.log(`ğŸ¦™ Ollama ì±„íŒ… ì‹œë„ ${attempt}/${this.retryCount} - ëª¨ë¸: ${model}`);
        
        if (!await this.checkConnection()) {
          throw new Error('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
        }

        const result = await this.performChatRequest(model, messages, options);
        console.log(`âœ… Ollama ì±„íŒ… ì„±ê³µ (ì‹œë„ ${attempt})`);
        return result;

      } catch (error: any) {
        console.error(`âŒ Ollama ì±„íŒ… ì‹œë„ ${attempt} ì‹¤íŒ¨:`, error.message);
        
        if (attempt === this.retryCount) {
          throw error;
        }
        
        await this.delay(1000 * attempt);
      }
    }

    throw new Error('ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨');
  }

  // ============================================================================
  // ğŸ”§ ë‚´ë¶€ ë©”ì„œë“œë“¤ (ì•ˆì „í•œ êµ¬í˜„)
  // ============================================================================

  /**
   * Ollama API ì§ì ‘ í˜¸ì¶œ
   */
  private async callOllamaAPI(model: string, messages: OllamaMessage[]): Promise<OllamaResponse> {
    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        messages,
        stream: false,
        options: {
          temperature: this.modelConfigs.get(model)?.temperature || 0.7,
          num_predict: this.modelConfigs.get(model)?.max_tokens || 2048
        }
      }),
      signal: AbortSignal.timeout(this.timeout)
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const data: unknown = await response.json();
    return data as OllamaResponse;
  }

  /**
   * ì‹¤ì œ ì±„íŒ… ìš”ì²­ ìˆ˜í–‰ (ê¸°ì¡´ í˜¸í™˜)
   */
  private async performChatRequest(
    model: string,
    messages: OllamaMessage[],
    options: {
      temperature?: number;
      stream?: boolean;
      personalizedContext?: any;
    }
  ): Promise<string> {
    
    const { temperature = 0.7, stream = false, personalizedContext } = options;

    const requestBody = {
      model,
      messages: this.formatMessages(messages, personalizedContext),
      stream,
      options: {
        temperature,
        num_predict: 2000,
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(requestBody),
      signal: AbortSignal.timeout(this.timeout)
    });

    if (!response.ok) {
      const errorText = await response.text();
      let errorMessage = `HTTP ${response.status}: ${response.statusText}`;
      
      try {
        const errorJson = JSON.parse(errorText);
        errorMessage = errorJson.error || errorMessage;
      } catch {
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
      }
      
      throw new Error(errorMessage);
    }

    const data: OllamaResponse = await response.json();
    
    if (!data.done) {
      throw new Error('Ollama ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    const content = data.message?.content || data.response || '';
    
    if (!content.trim()) {
      throw new Error('Ollamaì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤');
    }

    return content;
  }

  /**
   * ë©”ì‹œì§€ í¬ë§·íŒ… (ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
   */
  private buildMessages(message: string, model: string, context?: any): OllamaMessage[] {
    const messages: OllamaMessage[] = [];
    
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    const systemPrompt = this.buildSystemPrompt(model, context);
    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.push({ role: 'user', content: message });

    return messages;
  }

  /**
   * ê¸°ì¡´ í˜¸í™˜ - ë©”ì‹œì§€ í¬ë§·íŒ…
   */
  private formatMessages(messages: OllamaMessage[], personalizedContext?: any): OllamaMessage[] {
    const formattedMessages: OllamaMessage[] = [];

    if (personalizedContext) {
      formattedMessages.push({
        role: 'system',
        content: this.formatPersonalizedContext(personalizedContext)
      });
    }

    formattedMessages.push(...messages);
    return formattedMessages;
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private buildSystemPrompt(model: string, context?: any): string {
    let basePrompt = "You are a helpful AI assistant specialized in conversational interactions.";
    
    const config = this.modelConfigs.get(model);
    if (config?.type === 'code') {
      basePrompt = "You are an expert programming assistant. Provide clear, well-commented code solutions.";
    } else if (config?.type === 'reasoning') {
      basePrompt = "You are a logical reasoning assistant. Think step by step and provide detailed explanations.";
    }

    if (!context) return basePrompt;

    // ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if (context.personalityProfile?.type) {
      basePrompt += `\n\nUser Profile: The user has a ${context.personalityProfile.type} personality type.`;
    }

    if (context.cues && context.cues.length > 0) {
      basePrompt += `\n\nPersonalization: You have access to ${context.cues.length} personal preference data points.`;
    }

    return basePrompt;
  }

  /**
   * ê¸°ì¡´ í˜¸í™˜ - ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
   */
  private formatPersonalizedContext(context: any): string {
    if (!context || Object.keys(context).length === 0) {
      return 'ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.';
    }

    let contextPrompt = 'ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n';
    contextPrompt += 'ì‚¬ìš©ìì— ëŒ€í•œ ì •ë³´:\n';

    if (context.personalityProfile) {
      contextPrompt += `- ì„±ê²©: ${context.personalityProfile.type || 'Adaptive'}\n`;
      contextPrompt += `- ì†Œí†µ ìŠ¤íƒ€ì¼: ${context.personalityProfile.communicationStyle || 'Balanced'}\n`;
    }

    if (context.behaviorPatterns && context.behaviorPatterns.length > 0) {
      contextPrompt += `- ê´€ì‹¬ì‚¬: ${context.behaviorPatterns.slice(0, 3).join(', ')}\n`;
    }

    contextPrompt += '\nì‚¬ìš©ìì˜ ì„±ê²©ê³¼ ì„ í˜¸ë„ì— ë§ì¶° ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.\n';
    return contextPrompt;
  }

  /**
   * DatabaseServiceë¥¼ í†µí•œ ì•ˆì „í•œ ëŒ€í™” ì €ì¥
   */
  private async saveChatToDatabase(
    userId: string,
    userMessage: string,
    aiResponse: AIResponse,
    conversationId?: string
  ): Promise<void> {
    try {
      if (!this.db || typeof this.db.saveChatMessage !== 'function') {
        return; // DB ì„œë¹„ìŠ¤ ì—†ìœ¼ë©´ ìŠ¤í‚µ
      }

      // ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
      await this.db.saveChatMessage({
        conversation_id: conversationId || 'default',
        user_id: userId,
        content: userMessage,
        sender: 'user',
        model: null,
        tokens_used: aiResponse.metadata?.promptTokens || 0,
        timestamp: new Date().toISOString()
      });

      // AI ì‘ë‹µ ì €ì¥
      await this.db.saveChatMessage({
        conversation_id: conversationId || 'default',
        user_id: userId,
        content: aiResponse.content,
        sender: 'assistant',
        model: aiResponse.model,
        tokens_used: aiResponse.metadata?.completionTokens || 0,
        processing_time: aiResponse.processingTime,
        timestamp: new Date().toISOString()
      });

      console.log('âœ… ëŒ€í™” ì €ì¥ ì™„ë£Œ');
    } catch (error) {
      console.warn('âš ï¸ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨:', error);
    }
  }

  // ============================================================================
  // ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  // ============================================================================

  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }

  private getModelSize(model: string): string {
    const info = this.availableModels.get(model);
    return info ? this.formatSize(info.size) : 'Unknown';
  }

  private formatSize(bytes: number): string {
    const units = ['B', 'KB', 'MB', 'GB', 'TB'];
    let size = bytes;
    let unitIndex = 0;
    
    while (size >= 1024 && unitIndex < units.length - 1) {
      size /= 1024;
      unitIndex++;
    }
    
    return `${size.toFixed(1)} ${units[unitIndex]}`;
  }

  private getDisplayName(modelName: string): string {
    const nameMap: { [key: string]: string } = {
      // Llama ëª¨ë¸êµ°
      'llama3.2:3b': 'Llama 3.2 (3B) â­',
      'llama3.2:1b': 'Llama 3.2 (1B)',
      'llama3.2:latest': 'Llama 3.2 (Latest)',
      'llama3.1:8b': 'Llama 3.1 (8B)',
      'llama3.1:70b': 'Llama 3.1 (70B)',
      'llama2:7b': 'Llama 2 (7B)',
      'llama2:13b': 'Llama 2 (13B)', 
      'llama2:70b': 'Llama 2 (70B)',

      // ì½”ë”© ëª¨ë¸êµ°
      'deepseek-coder:6.7b': 'DeepSeek Coder (6.7B) ğŸ’»',
      'deepseek-coder:33b': 'DeepSeek Coder (33B)',
      'deepseek-coder-v2:16b': 'DeepSeek Coder V2 (16B)',
      'codellama:7b': 'Code Llama (7B)',
      'codellama:13b': 'Code Llama (13B)',
      'magicoder:7b': 'MagiCoder (7B)',
      'starcoder2:15b': 'StarCoder 2 (15B)',

      // ì¶”ë¡  ëª¨ë¸êµ°
      'phi3:mini': 'Phi-3 Mini ğŸ§ ',
      'phi3:latest': 'Phi-3 (Latest)',
      'phi:2.7b': 'Phi (2.7B)',

      // ë²”ìš© ëª¨ë¸êµ°
      'mistral:latest': 'Mistral 7B',
      'mistral:7b': 'Mistral 7B',
      'mixtral:8x7b': 'Mixtral 8x7B',
      'vicuna:7b': 'Vicuna (7B)',
      'qwen:7b': 'Qwen (7B)',

      // ì„ë² ë”© ëª¨ë¸êµ°
      'nomic-embed-text:latest': 'Nomic Embed ğŸ“Š',
      'mxbai-embed-large:latest': 'MxBai Embed Large'
    };
    
    return nameMap[modelName] || modelName;
  }

  private getModelDescription(modelName: string): string {
    const descriptions: { [key: string]: string } = {
      // Llama ëª¨ë¸êµ°
      'llama3.2:3b': 'ğŸ¯ ê°€ì¥ ê· í˜•ì¡íŒ ë²”ìš© ëŒ€í™” ëª¨ë¸ (ì¶”ì²œ)',
      'llama3.2:1b': 'âš¡ ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê°„ë‹¨í•œ ì‘ì—…ìš© ê²½ëŸ‰ ëª¨ë¸',
      'llama3.2:latest': 'ğŸ†• ìµœì‹  Llama 3.2 ëª¨ë¸',
      'llama3.1:8b': 'ğŸ’ª í–¥ìƒëœ ì„±ëŠ¥ì˜ ì¤‘ê¸‰ ë²”ìš© ëª¨ë¸',
      'llama3.1:70b': 'ğŸš€ ìµœê³  ì„±ëŠ¥ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸',
      'llama2:7b': 'ğŸ“š ì•ˆì •ì ì¸ ê¸°ë³¸ ëŒ€í™” ëª¨ë¸',
      'llama2:13b': 'ğŸ“ ì¤‘ê¸‰ ê·œëª¨ì˜ ë²”ìš© ëª¨ë¸',
      'llama2:70b': 'ğŸ† ëŒ€í˜• ê³ ì„±ëŠ¥ ì–¸ì–´ ëª¨ë¸',

      // ì½”ë”© ëª¨ë¸êµ°
      'deepseek-coder:6.7b': 'ğŸ’» ì½”ë“œ ìƒì„±/ë””ë²„ê¹… ìµœì í™” ëª¨ë¸ (ì¶”ì²œ)',
      'deepseek-coder:33b': 'ğŸ”§ ëŒ€í˜• ì½”ë”© ì „ë¬¸ ëª¨ë¸',
      'deepseek-coder-v2:16b': 'âœ¨ ê°œì„ ëœ ì½”ë”© ì „ë¬¸ ëª¨ë¸',
      'codellama:7b': 'ğŸ¦™ Metaì˜ ì½”ë“œ ìƒì„± ì „ë¬¸ ëª¨ë¸',
      'codellama:13b': 'ğŸ”¥ í–¥ìƒëœ ì½”ë“œ ìƒì„± ëŠ¥ë ¥',
      'magicoder:7b': 'ğŸª„ ë§ˆë²• ê°™ì€ ì½”ë“œ ìƒì„± ëª¨ë¸',
      'starcoder2:15b': 'â­ BigCodeì˜ ì°¨ì„¸ëŒ€ ì½”ë”© ëª¨ë¸',

      // ì¶”ë¡  ëª¨ë¸êµ°
      'phi3:mini': 'ğŸ§  ë…¼ë¦¬ì  ì¶”ë¡ ê³¼ ìˆ˜í•™ ë¬¸ì œ í•´ê²° íŠ¹í™” (ì¶”ì²œ)',
      'phi3:latest': 'ğŸ¯ ìµœì‹  ì†Œí˜• ì¶”ë¡  ëª¨ë¸',
      'phi:2.7b': 'ğŸ’¡ íš¨ìœ¨ì ì¸ ì¶”ë¡  ì „ë¬¸ ëª¨ë¸',

      // ë²”ìš© ëª¨ë¸êµ°
      'mistral:latest': 'ğŸŒŸ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ìœ ëŸ½ì‚° ëª¨ë¸',
      'mistral:7b': 'âš¡ ë¹ ë¥¸ ì‘ë‹µì˜ ë²”ìš© ëª¨ë¸',
      'mixtral:8x7b': 'ğŸ›ï¸ ì „ë¬¸ê°€ í˜¼í•© ëŒ€í˜• ëª¨ë¸',
      'vicuna:7b': 'ğŸ¦™ Llama ê¸°ë°˜ ëŒ€í™” ìµœì í™” ëª¨ë¸',
      'qwen:7b': 'ğŸ‡¨ğŸ‡³ Alibabaì˜ ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸',

      // ì„ë² ë”© ëª¨ë¸êµ°
      'nomic-embed-text:latest': 'ğŸ“Š í…ìŠ¤íŠ¸ ì„ë² ë”© ì „ìš© ëª¨ë¸',
      'mxbai-embed-large:latest': 'ğŸ”— ëŒ€í˜• ì„ë² ë”© ë²¡í„° ìƒì„±'
    };
    
    return descriptions[modelName] || 'ğŸ¤– ë²”ìš© AI ëª¨ë¸';
  }

  private generateMessageId(): string {
    return `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * í´ë°± ì‘ë‹µ ìƒì„±
   */
  getFallbackResponse(userMessage: string): string {
    return `ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ Ollama AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n1. \`ollama serve\` ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘\n2. \`ollama pull llama3.2:3b\` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n3. Ollamaê°€ ${this.baseURL} ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸\n\nì§ˆë¬¸ "${userMessage}"ì— ëŒ€í•œ ë‹µë³€ì€ ì„œë¹„ìŠ¤ ë³µêµ¬ í›„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.`;
  }

  private generateFallbackResponse(message: string, model: string, processingTime: number): AIResponse {
    const fallbackContent = this.getFallbackResponse(message);

    return {
      content: fallbackContent,
      model: `Fallback (${model})`,
      tokensUsed: fallbackContent.length,
      processingTime,
      confidence: 0.3,
      provider: 'ollama',
      local: true,
      metadata: {
        error: 'Service unavailable',
        fallback: true
      }
    };
  }

  // ============================================================================
  // ğŸ“Š ìƒíƒœ ë° ì •ë³´ ë©”ì„œë“œë“¤ (ë¬´í•œë£¨í”„ ë°©ì§€)
  // ============================================================================

  /**
   * ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜ (ì•ˆì „í•œ êµ¬í˜„)
   */
  async getServiceStatus(): Promise<any> {
    // ì—°ê²° ìƒíƒœë§Œ í™•ì¸ (ëª¨ë¸ ë¡œë”© ì—†ìŒ)
    const isConnected = this.isAvailable;

    return {
      provider: 'ollama',
      connected: isConnected,
      baseUrl: this.baseURL,
      models: this.models, // ìºì‹œëœ ëª¨ë¸ ëª©ë¡ ì‚¬ìš©
      defaultModel: this.getDefaultModel(),
      features: [
        'chat', 
        'completion', 
        'local', 
        'privacy-focused',
        'conversation_storage',
        'personalization_support'
      ],
      database: {
        connected: this.db?.isConnected?.() || false,
        available: !!this.db
      },
      status: {
        initialized: this.isInitialized,
        connecting: this.isConnecting,
        loadingModels: this.isLoadingModels,
        lastConnectionCheck: this.lastConnectionCheck,
        lastModelsCheck: this.lastModelsCheck
      }
    };
  }

  /**
   * ê¸°ì¡´ í˜¸í™˜ - ìƒíƒœ ì •ë³´
   */
  getStatus(): {
    available: boolean;
    baseUrl: string;
    lastHealthCheck: Date | null;
    timeout: number;
    retryCount: number;
    modelCount: number;
    cachedModels: string[];
  } {
    return {
      available: this.isAvailable,
      baseUrl: this.baseURL,
      lastHealthCheck: this.lastConnectionCheck ? new Date(this.lastConnectionCheck) : null,
      timeout: this.timeout,
      retryCount: this.retryCount,
      modelCount: this.models.length,
      cachedModels: this.models
    };
  }

  /**
   * ì—°ê²° í…ŒìŠ¤íŠ¸ (ì•ˆì „í•œ êµ¬í˜„)
   */
  async testConnection(): Promise<{success: boolean, message: string, details?: any}> {
    try {
      const isConnected = await this.forceHealthCheck();
      
      if (isConnected) {
        // ëª¨ë¸ ë¡œë”©ì€ ì„ íƒì ìœ¼ë¡œ
        if (this.models.length === 0) {
          await this.getModels();
        }
        
        return {
          success: true,
          message: 'Ollama ì—°ê²° ì„±ê³µ',
          details: {
            modelCount: this.models.length,
            availableModels: this.models.slice(0, 5)
          }
        };
      } else {
        return {
          success: false,
          message: 'Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
          details: {
            baseUrl: this.baseURL,
            suggestion: 'ollama serve ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”'
          }
        };
      }
    } catch (error: any) {
      return {
        success: false,
        message: `ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ${error.message}`,
        details: {
          baseUrl: this.baseURL,
          error: error.message
        }
      };
    }
  }

  /**
   * ì„œë¹„ìŠ¤ ì •ë¦¬ (DI Containerìš©)
   */
  public dispose(): void {
    console.log('ğŸ§¹ OllamaAIService ì •ë¦¬ ì¤‘...');
    this.isInitialized = false;
    this.models = [];
    this.availableModels.clear();
    this.isAvailable = false;
    this.isConnecting = false;
    this.isLoadingModels = false;
    this.lastConnectionCheck = 0;
    this.lastModelsCheck = 0;
    console.log('âœ… OllamaAIService ì •ë¦¬ ì™„ë£Œ');
  }
}

// ============================================================================
// ğŸ“¤ Export (ì¤‘ë³µ ì œê±° ë° í˜¸í™˜ì„± ë³´ì¥)
// ============================================================================

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
const ollamaService = OllamaAIService.getInstance();

// ê¸°ì¡´ ollama.ts í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
export const checkConnection = () => ollamaService.checkConnection();
export const getModels = () => ollamaService.getModels();
export const chat = (model: string, messages: OllamaMessage[], stream: boolean = false) => 
  ollamaService.chatCompletion(model, messages, { stream });

// í´ë˜ìŠ¤ì™€ ì¸ìŠ¤í„´ìŠ¤ export
export { ollamaService };
export default OllamaAIService;

// ============================================================================
// ğŸ‰ ìˆ˜ì • ì™„ë£Œ ë¡œê·¸
// ============================================================================

console.log('âœ… í–¥ìƒëœ Ollama AI ì„œë¹„ìŠ¤ ë¡œë“œë¨');
console.log('  ğŸ› FIXED: ë¬´í•œë£¨í”„ ì™„ì „ ë°©ì§€');
console.log('  âœ… DatabaseService ì•ˆì „í•œ ì—°ê²°');  
console.log('  ğŸ”§ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ ì‹œìŠ¤í…œ');
console.log('  ğŸ’ª ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€');
console.log('  ğŸ›¡ï¸ ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬');