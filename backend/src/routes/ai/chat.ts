
import express from 'express';
import { v4 as uuidv4 } from 'uuid';
import { DatabaseService } from '../../services/database/DatabaseService';
import { supabaseService } from '../../services/database/SupabaseService';
import { PersonalizationService } from '../../services/ai/PersonalizationService';
import { asyncHandler } from '../../middleware/errorHandler';

// ğŸ¦™ ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ ì‚¬ìš© (paste.txt)
import { ollamaService } from '../../services/ollama';

const router = express.Router();

// ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ ì„ íƒ
const db = process.env.USE_MOCK_DATABASE === 'true' || 
          !process.env.SUPABASE_URL || 
          process.env.SUPABASE_URL.includes('dummy')
  ? DatabaseService.getInstance()
  : supabaseService;

// ============================================================================
// ğŸ¦™ 23ê°œ Ollama ëª¨ë¸ ì„¤ì • (paste-2.txtì—ì„œ ê°€ì ¸ì˜´)
// ============================================================================
const MODEL_CONFIGS = {
  'llama3.2:1b': { speed: 'very-fast', type: 'general', priority: 1, cueBonus: 3 },
  'phi3:mini': { speed: 'very-fast', type: 'general', priority: 2, cueBonus: 3 },
  'llama3.2:3b': { speed: 'fast', type: 'general', priority: 3, cueBonus: 2 },
  'llama3.2:latest': { speed: 'fast', type: 'general', priority: 4, cueBonus: 2 },
  'phi3:latest': { speed: 'fast', type: 'general', priority: 5, cueBonus: 2 },
  'deepseek-coder:6.7b': { speed: 'moderate', type: 'coding', priority: 6, cueBonus: 1 },
  'codellama:7b': { speed: 'moderate', type: 'coding', priority: 7, cueBonus: 1 },
  'magicoder:7b': { speed: 'moderate', type: 'coding', priority: 8, cueBonus: 1 },
  'starcoder2:15b': { speed: 'slow', type: 'coding', priority: 9, cueBonus: 1 },
  'codellama:13b': { speed: 'slow', type: 'coding', priority: 10, cueBonus: 1 },
  'llama3.1:8b': { speed: 'moderate', type: 'general', priority: 11, cueBonus: 1 },
  'mistral:latest': { speed: 'moderate', type: 'general', priority: 12, cueBonus: 1 },
  'mistral:7b': { speed: 'moderate', type: 'general', priority: 13, cueBonus: 1 },
  'qwen:7b': { speed: 'moderate', type: 'general', priority: 14, cueBonus: 1 },
  'vicuna:7b': { speed: 'moderate', type: 'general', priority: 15, cueBonus: 1 },
  'nomic-embed-text:latest': { speed: 'fast', type: 'embedding', priority: 16, cueBonus: 2 },
  'mxbai-embed-large:latest': { speed: 'fast', type: 'embedding', priority: 17, cueBonus: 2 },
  'deepseek-coder-v2:16b': { speed: 'slow', type: 'coding', priority: 18, cueBonus: 1 },
  'deepseek-coder:33b': { speed: 'very-slow', type: 'coding', priority: 19, cueBonus: 1 },
  'mixtral:8x7b': { speed: 'slow', type: 'general', priority: 20, cueBonus: 1 },
  'llama2:70b': { speed: 'very-slow', type: 'general', priority: 21, cueBonus: 1 },
  'llama3.1:70b': { speed: 'very-slow', type: 'general', priority: 22, cueBonus: 1 }
};

// ============================================================================
// ğŸ” ê°„ì†Œí™”ëœ í´ë¼ìš°ë“œ AI í´ë¼ì´ì–¸íŠ¸
// ============================================================================
let openaiClient: any = null;
let anthropicClient: any = null;
let openaiAttempted = false;
let anthropicAttempted = false;

async function getOpenAIClient() {
  if (openaiAttempted) return openaiClient;
  
  openaiAttempted = true;
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey || apiKey.trim() === '' || apiKey === 'your-openai-key-here') {
    console.log('âš ï¸ OpenAI API key not configured');
    return null;
  }

  try {
    const { default: OpenAI } = await import('openai');
    openaiClient = new OpenAI({ apiKey });
    console.log('âœ… OpenAI client created successfully');
    return openaiClient;
  } catch (error: any) {
    console.error('âŒ Failed to create OpenAI client:', error.message);
    return null;
  }
}

async function getAnthropicClient() {
  if (anthropicAttempted) return anthropicClient;
  
  anthropicAttempted = true;
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey || apiKey.trim() === '' || apiKey === 'your-anthropic-key-here') {
    console.log('âš ï¸ Anthropic API key not configured');
    return null;
  }

  try {
    const { default: Anthropic } = await import('@anthropic-ai/sdk');
    anthropicClient = new Anthropic({ apiKey });
    console.log('âœ… Anthropic client created successfully');
    return anthropicClient;
  } catch (error: any) {
    console.error('âŒ Failed to create Anthropic client:', error.message);
    return null;
  }
}

// ============================================================================
// ğŸ¤– ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ í™œìš©)
// ============================================================================
router.post('/chat', asyncHandler(async (req, res) => {
  console.log('ğŸ¯ AI ì±„íŒ… ìš”ì²­ ì‹œì‘ (ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ í™œìš©)');
  
  const { 
    message, 
    model = 'llama3.2:1b', 
    user_did, 
    passport_data,
    userId,
    conversationId 
  } = req.body;

  if (!message || !message.trim()) {
    return res.status(400).json({
      success: false,
      error: 'Message is required'
    });
  }

  let userDid = user_did || `did:final0626:${userId || 'anonymous'}`;
  let actualUserId = userId || userDid.replace('did:final0626:', '');
  const startTime = Date.now();
  const currentConversationId = conversationId || uuidv4();

  try {
    // 1. ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ë¡œ ëª¨ë¸ ìµœì í™” ì„ íƒ
    const selectedModel = await optimizeModelSelectionAdvanced(model, message);
    console.log(`ğŸ¦™ ìµœì í™”ëœ ëª¨ë¸: ${selectedModel}`);

    // 2. ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
    let personalContext = {
      cues: [],
      personalityMatch: 0.5,
      behaviorPatterns: [],
      vaultIds: [],
      personalityProfile: passport_data?.personalityProfile || {},
      preferences: {}
    };

    try {
      const personalizationService = new PersonalizationService(db as any);
      personalContext = await personalizationService.getPersonalizedContext(userDid, message, {
        includeFullProfile: true,
        includeBehaviorPatterns: true,
        includeRecentInteractions: true
      });
    } catch (contextError) {
      console.warn('âš ï¸ ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨:', contextError);
    }

    // 3. ê³ ê¸‰ AI ì‘ë‹µ ìƒì„±
    let aiResult;
    const isOllama = await isOllamaModelAdvanced(selectedModel);
    
    if (isOllama) {
      aiResult = await generateAdvancedOllamaResponse(message, personalContext, selectedModel);
    } else {
      aiResult = await generateCloudResponse(message, personalContext, selectedModel);
    }

    const responseTime = Date.now() - startTime;

    // 4. CUE í† í° ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
    const modelConfig = MODEL_CONFIGS[selectedModel];
    const baseTokens = 5;
    const speedBonus = modelConfig?.cueBonus || 1;
    const personalBonus = personalContext.cues?.length > 0 ? 2 : 0;
    const advancedServiceBonus = 1; // ê³ ê¸‰ ì„œë¹„ìŠ¤ ì‚¬ìš© ë³´ë„ˆìŠ¤
    const cueTokensEarned = baseTokens + speedBonus + personalBonus + advancedServiceBonus;

    // 5. ì‘ë‹µ ë°˜í™˜
    res.json({
      success: true,
      message: {
        id: uuidv4(),
        conversationId: currentConversationId,
        content: aiResult.response,
        model: selectedModel,
        provider: aiResult.provider || 'ollama',
        usedPassportData: aiResult.usedData || [],
        cueTokensEarned,
        responseTimeMs: responseTime,
        tokensUsed: aiResult.tokensUsed,
        verification: {
          verified: true,
          signature: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
          biometric: true
        }
      },
      personalContext: {
        cuesUsed: personalContext.cues?.length || 0,
        vaultsAccessed: personalContext.vaultIds?.length || 0,
        personalityMatch: personalContext.personalityMatch || 0.5,
        behaviorPatterns: personalContext.behaviorPatterns?.slice(0, 3) || []
      },
      advancedFeatures: {
        ollamaServiceVersion: 'advanced',
        cachingEnabled: true,
        streamingSupported: true,
        modelManagementAvailable: true
      }
    });

    console.log(`âœ… ê³ ê¸‰ ì±„íŒ… ì™„ë£Œ: ${responseTime}ms, ${aiResult.provider}, +${cueTokensEarned} CUE`);

  } catch (error: any) {
    console.error('âŒ ê³ ê¸‰ AI ì±„íŒ… ì˜¤ë¥˜:', error);
    
    res.status(500).json({
      success: false,
      error: 'AI chat processing failed',
      details: process.env.NODE_ENV === 'development' ? error.message : undefined,
      conversationId: currentConversationId
    });
  }
}));

// ============================================================================
// ğŸ¯ ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ í™œìš© í•¨ìˆ˜ë“¤
// ============================================================================

async function optimizeModelSelectionAdvanced(requestedModel: string, message: string): Promise<string> {
  try {
    // ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ì˜ ìºì‹±ëœ ì—°ê²° ìƒíƒœ í™•ì¸
    const isConnected = await ollamaService.checkConnection();
    if (!isConnected) return requestedModel;

    // ìºì‹±ëœ ëª¨ë¸ ëª©ë¡ ì‚¬ìš©
    let availableModels = ollamaService.getCachedModels();
    if (availableModels.length === 0) {
      availableModels = await ollamaService.getModels();
    }

    if (availableModels.includes(requestedModel)) return requestedModel;

    const lowerMessage = message.toLowerCase();
    
    // ì½”ë”© ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
    const codingKeywords = ['code', 'programming', 'ì½”ë“œ', 'í”„ë¡œê·¸ë˜ë°', 'function', 'class'];
    const isCoding = codingKeywords.some(keyword => lowerMessage.includes(keyword));
    
    if (isCoding) {
      const codingModels = ['deepseek-coder:6.7b', 'codellama:7b', 'magicoder:7b'];
      for (const model of codingModels) {
        if (availableModels.includes(model)) return model;
      }
    }

    // ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
    if (message.length < 50) {
      const fastModels = ['llama3.2:1b', 'phi3:mini', 'llama3.2:3b'];
      for (const model of fastModels) {
        if (availableModels.includes(model)) return model;
      }
    }

    // ê¶Œì¥ ëª¨ë¸ ì‚¬ìš©
    const recommendedModels = ollamaService.getRecommendedModels()
      .filter(m => m.recommended)
      .map(m => m.name);
      
    for (const model of recommendedModels) {
      if (availableModels.includes(model)) return model;
    }

    return availableModels[0] || requestedModel;
  } catch (error) {
    console.error('ê³ ê¸‰ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨:', error);
    return requestedModel;
  }
}

async function isOllamaModelAdvanced(model: string): Promise<boolean> {
  try {
    // ìºì‹±ëœ ìƒíƒœ ë¨¼ì € í™•ì¸
    const cachedModels = ollamaService.getCachedModels();
    if (cachedModels.includes(model)) return true;

    // ì—°ê²° ìƒíƒœ í™•ì¸ (ìºì‹±ë¨)
    const isConnected = await ollamaService.checkConnection();
    if (!isConnected) return false;

    // ì‹¤ì œ ëª¨ë¸ ëª©ë¡ í™•ì¸
    const availableModels = await ollamaService.getModels();
    return availableModels.includes(model) || !!MODEL_CONFIGS[model];
  } catch (error) {
    return !!MODEL_CONFIGS[model];
  }
}

async function generateAdvancedOllamaResponse(message: string, context: any, model: string): Promise<any> {
  try {
    const systemPrompt = `ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°œì¸ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.
ê³ ê¸‰ ë¡œì»¬ AI ëª¨ë¸(${model})ë¡œì„œ ì‚¬ìš©ìì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ì™„ì „íˆ ë³´í˜¸í•˜ë©° ìµœì í™”ëœ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`;

    const messages = [
      { role: 'system' as const, content: systemPrompt },
      { role: 'user' as const, content: message }
    ];

    // ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ì˜ chat ë©”ì„œë“œ ì‚¬ìš© (ìºì‹±, ì—ëŸ¬ ì²˜ë¦¬ ë“± í¬í•¨)
    const response = await ollamaService.chat(model, messages, false);
    
    return {
      response: response,
      tokensUsed: Math.floor(response.length / 4),
      usedData: ['Personality Profile', `${context.cues?.length || 0} Personal Contexts`],
      provider: 'ollama-advanced',
      local: true,
      cached: true
    };

  } catch (error: any) {
    console.error(`âŒ ê³ ê¸‰ Ollama ì˜¤ë¥˜:`, error.message);
    return {
      response: `**${model}** (ê³ ê¸‰ ë¡œì»¬ ì„œë¹„ìŠ¤)\n\nì•ˆë…•í•˜ì„¸ìš”! "${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nì¼ì‹œì ì¸ ì—°ê²° ë¬¸ì œê°€ ìˆì§€ë§Œ ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ê°€ ë³µêµ¬ë¥¼ ì‹œë„ ì¤‘ì…ë‹ˆë‹¤.`,
      tokensUsed: 100,
      usedData: [],
      provider: 'mock-advanced'
    };
  }
}

async function generateCloudResponse(message: string, context: any, model: string) {
  // ê¸°ì¡´ í´ë¼ìš°ë“œ ì‘ë‹µ ë¡œì§ (paste-2.txtì™€ ë™ì¼)
  if (model.startsWith('gpt')) {
    const client = await getOpenAIClient();
    if (!client) {
      return {
        response: `**GPT-4o** (Mock)\n\n"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Mock ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`,
        tokensUsed: 150,
        usedData: [],
        provider: 'mock'
      };
    }

    try {
      const completion = await client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: 'ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.' },
          { role: 'user', content: message }
        ],
        max_tokens: 1000,
        temperature: 0.7,
      });

      return {
        response: completion.choices[0]?.message?.content || 'Sorry, I could not generate a response.',
        tokensUsed: completion.usage?.total_tokens || 0,
        usedData: ['Personality Profile'],
        provider: 'openai'
      };
    } catch (error: any) {
      return {
        response: `**GPT-4o** (API ì˜¤ë¥˜)\n\n"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nAPI ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error.message}`,
        tokensUsed: 100,
        usedData: [],
        provider: 'mock'
      };
    }
  }
  
  // Claudeë„ ìœ ì‚¬í•˜ê²Œ ì²˜ë¦¬...
  return {
    response: `**Unknown Model** (${model})\n\n"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.\n\nì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì´ë¯€ë¡œ Mock ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`,
    tokensUsed: 100,
    usedData: [],
    provider: 'mock'
  };
}

// ============================================================================
// ğŸ“Š ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ í™œìš© API ì—”ë“œí¬ì¸íŠ¸ë“¤
// ============================================================================

// ê³ ê¸‰ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
router.get('/models', asyncHandler(async (req, res) => {
  try {
    const healthCheck = await ollamaService.healthCheck();
    const cachedModels = ollamaService.getCachedModels();
    const recommendedModels = ollamaService.getRecommendedModels();

    // ì‹¤ì œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    let ollamaModels: string[] = [];
    if (healthCheck.connected) {
      try {
        ollamaModels = await ollamaService.getModels();
      } catch (error) {
        ollamaModels = cachedModels; // ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©
      }
    }

    // ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë¶„ë¥˜
    const categorizedModels = {
      ultraFast: [] as any[],
      balanced: [] as any[],
      coding: [] as any[],
      advanced: [] as any[],
      cloud: [] as any[]
    };

    // Ollama ëª¨ë¸ë“¤ ë¶„ë¥˜ (23ê°œ ëª¨ë¸ ì„¤ì • í™œìš©)
    ollamaModels.forEach(modelName => {
      const config = MODEL_CONFIGS[modelName];
      if (!config) return;

      const modelInfo = {
        id: modelName,
        name: `ğŸ¦™ ${modelName}`,
        provider: 'ollama-advanced',
        description: getModelDescription(modelName, config),
        available: true,
        type: 'local',
        speed: config.speed,
        category: config.type,
        recommended: config.priority <= 5,
        local: true,
        cueBonus: config.cueBonus,
        cached: cachedModels.includes(modelName)
      };

      if (config.speed === 'very-fast') {
        categorizedModels.ultraFast.push(modelInfo);
      } else if (config.speed === 'fast') {
        categorizedModels.balanced.push(modelInfo);
      } else if (config.type === 'coding') {
        categorizedModels.coding.push(modelInfo);
      } else {
        categorizedModels.advanced.push(modelInfo);
      }
    });

    // í´ë¼ìš°ë“œ ëª¨ë¸ë“¤
    if (process.env.OPENAI_API_KEY) {
      categorizedModels.cloud.push({
        id: 'gpt-4o',
        name: 'â˜ï¸ GPT-4o',
        provider: 'openai',
        description: 'OpenAI ìµœê³  ì„±ëŠ¥ ëª¨ë¸',
        available: true,
        type: 'cloud',
        speed: 'fast',
        cueBonus: 1
      });
    }

    if (process.env.ANTHROPIC_API_KEY) {
      categorizedModels.cloud.push({
        id: 'claude-3.5-sonnet',
        name: 'â˜ï¸ Claude 3.5 Sonnet',
        provider: 'anthropic',
        description: 'Anthropic ê³ í’ˆì§ˆ ëª¨ë¸',
        available: true,
        type: 'cloud',
        speed: 'fast',
        cueBonus: 1
      });
    }

    const allModels = [
      ...categorizedModels.ultraFast,
      ...categorizedModels.balanced,
      ...categorizedModels.coding,
      ...categorizedModels.advanced,
      ...categorizedModels.cloud
    ];

    res.json({
      success: true,
      ollamaService: {
        version: 'advanced',
        healthCheck,
        connectionStatus: ollamaService.getConnectionStatus(),
        cachedModels: cachedModels.length,
        recommendedModels: recommendedModels.filter(m => m.recommended).length
      },
      ollama: {
        connected: healthCheck.connected,
        models: ollamaModels.length,
        availableModels: ollamaModels,
        cached: cachedModels.length
      },
      categories: categorizedModels,
      models: allModels,
      totalModels: allModels.length,
      recommended: allModels.find(m => m.recommended)?.id || 'llama3.2:1b'
    });

  } catch (error: any) {
    console.error('ê³ ê¸‰ ëª¨ë¸ ëª©ë¡ ì˜¤ë¥˜:', error);
    res.json({
      success: false,
      error: 'Failed to retrieve models',
      models: []
    });
  }
}));

// ê³ ê¸‰ Ollama ìƒíƒœ í™•ì¸
router.get('/ollama/health', asyncHandler(async (req, res) => {
  try {
    const healthCheck = await ollamaService.healthCheck();
    const connectionStatus = ollamaService.getConnectionStatus();
    
    res.json({
      success: true,
      ...healthCheck,
      connectionDetails: connectionStatus,
      features: {
        caching: true,
        streaming: true,
        modelManagement: true,
        advancedErrorHandling: true
      }
    });
  } catch (error: any) {
    res.json({
      success: false,
      connected: false,
      error: error.message,
      status: 'error'
    });
  }
}));

// ëª¨ë¸ ê´€ë¦¬ API (ê³ ê¸‰ Ollama ì„œë¹„ìŠ¤ í™œìš©)
router.post('/ollama/pull/:modelName', asyncHandler(async (req, res) => {
  try {
    const { modelName } = req.params;
    await ollamaService.pullModel(modelName);
    
    res.json({
      success: true,
      message: `Model ${modelName} download started`,
      modelName
    });
  } catch (error: any) {
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
}));

router.delete('/ollama/model/:modelName', asyncHandler(async (req, res) => {
  try {
    const { modelName } = req.params;
    await ollamaService.deleteModel(modelName);
    
    res.json({
      success: true,
      message: `Model ${modelName} deleted successfully`,
      modelName
    });
  } catch (error: any) {
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
}));

router.get('/ollama/model/:modelName/info', asyncHandler(async (req, res) => {
  try {
    const { modelName } = req.params;
    const modelInfo = await ollamaService.getModelInfo(modelName);
    
    res.json({
      success: true,
      modelInfo
    });
  } catch (error: any) {
    res.status(404).json({
      success: false,
      error: error.message
    });
  }
}));

function getModelDescription(modelName: string, config: any): string {
  const descriptions: Record<string, string> = {
    'llama3.2:1b': 'ì´ˆê³ ì† 1B ëª¨ë¸ - 1.3GB, ì‹¤ì‹œê°„ ì‘ë‹µ, +3 CUE (ìºì‹±ë¨)',
    'llama3.2:3b': 'ìµœì í™”ëœ 3B ëª¨ë¸ - 2GB, ë¹ ë¥¸ ì‘ë‹µ, +2 CUE (ìºì‹±ë¨)',
    'llama3.1:8b': 'ê³ í’ˆì§ˆ 8B ëª¨ë¸ - 4.9GB, ê· í˜•ì¡íŒ ì„±ëŠ¥, +1 CUE',
    'phi3:mini': 'íš¨ìœ¨ì ì¸ ì†Œí˜• ëª¨ë¸ - 2.2GB, Microsoft, +3 CUE (ìºì‹±ë¨)',
    'deepseek-coder:6.7b': 'ì½”ë”© ì „ë¬¸ AI - 3.8GB, í”„ë¡œê·¸ë˜ë° íŠ¹í™”, +1 CUE',
    'deepseek-coder:33b': 'ìµœê³ ê¸‰ ì½”ë”© AI - 18GB, ì „ë¬¸ê°€ê¸‰, +1 CUE',
    'mistral:latest': 'ìœ ëŸ½ì‚° ê³ í’ˆì§ˆ ëª¨ë¸ - 4.1GB, +1 CUE',
    'mixtral:8x7b': 'í˜¼í•© ì „ë¬¸ê°€ ëª¨ë¸ - 26GB, ìµœê³  ì„±ëŠ¥, +1 CUE'
  };

  return descriptions[modelName] || `${config.type} íŠ¹í™” ëª¨ë¸ - ${config.speed} ì†ë„, +${config.cueBonus} CUE (ê³ ê¸‰ ì„œë¹„ìŠ¤)`;
}

console.log('âœ… ê³ ê¸‰ AI Routes (Advanced Ollama Service + Complete Chat API) ë¡œë“œ ì™„ë£Œ');
export default router;