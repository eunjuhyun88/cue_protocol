// ============================================================================
// ğŸ“ backend/src/routes/ai/chat.ts  
// ğŸ’¬ Ollama ì „ìš© AI ì±„íŒ… ë¼ìš°íŠ¸ - OpenAI, Claude ì œê±°
// ============================================================================

import express, { Request, Response, Router } from 'express';
import { 
  getService, 
  getAIService, 
  getCueService, 
  getDatabaseService,
  getOllamaService,
  getAuthService 
} from '../../core/DIContainer';

const router: Router = express.Router();

// ============================================================================
// ğŸ”§ DI ì„œë¹„ìŠ¤ í—¬í¼ í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ì ‘ê·¼ - Ollama ì „ìš©)
// ============================================================================

function safeGetOllamaService() {
  try {
    return getOllamaService();
  } catch (error) {
    console.warn('âš ï¸ OllamaServiceë¥¼ DIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜');
    return {
      checkConnection: async () => false,
      getModels: async () => [],
      chatCompletion: async () => 'Ollama ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
      getStatus: async () => ({ available: false, error: 'Service not available' }),
      getDetailedStatus: async () => ({ connection: { available: false } })
    };
  }
}

function safeGetAIService() {
  try {
    return getAIService();
  } catch (error) {
    console.warn('âš ï¸ AIServiceë¥¼ DIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜');
    return {
      sendMessage: async (message: string, options: any) => ({
        response: `Ollama Echo: ${message}`,
        model: 'mock',
        timestamp: new Date().toISOString(),
        provider: 'ollama-mock',
        local: true,
        privacy: 'local-processing-only'
      }),
      getPersonalizedContext: async () => ({}),
      getChatHistory: async () => [],
      getStatus: async () => ({ status: 'mock', available: false })
    };
  }
}

function safeGetCueService() {
  try {
    return getCueService();
  } catch (error) {
    console.warn('âš ï¸ CueServiceë¥¼ DIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜');
    return {
      calculateReward: async () => 5,
      awardTokens: async () => ({ success: true, tokens: 5 }),
      getBalance: async () => 0,
      getStatus: async () => ({ status: 'mock', available: true })
    };
  }
}

function safeGetDatabaseService() {
  try {
    return getDatabaseService();
  } catch (error) {
    console.warn('âš ï¸ DatabaseServiceë¥¼ DIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜');
    return {
      query: async () => ({ data: [], error: null }),
      insert: async () => ({ data: null, error: null }),
      update: async () => ({ data: null, error: null })
    };
  }
}

// ============================================================================
// ğŸ›¡ï¸ ì¸ì¦ ë¯¸ë“¤ì›¨ì–´ (ê°„ì†Œí™”)
// ============================================================================

const authMiddleware = (req: Request, res: Response, next: express.NextFunction) => {
  try {
    // Authorization í—¤ë” í™•ì¸
    const authHeader = req.headers.authorization;
    if (!authHeader || !authHeader.startsWith('Bearer ')) {
      console.log('âš ï¸ ì¸ì¦ í† í° ì—†ìŒ, ìµëª… ì‚¬ìš©ìë¡œ ì²˜ë¦¬');
      (req as any).user = { 
        did: `anonymous_${Date.now()}`, 
        username: 'Anonymous',
        authenticated: false 
      };
      return next();
    }

    // í† í° íŒŒì‹± (ê°„ë‹¨í•œ êµ¬í˜„)
    const token = authHeader.substring(7);
    if (token === 'test-token' || token.length > 10) {
      (req as any).user = { 
        did: `user_${Date.now()}`, 
        username: 'TestUser',
        authenticated: true 
      };
    } else {
      (req as any).user = { 
        did: `anonymous_${Date.now()}`, 
        username: 'Anonymous',
        authenticated: false 
      };
    }
    
    next();
  } catch (error) {
    console.error('ğŸ’¥ ì¸ì¦ ë¯¸ë“¤ì›¨ì–´ ì˜¤ë¥˜:', error);
    (req as any).user = { 
      did: `error_${Date.now()}`, 
      username: 'Error',
      authenticated: false 
    };
    next();
  }
};

// ============================================================================
// ğŸš€ í•µì‹¬ Ollama AI ì±„íŒ… API
// ============================================================================

/**
 * Ollama AI ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡
 * POST /api/ai/chat
 */
router.post('/chat', authMiddleware, async (req: Request, res: Response): Promise<void> => {
  const startTime = Date.now();
  
  try {
    const user = (req as any).user;
    const { 
      message, 
      model = 'llama3.2:3b', 
      includeContext = true,
      temperature = 0.7,
      maxTokens = 2000 
    } = req.body;

    console.log(`ğŸ’¬ === Ollama AI ì±„íŒ… ìš”ì²­: ${user.did} ===`);
    console.log(`ğŸ“ ë©”ì‹œì§€: "${message}"`);
    console.log(`ğŸ¦™ ëª¨ë¸: ${model}`);

    // ì…ë ¥ ê²€ì¦
    if (!message || typeof message !== 'string' || message.trim().length === 0) {
      res.status(400).json({
        success: false,
        error: 'Message is required and must be a non-empty string'
      });
      return;
    }

    // DI ì„œë¹„ìŠ¤ë“¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    const ollamaService = safeGetOllamaService();
    const aiService = safeGetAIService();
    const cueService = safeGetCueService();

    // Ollama ì—°ê²° ìƒíƒœ í™•ì¸
    console.log('ğŸ” Ollama ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...');
    const isOllamaConnected = await ollamaService.checkConnection();
    
    if (!isOllamaConnected) {
      console.log('âš ï¸ Ollama ì„œë²„ ì—°ê²° ë¶ˆê°€, ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜');
      
      // Ollama ì—°ê²° ì‹¤íŒ¨ ì‹œ ë„ì›€ë§ í¬í•¨í•œ ì‘ë‹µ
      const helpResponse = {
        success: true,
        response: `ğŸ¦™ Ollama AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•:**
1. \`ollama serve\` ëª…ë ¹ì–´ë¡œ Ollama ì„œë²„ ì‹œì‘
2. \`ollama pull ${model}\` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
3. Ollama ì„¤ì¹˜ê°€ í•„ìš”í•œ ê²½ìš°: \`brew install ollama\`

**ë‹¹ì‹ ì˜ ì§ˆë¬¸:** "${message}"

í˜„ì¬ ë¡œì»¬ AI ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ë©´ ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ ë³´ì¥ í•˜ì— ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
CUE Protocolì˜ AI PassportëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œì»¬ì—ì„œë§Œ ì²˜ë¦¬í•˜ì—¬ ê°œì¸ì •ë³´ë¥¼ 100% ë³´í˜¸í•©ë‹ˆë‹¤.`,
        model: `${model} (ì—°ê²° ëŒ€ê¸°)`,
        timestamp: new Date().toISOString(),
        cueReward: 2, // ê¸°ë³¸ ë³´ìƒ
        trustScore: 0.5,
        processingTime: Date.now() - startTime,
        provider: 'ollama',
        local: true,
        privacy: 'local-processing-only',
        debug: {
          ollamaConnected: false,
          suggestion: 'Start Ollama server and pull model'
        }
      };
      
      res.json(helpResponse);
      return;
    }

    // Ollama AI ì‘ë‹µ ìƒì„±
    console.log('ğŸ¤– Ollama AI ì„œë¹„ìŠ¤ë¡œ ë©”ì‹œì§€ ì „ì†¡ ì¤‘...');
    const aiResponse = await aiService.sendMessage(message, {
      model,
      userDid: user.did,
      includeContext,
      temperature,
      maxTokens
    });

    // CUE í† í° ë³´ìƒ ê³„ì‚° (ë¡œì»¬ AI ì‚¬ìš© ë³´ë„ˆìŠ¤ í¬í•¨)
    console.log('âš¡ CUE í† í° ë³´ìƒ ê³„ì‚° ì¤‘...');
    const baseReward = await cueService.calculateReward({
      action: 'ollama_chat',
      quality: aiResponse.qualityScore || 0.8,
      messageLength: message.length,
      responseLength: aiResponse.response?.length || 0
    });
    
    // ë¡œì»¬ AI ì‚¬ìš© í”„ë¼ì´ë²„ì‹œ ë³´ë„ˆìŠ¤ ì¶”ê°€
    const privacyBonus = 3; // ë¡œì»¬ AI ì‚¬ìš© ë³´ë„ˆìŠ¤
    const totalReward = baseReward + privacyBonus;

    // CUE í† í° ì§€ê¸‰
    const awardResult = await cueService.awardTokens(user.did, totalReward, {
      type: 'ollama_chat',
      metadata: {
        model,
        messageId: aiResponse.messageId || `msg_${Date.now()}`,
        conversationId: aiResponse.conversationId,
        privacyBonus: privacyBonus
      }
    });

    const processingTime = Date.now() - startTime;

    console.log(`âœ… Ollama AI ì±„íŒ… ì™„ë£Œ (${processingTime}ms)`);
    console.log(`ğŸ CUE ë³´ìƒ: ${totalReward} í† í° (í”„ë¼ì´ë²„ì‹œ ë³´ë„ˆìŠ¤ +${privacyBonus})`);

    // í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ ì‘ë‹µ í¬ë§·
    res.json({
      success: true,
      response: aiResponse.response || `Ollama Echo: ${message}`,
      model: aiResponse.model || model,
      timestamp: new Date().toISOString(),
      cueReward: totalReward,
      trustScore: aiResponse.trustScore || 0.8,
      contextLearned: aiResponse.contextLearned || false,
      qualityScore: aiResponse.qualityScore || 0.8,
      processingTime,
      tokensUsed: aiResponse.tokensUsed || 0,
      // Ollama ì „ìš© ì¶”ê°€ ë©”íƒ€ë°ì´í„°
      provider: 'ollama',
      local: true,
      privacy: 'local-processing-only',
      user: {
        did: user.did,
        authenticated: user.authenticated
      },
      aiMetadata: {
        messageId: aiResponse.messageId || `msg_${Date.now()}`,
        conversationId: aiResponse.conversationId || `conv_${Date.now()}`,
        personalityMatch: aiResponse.personalityMatch || 0.7,
        privacyBonus: privacyBonus
      },
      debug: {
        ollamaConnected: true,
        provider: 'ollama',
        modelUsed: aiResponse.model || model,
        localProcessing: true
      }
    });

  } catch (error) {
    const processingTime = Date.now() - startTime;
    
    console.error('ğŸ’¥ Ollama AI ì±„íŒ… ì˜¤ë¥˜:', error);
    
    res.status(500).json({
      success: false,
      error: 'Ollama AI chat failed',
      message: error.message,
      processingTime,
      timestamp: new Date().toISOString(),
      provider: 'ollama',
      suggestion: 'Check if Ollama server is running and model is available'
    });
  }
});

// ============================================================================
// ğŸ“‹ Ollama ëª¨ë¸ ëª©ë¡ API  
// ============================================================================

/**
 * ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
 * GET /api/ai/models
 */
router.get('/models', async (req: Request, res: Response): Promise<void> => {
  try {
    console.log('ğŸ“‹ === Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ===');

    const ollamaService = safeGetOllamaService();
    
    // Ollama ì—°ê²° í™•ì¸
    const isConnected = await ollamaService.checkConnection();
    
    if (!isConnected) {
      res.json({
        success: false,
        message: 'Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
        models: [],
        instructions: {
          install: 'brew install ollama',
          start: 'ollama serve',
          pullModel: 'ollama pull llama3.2:3b',
          checkConnection: 'curl http://localhost:11434/api/tags'
        },
        recommended: [
          {
            id: 'llama3.2:3b',
            name: 'Llama 3.2 3B',
            size: '2.0GB',
            description: 'ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ëŒ€í™”í˜• ëª¨ë¸ (ì¶”ì²œ)',
            command: 'ollama pull llama3.2:3b'
          },
          {
            id: 'llama3.2:1b',
            name: 'Llama 3.2 1B',
            size: '1.3GB',
            description: 'ë§¤ìš° ë¹ ë¥¸ ê²½ëŸ‰ ëª¨ë¸',
            command: 'ollama pull llama3.2:1b'
          }
        ]
      });
      return;
    }

    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    const availableModels = await ollamaService.getModels();
    
    // ëª¨ë¸ ì •ë³´ êµ¬ì„±
    const modelList = availableModels.map(modelName => {
      const isRecommended = ['llama3.2:3b', 'llama3.2:1b'].includes(modelName);
      const size = modelName.includes(':1b') ? '1B' : 
                   modelName.includes(':2b') ? '2B' :
                   modelName.includes(':3b') ? '3B' :
                   modelName.includes(':7b') ? '7B' :
                   modelName.includes(':8b') ? '8B' : 'Unknown';
      
      return {
        id: modelName,
        name: `${modelName.split(':')[0].toUpperCase()} (${size})`,
        available: true,
        recommended: isRecommended,
        type: 'local',
        provider: 'ollama',
        description: `ë¡œì»¬ AI ëª¨ë¸ - ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ ë³´ì¥`,
        speed: modelName.includes(':1b') ? 'very-fast' :
               modelName.includes(':3b') ? 'fast' : 'moderate',
        privacy: 'local-processing-only'
      };
    });

    // ê¸°ë³¸ ì¶”ì²œ ëª¨ë¸ë„ í¬í•¨ (ë‹¤ìš´ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°)
    const baseModels = [
      { 
        id: 'llama3.2:3b', 
        name: 'Llama 3.2 3B', 
        available: availableModels.includes('llama3.2:3b'),
        recommended: true,
        type: 'local',
        provider: 'ollama',
        description: 'ì¶”ì²œ: ë¹ ë¥´ê³  ì •í™•í•œ ëŒ€í™”í˜• ëª¨ë¸',
        speed: 'fast',
        size: '2.0GB',
        privacy: 'local-processing-only',
        command: availableModels.includes('llama3.2:3b') ? null : 'ollama pull llama3.2:3b'
      },
      { 
        id: 'llama3.2:1b', 
        name: 'Llama 3.2 1B', 
        available: availableModels.includes('llama3.2:1b'),
        recommended: true,
        type: 'local',
        provider: 'ollama',
        description: 'ì¶”ì²œ: ë§¤ìš° ë¹ ë¥¸ ê²½ëŸ‰ ëª¨ë¸',
        speed: 'very-fast',
        size: '1.3GB',
        privacy: 'local-processing-only',
        command: availableModels.includes('llama3.2:1b') ? null : 'ollama pull llama3.2:1b'
      }
    ];
    
    // ì¤‘ë³µ ì œê±°í•˜ì—¬ ê²°í•©
    const allModels = [...baseModels];
    modelList.forEach(model => {
      if (!allModels.find(m => m.id === model.id)) {
        allModels.push(model);
      }
    });

    res.json({
      success: true,
      connected: isConnected,
      ollama: {
        connected: true,
        host: process.env.OLLAMA_HOST || 'http://localhost:11434',
        modelCount: availableModels.length,
        availableModels: availableModels
      },
      models: allModels,
      totalModels: allModels.length,
      availableModels: allModels.filter(m => m.available).length,
      privacy: 'All models process data locally - 100% privacy guaranteed'
    });

  } catch (error) {
    console.error('Error getting Ollama models:', error);
    res.json({
      success: false,
      error: 'Failed to retrieve Ollama models',
      models: [
        { 
          id: 'llama3.2:3b', 
          name: 'Llama 3.2 3B', 
          available: false, 
          recommended: true,
          note: 'ollama pull llama3.2:3b ëª…ë ¹ì–´ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”',
          command: 'ollama pull llama3.2:3b'
        }
      ],
      instructions: {
        install: 'brew install ollama',
        start: 'ollama serve',
        pullModel: 'ollama pull llama3.2:3b'
      }
    });
  }
});

// ============================================================================
// ğŸ“Š Ollama ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ API
// ============================================================================

/**
 * Ollama ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
 * GET /api/ai/status
 */
router.get('/status', async (req: Request, res: Response): Promise<void> => {
  try {
    console.log('ğŸ” === Ollama ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ===');

    const ollamaService = safeGetOllamaService();
    const aiService = safeGetAIService();
    const cueService = safeGetCueService();

    const [ollamaStatus, aiStatus, cueStatus] = await Promise.all([
      ollamaService.getDetailedStatus(),
      aiService.getStatus(),
      cueService.getStatus()
    ]);

    res.json({
      success: true,
      status: {
        ollama: ollamaStatus,
        ai: aiStatus,
        cue: cueStatus,
        features: {
          localProcessing: true,
          noDataCollection: true,
          offlineCapable: true,
          privacyGuaranteed: true,
          ollamaIntegration: true,
          cueRewards: true,
          personalization: true
        },
        version: '3.0-ollama-only',
        uptime: process.uptime(),
        provider: 'ollama',
        privacy: 'local-processing-only'
      },
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('ğŸ’¥ Ollama ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜:', error);
    
    res.status(500).json({
      success: false,
      error: 'Failed to get Ollama system status',
      message: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

// ============================================================================
// ğŸ” ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ API (Ollama ìµœì í™”)
// ============================================================================

/**
 * ì‚¬ìš©ì ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ  
 * GET /api/ai/personalization/context
 */
router.get('/personalization/context', authMiddleware, async (req: Request, res: Response): Promise<void> => {
  try {
    const user = (req as any).user;
    
    console.log(`ğŸ§  === ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ: ${user.did} ===`);

    const aiService = safeGetAIService();
    const context = await aiService.getPersonalizedContext(user.did);

    res.json({
      success: true,
      context: {
        personalityProfile: context.personalityProfile || {
          type: 'Privacy-Focused User',
          communicationStyle: 'Direct',
          learningPattern: 'Visual',
          decisionMaking: 'Analytical',
          aiPreference: 'Local AI (Ollama)'
        },
        totalCues: context.cues?.length || 0,
        vaultsCount: context.vaultIds?.length || 0,
        behaviorPatterns: context.behaviorPatterns || ['Ollama ì‚¬ìš© ì„ í˜¸', 'í”„ë¼ì´ë²„ì‹œ ì¤‘ì‹œ'],
        recentInteractions: context.recentInteractions || [],
        personalityMatch: context.personalityMatch || 0.8,
        preferences: {
          ...context.preferences,
          aiProvider: 'ollama',
          privacyLevel: 'maximum',
          localProcessing: true
        }
      },
      user: {
        did: user.did,
        authenticated: user.authenticated
      },
      timestamp: new Date().toISOString(),
      privacy: 'All personalization data stays local'
    });

  } catch (error) {
    console.error('ğŸ’¥ ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜:', error);
    
    res.status(500).json({
      success: false,
      error: 'Failed to get personalization context',
      message: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

// ============================================================================
// ğŸ“š ëŒ€í™” ê¸°ë¡ ì¡°íšŒ API (Ollama ì „ìš©)
// ============================================================================

/**
 * Ollama ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
 * GET /api/ai/history?page=1&limit=20
 */
router.get('/history', authMiddleware, async (req: Request, res: Response): Promise<void> => {
  try {
    const user = (req as any).user;
    const { page = 1, limit = 20, conversationId } = req.query;

    console.log(`ğŸ“– === Ollama ëŒ€í™” ê¸°ë¡ ì¡°íšŒ: ${user.did} ===`);

    const aiService = safeGetAIService();
    const history = await aiService.getChatHistory(user.did, {
      page: parseInt(page as string),
      limit: parseInt(limit as string),
      conversationId: conversationId as string,
      provider: 'ollama'
    });

    res.json({
      success: true,
      history: history || [],
      pagination: {
        page: parseInt(page as string),
        limit: parseInt(limit as string),
        total: history?.length || 0
      },
      user: {
        did: user.did,
        authenticated: user.authenticated
      },
      privacy: 'All conversation history stored locally',
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('ğŸ’¥ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ ì˜¤ë¥˜:', error);
    
    res.status(500).json({
      success: false,
      error: 'Failed to get chat history',
      message: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

// ============================================================================
// ğŸ¦™ Ollama í—¬ìŠ¤ì²´í¬ API
// ============================================================================

/**
 * Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
 * GET /api/ai/ollama/health
 */
router.get('/ollama/health', async (req: Request, res: Response): Promise<void> => {
  try {
    console.log('ğŸ” === Ollama í—¬ìŠ¤ì²´í¬ ===');

    const ollamaService = safeGetOllamaService();
    const healthResult = await ollamaService.testConnection();

    res.json({
      success: healthResult.success,
      ...healthResult,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('ğŸ’¥ Ollama í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜:', error);
    
    res.json({
      success: false,
      error: 'Ollama health check failed',
      message: error.message,
      timestamp: new Date().toISOString(),
      instructions: {
        install: 'brew install ollama',
        start: 'ollama serve',
        pullModel: 'ollama pull llama3.2:3b'
      }
    });
  }
});

// ============================================================================
// ğŸ“– Ollama ì „ìš© API ì‚¬ìš© ê°€ì´ë“œ
// ============================================================================

/**
 * Ollama AI Chat API ì‚¬ìš© ê°€ì´ë“œ
 * GET /api/ai/guide
 */
router.get('/guide', (req: Request, res: Response): void => {
  res.json({
    success: true,
    service: 'Ollama ì „ìš© AI ì±„íŒ… ì„œë¹„ìŠ¤',
    version: '3.0-ollama-only',
    
    features: {
      core: [
        'âœ… Ollama ë¡œì»¬ AI ëª¨ë¸ ì „ìš© ì§€ì›',
        'âœ… 100% ë¡œì»¬ ë°ì´í„° ì²˜ë¦¬ (ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ)',
        'âœ… ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ',
        'âœ… ì‹¤ì‹œê°„ CUE í† í° ë§ˆì´ë‹ (í”„ë¼ì´ë²„ì‹œ ë³´ë„ˆìŠ¤ í¬í•¨)',
        'âœ… í’ˆì§ˆ ê¸°ë°˜ ë³´ìƒ ì‹œìŠ¤í…œ',
        'âœ… ëŒ€í™” ê¸°ë¡ ê´€ë¦¬',
        'âœ… DI Container ì™„ì „ ì—°ë™'
      ],
      privacy: [
        'ğŸ”’ ëª¨ë“  AI ì²˜ë¦¬ê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰',
        'ğŸ”’ ë°ì´í„°ê°€ ì™¸ë¶€ ì„œë²„ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ',
        'ğŸ”’ ì¸í„°ë„· ì—†ì´ë„ AI ì‚¬ìš© ê°€ëŠ¥',
        'ğŸ”’ ê°œì¸ ì •ë³´ 100% ë³´í˜¸'
      ],
      personalization: [
        'ğŸ§  ê°œì¸ ì„±ê²© í”„ë¡œí•„ í•™ìŠµ',
        'ğŸ§  ëŒ€í™” íŒ¨í„´ ë¶„ì„',
        'ğŸ§  ì„ í˜¸ë„ í•™ìŠµ ë° ì ìš©',
        'ğŸ§  ë¡œì»¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±'
      ],
      mining: [
        'ğŸ’° ëŒ€í™” ì°¸ì—¬ ê¸°ë³¸ ë³´ìƒ',
        'ğŸ’° ì‘ë‹µ í’ˆì§ˆ ê¸°ë°˜ ë³´ë„ˆìŠ¤',
        'ğŸ’° ë¡œì»¬ AI ì‚¬ìš© í”„ë¼ì´ë²„ì‹œ ë³´ë„ˆìŠ¤ (+3 CUE)',
        'ğŸ’° ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ë³´ë„ˆìŠ¤',
        'ğŸ’° ê°œì¸ ë‹¨ì„œ ì¶”ì¶œ ë³´ë„ˆìŠ¤'
      ]
    },
    
    endpoints: {
      chat: {
        'POST /chat': 'Ollama AI ì±„íŒ… (ì¸ì¦ ê¶Œì¥)',
        'GET /models': 'ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡',
        'GET /history': 'ëŒ€í™” ê¸°ë¡ ì¡°íšŒ',
        'GET /personalization/context': 'ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸'
      },
      system: {
        'GET /status': 'ì‹œìŠ¤í…œ ìƒíƒœ',
        'GET /ollama/health': 'Ollama í—¬ìŠ¤ì²´í¬',
        'GET /guide': 'ì´ ê°€ì´ë“œ'
      }
    },
    
    usage: {
      basicChat: {
        method: 'POST',
        endpoint: '/chat',
        headers: { 
          'Content-Type': 'application/json',
          'Authorization': 'Bearer YOUR_TOKEN (ì„ íƒì‚¬í•­)'
        },
        body: {
          message: 'ì•ˆë…•í•˜ì„¸ìš”',
          model: 'llama3.2:3b',
          includeContext: true
        }
      },
      advancedChat: {
        method: 'POST',
        endpoint: '/chat',
        body: {
          message: 'ë³µì¡í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤',
          model: 'llama3.2:3b',
          includeContext: true,
          temperature: 0.8,
          maxTokens: 2000
        }
      }
    },
    
    ollama: {
      installation: {
        mac: 'brew install ollama',
        linux: 'curl -fsSL https://ollama.ai/install.sh | sh',
        windows: 'Download from ollama.ai'
      },
      commands: {
        startServer: 'ollama serve',
        pullModel: 'ollama pull llama3.2:3b',
        listModels: 'ollama list',
        checkHealth: 'curl http://localhost:11434/api/tags'
      },
      recommendedModels: [
        {
          name: 'llama3.2:3b',
          size: '2.0GB',
          description: 'ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ëŒ€í™”í˜• ëª¨ë¸ (ì¶”ì²œ)',
          command: 'ollama pull llama3.2:3b'
        },
        {
          name: 'llama3.2:1b',
          size: '1.3GB',
          description: 'ë§¤ìš° ë¹ ë¥¸ ê²½ëŸ‰ ëª¨ë¸',
          command: 'ollama pull llama3.2:1b'
        }
      ]
    },
    
    rewards: {
      base: '5 CUE (ê¸°ë³¸ ëŒ€í™” ì°¸ì—¬)',
      quality: '0-10 CUE (ì‘ë‹µ í’ˆì§ˆ ê¸°ë°˜)',
      privacy: '3 CUE (ë¡œì»¬ AI ì‚¬ìš© ë³´ë„ˆìŠ¤)',
      context: '3 CUE (ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)',
      extraction: '1-5 CUE (ìƒˆë¡œìš´ ê°œì¸ ë‹¨ì„œ ì¶”ì¶œ)'
    },
    
    authentication: {
      required: false,
      recommended: true,
      anonymous: 'ìµëª… ì‚¬ìš©ìë„ ì‚¬ìš© ê°€ëŠ¥ (ê¸°ëŠ¥ ì œí•œ)',
      bearer: 'Authorization: Bearer TOKEN'
    },
    
    privacy: {
      guarantee: '100% ë¡œì»¬ ì²˜ë¦¬',
      dataStorage: 'ëª¨ë“  ë°ì´í„°ê°€ ì‚¬ìš©ì ê¸°ê¸°ì—ë§Œ ì €ì¥',
      internetRequired: 'ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì—ë§Œ í•„ìš”',
      thirdParty: 'ì œ3ì ì„œë²„ì™€ ë°ì´í„° ê³µìœ  ì—†ìŒ'
    },
    
    troubleshooting: {
      connectionIssues: {
        problem: 'Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨',
        solutions: [
          'ollama serve ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘',
          'í¬íŠ¸ 11434ê°€ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸',
          'Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸'
        ]
      },
      modelIssues: {
        problem: 'ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ',
        solutions: [
          'ollama pull llama3.2:3bë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ',
          'ollama listë¡œ ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸',
          'ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ í™•ë³´'
        ]
      }
    },
    
    frontendIntegration: {
      baseURL: 'http://localhost:3001/api/ai',
      examples: {
        fetch: `
fetch('/api/ai/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ 
    message: 'ì•ˆë…•í•˜ì„¸ìš”', 
    model: 'llama3.2:3b' 
  })
})`,
        axios: `
axios.post('/api/ai/chat', { 
  message: 'ì•ˆë…•í•˜ì„¸ìš”', 
  model: 'llama3.2:3b' 
})`
      }
    },
    
    note: 'Ollama ì „ìš© AI ì„œë¹„ìŠ¤ë¡œ ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. ëª¨ë“  AI ì²˜ë¦¬ê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ë©°, ê°œì¸ ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
  });
});

console.log('âœ… Ollama ì „ìš© AI Chat routes initialized');
export default router;