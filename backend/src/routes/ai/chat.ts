// ============================================================================
// ğŸ¤– AI ì±„íŒ… ë¼ìš°íŠ¸ (ìˆ˜ì •ëœ ì™„ì „í•œ êµ¬í˜„)
// ê²½ë¡œ: backend/src/routes/ai/chat.ts
// ìš©ë„: AI ì±„íŒ…, ê°œì¸í™”, CUE ë§ˆì´ë‹ í†µí•© API
// ìˆ˜ì •ì‚¬í•­: ë©”ì„œë“œ í†µì¼, ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ , DatabaseServiceì™€ SupabaseService í˜¸í™˜
// ============================================================================
import { ollamaService } from '../../services/ollama';
import express from 'express';
import { v4 as uuidv4 } from 'uuid';
import { DatabaseService } from '../../services/database/DatabaseService';
import { supabaseService } from '../../services/database/SupabaseService';
import { SemanticCompressionService } from '../../services/ai/SemanticCompressionService';
import { PersonalizationService } from '../../services/ai/PersonalizationService';
import { asyncHandler } from '../../middleware/errorHandler';
import { PersonalCueExtractor } from '../../services/ai/PersonalCueExtractor';
const cueExtractor = new PersonalCueExtractor();

const router = express.Router();

// ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ ì„ íƒ (í™˜ê²½ì— ë”°ë¼ ìë™ ì„ íƒ)
const db = process.env.USE_MOCK_DATABASE === 'true' || 
          !process.env.SUPABASE_URL || 
          process.env.SUPABASE_URL.includes('dummy')
  ? DatabaseService.getInstance()
  : supabaseService;

// AI í´ë¼ì´ì–¸íŠ¸ë“¤ - ì§€ì—° ë¡œë”©
let openaiClient: any = null;
let anthropicClient: any = null;
let openaiAttempted = false;
let anthropicAttempted = false;

console.log('ğŸ¤– AI Routes module loaded - using', db.constructor.name);

// ============================================================================
// ğŸ” ì•ˆì „í•œ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
// ============================================================================
async function getOpenAIClient() {
  if (openaiAttempted) {
    return openaiClient;
  }

  openaiAttempted = true;

  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey || apiKey.trim() === '' || apiKey === 'your-openai-key-here') {
    console.log('âš ï¸ OpenAI API key not configured - will use mock responses');
    return null;
  }

  try {
    console.log('ğŸ”„ Dynamically importing OpenAI...');
    const { default: OpenAI } = await import('openai');
    
    console.log('ğŸ”„ Creating OpenAI client...');
    openaiClient = new OpenAI({ apiKey });
    
    console.log('âœ… OpenAI client created successfully');
    return openaiClient;
  } catch (error: any) {
    console.error('âŒ Failed to create OpenAI client:', error.message);
    openaiClient = null;
    return null;
  }
}

// ============================================================================
// ğŸ” ì•ˆì „í•œ Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„±
// ============================================================================
async function getAnthropicClient() {
  if (anthropicAttempted) {
    return anthropicClient;
  }

  anthropicAttempted = true;

  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey || apiKey.trim() === '' || apiKey === 'your-anthropic-key-here') {
    console.log('âš ï¸ Anthropic API key not configured - will use mock responses');
    return null;
  }

  try {
    console.log('ğŸ”„ Dynamically importing Anthropic...');
    const { default: Anthropic } = await import('@anthropic-ai/sdk');
    
    console.log('ğŸ”„ Creating Anthropic client...');
    anthropicClient = new Anthropic({ apiKey });
    
    console.log('âœ… Anthropic client created successfully');
    return anthropicClient;
  } catch (error: any) {
    console.error('âŒ Failed to create Anthropic client:', error.message);
    anthropicClient = null;
    return null;
  }
}

// ============================================================================
// ğŸ¤– AI ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ìˆ˜ì •ë¨)
// ============================================================================
router.post('/chat', asyncHandler(async (req, res) => {
  console.log('ğŸ¯ === AI CHAT ë¼ìš°íŠ¸ ì‹œì‘ ===');
  console.log('ğŸ“ Request body:', req.body);
  
  const { message, model = 'personalized-agent', conversationId, userId, passportData } = req.body;
  
  console.log('ğŸ” ì¶”ì¶œëœ ê°’ë“¤:', {
    message: message?.slice(0, 50),
    model,
    userId,
    hasPassportData: !!passportData
  });
   const userDid = (req as any).user?.did || 
                  (passportData?.did) || 
                  (userId ? `did:final0626:${userId}` : null);

  console.log(`ğŸ¯ AI Chat Request: ${model} for user ${userDid?.slice(0, 20)}...`);
  
router.post('/chat', asyncHandler(async (req, res) => {
  const { message, model = 'personalized-agent', conversationId, userId, passportData } = req.body;
  
  // ì‚¬ìš©ì ì •ë³´ í™•ì¸ (req.user ë˜ëŠ” bodyì—ì„œ)
  const userDid = (req as any).user?.did || 
                  (passportData?.did) || 
                  (userId ? `did:final0626:${userId}` : null);

  console.log(`ğŸ¯ AI Chat Request: ${model} for user ${userDid?.slice(0, 20)}...`);

  if (!message || !message.trim()) {
    return res.status(400).json({
      success: false,
      error: 'Message is required'
    });
  }

  if (!userDid) {
    return res.status(400).json({
      success: false,
      error: 'User identification required (userId or authenticated user)'
    });
  }

  const startTime = Date.now();
  const currentConversationId = conversationId || uuidv4();

  try {
    // 1. ì‚¬ìš©ì ì •ë³´ í™•ì¸ ë° ìƒì„±
    console.log('ğŸ‘¤ Checking user information...');
    let user = null;
    
    if (userId) {
      // ìˆ˜ì •ë¨: getUserById ë©”ì„œë“œ ì‚¬ìš© (í†µì¼ë¨)
      user = await db.getUserById(userId);
      
      if (!user) {
        // ì‚¬ìš©ìê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‚¬ìš©ì ìƒì„±
        const newUserData = {
          id: userId,
          username: `user_${userId.slice(-8)}`,
          did: userDid,
          wallet_address: `0x${Math.random().toString(16).substring(2, 42)}`,
          passkey_registered: false,
          created_at: new Date().toISOString()
        };
        
        user = await db.createUser(newUserData);
        console.log(`âœ… ìƒˆ ì‚¬ìš©ì ìƒì„±: ${userId}`);
      }
    } else {
      // DIDë¡œ ì‚¬ìš©ì ì¡°íšŒ
      user = await db.getUserById(userDid);
    }

    // 2. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    console.log('ğŸ’¾ Storing user message...');
    const userMessageData = {
      id: uuidv4(),
      user_did: userDid,
      user_id: user?.id || userId,
      conversation_id: currentConversationId,
      message_type: 'user',
      content: message,
      ip_address: req.ip,
      user_agent: req.get('User-Agent')
    };

    await db.saveChatMessage(userMessageData);

    // 3. ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
    console.log('ğŸ§  Loading personalization context...');
    const personalizationService = new PersonalizationService(db as any);
    const personalContext = await personalizationService.getPersonalizedContext(userDid, message, {
      includeFullProfile: true,
      includeBehaviorPatterns: true,
      includeRecentInteractions: true
    });

    // 4. AI ì‘ë‹µ ìƒì„±
    console.log(`ğŸ¤– Generating ${model} response...`);
    let aiResult;

    switch (model) {
  case 'gpt-4o':
  case 'gpt-4':
    aiResult = await generateGPTResponse(message, personalContext);
    break;
  case 'claude-3.5-sonnet':
  case 'claude-sonnet':
    aiResult = await generateClaudeResponse(message, personalContext);
    break;
  case 'gemini-pro':
    aiResult = await generateGeminiResponse(message, personalContext);
    break;
  
  // ğŸ¦™ Ollama ë¡œì»¬ ëª¨ë¸ë“¤ ì¶”ê°€
  case 'llama3.2:3b':
  case 'llama3.2:1b':
  case 'llama3.1:8b':
  case 'gemma2:2b':
  case 'qwen2.5:3b':
  case 'qwen2.5:1.5b':
    aiResult = await generateOllamaResponse(message, personalContext, model);
    break;
  
  // ê¸°ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ Ollama ëª¨ë¸ ê°ì§€
  default:
    if (model.startsWith('llama') || model.startsWith('gemma') || model.startsWith('qwen') || model.startsWith('phi') || model.startsWith('mistral')) {
      aiResult = await generateOllamaResponse(message, personalContext, model);
    } else {
      // ê¸°ì¡´ ê°œì¸í™” ì—ì´ì „íŠ¸
      aiResult = await generatePersonalizedResponse(message, personalContext, userDid);
    }
    break;
}


    const responseTime = Date.now() - startTime;

// ìœ„ì¹˜: AI ì‘ë‹µ ìƒì„± ì™„ë£Œ í›„, ë©”ì‹œì§€ ì €ì¥ ì „ì— ì¶”ê°€
// ì¦‰, "const responseTime = Date.now() - startTime;" ë¼ì¸ ë‹¤ìŒì— ì¶”ê°€

    // âœ¨ Personal CUE ì¶”ì¶œ ë° ì €ì¥ (ìƒˆë¡œ ì¶”ê°€)
    console.log('ğŸ§  Personal CUE ì¶”ì¶œ ì‹œì‘...');
    const chatContext = {
      userMessage: message,
      aiResponse: aiResult.response,
      model,
      timestamp: new Date(),
      conversationId: currentConversationId
    };

    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ CUE ì¶”ì¶œ (ë¹„ë™ê¸° - ê¸°ì¡´ í”Œë¡œìš°ì— ì˜í–¥ ì—†ìŒ)
    setImmediate(async () => {
      try {
        const extractedCues = await cueExtractor.extractAndStoreCues(userDid, chatContext);
        console.log(`âœ… ${extractedCues.length}ê°œ ìƒˆë¡œìš´ CUE ì¶”ì¶œ ì™„ë£Œ`);
      } catch (error) {
        console.error('âŒ CUE ì¶”ì¶œ ì˜¤ë¥˜:', error);
      }
    });

    // 5. CUE í† í° ê³„ì‚° (ê°„ë‹¨í•œ ê³µì‹)
    console.log('â›ï¸ Calculating CUE tokens...');
    const baseTokens = Math.floor(message.length / 10);
    const personalityBonus = personalContext.personalityMatch > 0.7 ? 2 : 1;
    const contextBonus = Math.min(personalContext.cues.length, 5);
    const minedTokens = baseTokens + personalityBonus + contextBonus;

    // 6. AI ì‘ë‹µ ì €ì¥
    console.log('ğŸ’¾ Storing AI response...');
    const aiMessageData = {
      id: uuidv4(),
      user_did: userDid,
      user_id: user?.id || userId,
      conversation_id: currentConversationId,
      message_type: 'ai',
      content: aiResult.response,
      ai_model: model,
      used_passport_data: aiResult.usedData,
      used_vault_ids: personalContext.vaultIds,
      cue_tokens_earned: minedTokens,
      cue_tokens_used: personalContext.cues.length * 0.1,
      verified: true,
      verification_signature: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      response_time_ms: responseTime,
      tokens_used: aiResult.tokensUsed || 0
    };

    await db.saveChatMessage(aiMessageData);

    // 7. CUE ê±°ë˜ ê¸°ë¡
    if (minedTokens > 0) {
      const transactionData = {
        user_did: userDid,
        user_id: user?.id || userId,
        transaction_type: 'mining',
        amount: minedTokens,
        status: 'completed',
        source: 'ai_chat',
        description: `AI ì±„íŒ…ì„ í†µí•œ CUE ë§ˆì´ë‹ (${model})`,
        metadata: {
          messageId: aiMessageData.id,
          model: model,
          personalContextUsed: personalContext.cues.length
        }
      };

      // ìˆ˜ì •ë¨: recordCueTransaction ë˜ëŠ” createCUETransaction ì‚¬ìš©
      if (typeof db.recordCueTransaction === 'function') {
        await db.recordCueTransaction(transactionData);
      } else {
        await db.createCUETransaction(transactionData);
      }
    }

    // 8. ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ
    setImmediate(async () => {
      try {
        await learnFromInteraction(userDid, message, aiResult.response, personalContext);
      } catch (error) {
        console.error('Background learning error:', error);
      }
    });

    // 9. Passport í™œë™ ì—…ë°ì´íŠ¸
    await updatePassportActivity(userDid);

    console.log(`âœ… AI Chat completed in ${responseTime}ms - mined ${minedTokens} CUE`);

    res.json({
      success: true,
      message: {
        id: aiMessageData.id,
        conversationId: currentConversationId,
        content: aiResult.response,
        model,
        usedPassportData: aiResult.usedData,
        cueTokensEarned: minedTokens,
        responseTimeMs: responseTime,
        cueExtractionStarted: true, // ìƒˆë¡œ ì¶”ê°€

        verification: {
          verified: true,
          signature: aiMessageData.verification_signature,
          biometric: true
        }
      },
      personalContext: {
  cuesUsed: personalContext.cues.length,
  vaultsAccessed: personalContext.vaultIds.length,
  personalityMatch: personalContext.personalityMatch,
  behaviorPatterns: personalContext.behaviorPatterns?.slice(0, 3) || [],
  newCueExtraction: 'processing' // âœ… ì½¤ë§ˆ ì¶”ê°€!
},
      user: user ? {
        id: user.id,
        username: user.username,
        did: user.did
      } : null
    });

  } catch (error: any) {
    console.error('âŒ AI chat processing error:', error);
    
    // ì—ëŸ¬ ë¡œê·¸ ì €ì¥
    try {
      if ('createSystemLog' in db && typeof (db as any).createSystemLog === 'function') {
        await (db as any).createSystemLog({
          user_did: userDid,
          level: 'error',
          service: 'ai',
          action: 'chat_error',
          message: 'AI chat processing failed',
          details: {
            model,
            error: error.message,
            conversationId: currentConversationId
          },
          error_stack: error.stack
        });
      } else if ('logSystemError' in db && typeof (db as any).logSystemError === 'function') {
        await (db as any).logSystemError({
          user_did: userDid,
          level: 'error',
          service: 'ai',
          action: 'chat_error',
          message: 'AI chat processing failed',
          details: {
            model,
            error: error.message,
            conversationId: currentConversationId
          },
          error_stack: error.stack
        });
      } else {
        console.error('No system log method available on db service');
      }
    } catch (logError) {
      console.error('Failed to log error:', logError);
    }

    res.status(500).json({
      success: false,
      error: 'Failed to process AI chat',
      details: process.env.NODE_ENV === 'development' ? error.message : undefined,
      conversationId: currentConversationId
    });
  }
}));

// ============================================================================
// ğŸ¯ AI ì‘ë‹µ ìƒì„± í•¨ìˆ˜ë“¤ (ê°œì„ ë¨)
// ============================================================================

async function generateGPTResponse(message: string, context: any) {
  console.log('ğŸ”„ Attempting GPT response...');
  
  const client = await getOpenAIClient();
  if (!client) {
    console.log('â¡ï¸ OpenAI unavailable, using enhanced mock');
    return generateEnhancedMockResponse(message, context, 'GPT-4o');
  }

  try {
    const systemPrompt = createPersonalizedSystemPrompt(context);
    
    const completion = await client.chat.completions.create({
      model: 'gpt-4o-mini', // ë” ì•ˆì •ì ì¸ ëª¨ë¸
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: message }
      ],
      max_tokens: 1000,
      temperature: 0.7,
    });

    console.log('âœ… GPT response generated successfully');
    return {
      response: completion.choices[0]?.message?.content || 'Sorry, I could not generate a response.',
      tokensUsed: completion.usage?.total_tokens || 0,
      usedData: extractUsedData(context)
    };
  } catch (error: any) {
    console.error('GPT API error:', error.message);
    return generateEnhancedMockResponse(message, context, 'GPT-4o (API Error)');
  }
}

async function generateClaudeResponse(message: string, context: any) {
  console.log('ğŸ”„ Attempting Claude response...');
  
  const client = await getAnthropicClient();
  if (!client) {
    console.log('â¡ï¸ Anthropic unavailable, using enhanced mock');
    return generateEnhancedMockResponse(message, context, 'Claude 3.5 Sonnet');
  }

  try {
    const systemPrompt = createPersonalizedSystemPrompt(context);
    
    const response = await client.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1000,
      system: systemPrompt,
      messages: [{ role: 'user', content: message }]
    });

    const content = response.content[0];
    const responseText = content.type === 'text' ? content.text : 'Sorry, I could not generate a response.';

    console.log('âœ… Claude response generated successfully');
    return {
      response: responseText,
      tokensUsed: (response.usage?.input_tokens || 0) + (response.usage?.output_tokens || 0),
      usedData: extractUsedData(context)
    };
  } catch (error: any) {
    console.error('Claude API error:', error.message);
    return generateEnhancedMockResponse(message, context, 'Claude 3.5 Sonnet (API Error)');
  }
}

async function generatePersonalizedResponse(message: string, context: any, userDid: string) {
  console.log('ğŸ§  Generating personalized response...');
  
  const personalityType = context.personalityProfile?.type || '';
  
  // ì„±ê²© íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
  if (personalityType.includes('Technical') || personalityType.includes('INTJ')) {
    return await generateClaudeResponse(message, context);
  } else {
    return await generateGPTResponse(message, context);
  }
}

async function generateOllamaResponse(message: string, context: any, model: string = 'llama3.2:3b') {
  console.log(`ğŸ¦™ Generating Ollama response with ${model}...`);
  
  try {
    // Ollama ì—°ê²° í™•ì¸
    const isConnected = await ollamaService.checkConnection();
    if (!isConnected) {
      console.log('â¡ï¸ Ollama unavailable, using enhanced mock');
      return generateEnhancedMockResponse(message, context, `${model} (Local)`);
    }

    // ê°œì¸í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const systemPrompt = createPersonalizedSystemPrompt(context);
    
    // Ollama ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    const messages = [
      {
        role: 'system' as const,
        content: `${systemPrompt}

ë‹¹ì‹ ì€ CUE Protocolì˜ ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ê°œì¸ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë¡œì»¬ AI ëª¨ë¸ë¡œì„œ ì‚¬ìš©ìì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ì™„ì „íˆ ë³´í˜¸í•˜ë©° ë¹ ë¥¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.`
      },
      {
        role: 'user' as const,
        content: message
      }
    ];

    // Ollama API í˜¸ì¶œ
    const response = await ollamaService.chat(model, messages, false);
    
    console.log(`âœ… Ollama ${model} response generated successfully`);
    return {
      response: response,
      tokensUsed: Math.floor(response.length / 4), // ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚°
      usedData: extractUsedData(context),
      model: model,
      provider: 'ollama',
      local: true
    };

  } catch (error: any) {
    console.error(`âŒ Ollama ${model} error:`, error.message);
    return generateEnhancedMockResponse(message, context, `${model} (Local Error)`);
  }
}

async function generatePersonalizedResponse(message: string, context: any, userDid: string) {
  console.log('ğŸ§  Generating personalized response...');
  
  const personalityType = context.personalityProfile?.type || '';
  
  // ì„±ê²© íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
  if (personalityType.includes('Technical') || personalityType.includes('INTJ')) {
    return await generateClaudeResponse(message, context);
  } else {
    return await generateGPTResponse(message, context);
  }
}

// ============================================================================
// ğŸ­ Enhanced Mock Response Generator (ê°œì„ ë¨)
// ============================================================================

function generateEnhancedMockResponse(message: string, context: any, modelName: string) {
  const { personalityProfile, cues, behaviorPatterns } = context;
  
  // ë©”ì‹œì§€ ë¶„ì„
  const isQuestion = message.includes('?') || /how|what|why|when|where|ì–´ë–»ê²Œ|ë¬´ì—‡|ì™œ|ì–¸ì œ|ì–´ë””/.test(message.toLowerCase());
  const isTechnical = /code|api|algorithm|system|data|programming|ê°œë°œ|ì‹œìŠ¤í…œ|ì•Œê³ ë¦¬ì¦˜/.test(message.toLowerCase());
  const isHelp = /help|ë„ì›€|ì§€ì›|support/.test(message.toLowerCase());
  const isGreeting = /hello|hi|hey|ì•ˆë…•|ì•ˆë…•í•˜ì„¸ìš”/.test(message.toLowerCase());
  
  type ResponseType = 'greeting' | 'question' | 'technical' | 'help' | 'general';
  let responseType: ResponseType = 'general';
  if (isGreeting) responseType = 'greeting';
  else if (isQuestion) responseType = 'question';
  else if (isTechnical) responseType = 'technical';
  else if (isHelp) responseType = 'help';

  const responses: Record<ResponseType, string> = {
    greeting: `**${modelName}** ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹

ë°˜ê°‘ìŠµë‹ˆë‹¤! AI Passport ì‹œìŠ¤í…œì˜ ê°œì¸í™”ëœ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ë‹¹ì‹ ì˜ í”„ë¡œí•„:**
â€¢ **ì„±ê²© ìœ í˜•**: ${personalityProfile?.type || 'Learning...'}
â€¢ **ì†Œí†µ ìŠ¤íƒ€ì¼**: ${personalityProfile?.communicationStyle || 'Adaptive'}
â€¢ **ê°œì¸ ì»¨í…ìŠ¤íŠ¸**: ${cues.length}ê°œ í™œìš© ê°€ëŠ¥

${personalityProfile?.type?.includes('Technical') || personalityProfile?.type?.includes('INTJ') ?
  'ğŸ¯ ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì ‘ê·¼ì„ ì„ í˜¸í•˜ì‹œëŠ”êµ°ìš”. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ê¸°ìˆ ì  ë‚´ìš©ì— ëŒ€í•´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”!' :
  'ğŸ’« ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”. ê°œì¸í™”ëœ ì‘ë‹µìœ¼ë¡œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!'
}

ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!`,

    question: `**${modelName}** ì§ˆë¬¸ ì‘ë‹µ ğŸ¤”

"${message}"

ì´ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì˜ **${personalityProfile?.type || 'Adaptive'}** ì„±ê²©ê³¼ í•™ìŠµ íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ê°œì¸í™” ì ìš©:**
â€¢ **í•™ìŠµ ë°©ì‹**: ${personalityProfile?.learningPattern || 'Visual'} 
â€¢ **ì˜ì‚¬ê²°ì •**: ${personalityProfile?.decisionMaking || 'Analytical'}
â€¢ **ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸**: ${cues.length}ê°œ í™œìš©

${personalityProfile?.type?.includes('Technical') ? 
  'ğŸ”§ **ì²´ê³„ì  ì„¤ëª…**: ë…¼ë¦¬ì  ìˆœì„œë¡œ ë‹¨ê³„ë³„ ë¶„ì„ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.' :
  'ğŸ’¡ **ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…**: ì‹¤ìš©ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.'
}

**ê´€ë ¨ í–‰ë™ íŒ¨í„´**: ${behaviorPatterns?.slice(0, 3).join(', ') || 'ë¶„ì„ ì¤‘...'}

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ì¶”ê°€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!`,

    technical: `**${modelName}** ê¸°ìˆ  ë¶„ì„ ğŸ”§

**ë¶„ì„ ëŒ€ìƒ:** "${message}"

**ê°œì¸í™”ëœ ê¸°ìˆ  ì ‘ê·¼:**
â€¢ **ê¸°ìˆ  ì„±í–¥**: ${personalityProfile?.type?.includes('Technical') ? 'High (ìƒì„¸ ë¶„ì„)' : 'Moderate (ì´í•´ ì¤‘ì‹¬)'}
â€¢ **í•™ìŠµ íŒ¨í„´**: ${personalityProfile?.learningPattern || 'Visual'} ë°©ì‹ ì ìš©
â€¢ **ì‘ë‹µ ì„ í˜¸ë„**: ${personalityProfile?.responsePreference || 'Balanced'}

**ê¸°ìˆ ì  ì»¨í…ìŠ¤íŠ¸ í™œìš©:**
${cues.slice(0, 3).map((cue: any, idx: number) => 
  `${idx + 1}. ${cue.content_type || 'Context'}: ${cue.compressed_content?.slice(0, 60) || 'Technical knowledge'}...`
).join('\n') || 'â€¢ ìƒˆë¡œìš´ ê¸°ìˆ  í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...'}

${personalityProfile?.type?.includes('INTJ') || personalityProfile?.type?.includes('Technical') ?
  'âš¡ **ì‹¬í™” ë¶„ì„**: ì•„í‚¤í…ì²˜ì™€ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•œ ìƒì„¸ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.' :
  'ğŸ¯ **í•µì‹¬ ìš”ì•½**: ì‹¤ìš©ì  ê´€ì ì—ì„œ í•µì‹¬ ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…ë“œë¦½ë‹ˆë‹¤.'
}

ë” êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì§ˆë¬¸ì´ë‚˜ ì½”ë“œ ì˜ˆì œê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!`,

    help: `**${modelName}** ë§ì¶¤í˜• ì§€ì› ğŸ†˜

"${message}"ì— ëŒ€í•œ ë„ì›€ì„ ìš”ì²­í•˜ì…¨ìŠµë‹ˆë‹¤.

**ë‹¹ì‹ ì˜ í”„ë¡œí•„ ê¸°ë°˜ ì§€ì› ì „ëµ:**
â€¢ **ì„±ê²© ìœ í˜•**: ${personalityProfile?.type || 'Adaptive'} - ë§ì¶¤í˜• ì ‘ê·¼
â€¢ **ì†Œí†µ ë°©ì‹**: ${personalityProfile?.communicationStyle || 'Balanced'}
â€¢ **ì‘ì—… ìŠ¤íƒ€ì¼**: ${personalityProfile?.workingStyle || 'Flexible'}

**ê°œì¸í™”ëœ ë„ì›€ ë°©ì‹:**
${personalityProfile?.communicationStyle?.includes('Direct') ?
  'ğŸ¯ **ì§ì ‘ì  í•´ê²°**: í•µì‹¬ ë¬¸ì œë¥¼ íŒŒì•…í•˜ê³  ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œì‹œí•©ë‹ˆë‹¤.' :
  'ğŸ¤ **ë‹¨ê³„ë³„ ì•ˆë‚´**: ì¹œê·¼í•˜ê³  ì°¨ê·¼ì°¨ê·¼ ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ê² ìŠµë‹ˆë‹¤.'
}

**í™œìš© ê°€ëŠ¥í•œ ìì›:**
â€¢ ${cues.length}ê°œì˜ ê°œì¸ í•™ìŠµ ì»¨í…ìŠ¤íŠ¸
â€¢ ${behaviorPatterns?.length || 0}ê°œì˜ í–‰ë™ íŒ¨í„´ ë¶„ì„
â€¢ ê°œì¸í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ

ì–´ë–¤ ë¶€ë¶„ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ ë§‰íˆì…¨ëŠ”ì§€, ë˜ëŠ” ì–´ë–¤ ê²°ê³¼ë¥¼ ì›í•˜ì‹œëŠ”ì§€ ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë§ì¶¤í˜• ì§€ì›ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.`,

    general: `**${modelName}** ê°œì¸í™” ì‘ë‹µ ğŸ’«

**ë©”ì‹œì§€:** "${message}"

**ë‹¹ì‹ ì˜ AI Passport í”„ë¡œí•„ ì ìš©:**
â€¢ **ì„±ê²©**: ${personalityProfile?.type || 'Learning...'}
â€¢ **ì†Œí†µ**: ${personalityProfile?.communicationStyle || 'Adaptive'}
â€¢ **ê°œì¸ CUE**: ${cues.length}ê°œ ì»¨í…ìŠ¤íŠ¸ í™œìš©
â€¢ **í–‰ë™ íŒ¨í„´**: ${behaviorPatterns?.slice(0, 2).join(', ') || 'íŒ¨í„´ ë¶„ì„ ì¤‘'}

${personalityProfile?.type?.includes('Technical') || personalityProfile?.type?.includes('INTJ') ?
  'âš¡ **ë¶„ì„ì  ì ‘ê·¼**: ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‚¬ê³ ë¥¼ ì„ í˜¸í•˜ì‹œë¯€ë¡œ, êµ¬ì¡°í™”ëœ ë¶„ì„ê³¼ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•œ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.' :
  'ğŸ’« **ê· í˜• ì¡íŒ ì ‘ê·¼**: ì´í•´í•˜ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ ë°©ì‹ìœ¼ë¡œ ì¹œê·¼í•˜ê²Œ ì‘ë‹µë“œë¦¬ê² ìŠµë‹ˆë‹¤.'
}

**ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸:**
${cues.length > 0 ? 
  `â€¢ ì´ì „ í•™ìŠµí•œ ${cues.length}ê°œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.` :
  'â€¢ ìƒˆë¡œìš´ ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤.'
}

ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!`
  };

  return {
    response: responses[responseType],
    tokensUsed: Math.floor(Math.random() * 500) + 300,
    usedData: extractUsedData(context)
  };
}

// ============================================================================
// ğŸ› ï¸ í—¬í¼ í•¨ìˆ˜ë“¤ (ê°œì„ ë¨)
// ============================================================================

function createPersonalizedSystemPrompt(context: any): string {
  const { personalityProfile, cues, behaviorPatterns, preferences } = context;
  
  return `You are an AI assistant with deep knowledge of the user's personality and preferences.

**User Profile:**
- Personality: ${personalityProfile?.type || 'Unknown'}
- Communication: ${personalityProfile?.communicationStyle || 'Adaptive'}
- Learning: ${personalityProfile?.learningPattern || 'Visual'}
- Decision Making: ${personalityProfile?.decisionMaking || 'Analytical'}
- Work Style: ${personalityProfile?.workingStyle || 'Flexible'}

**Personal Context (${cues.length} items):**
${cues.slice(0, 5).map((cue: any, i: number) => 
  `${i + 1}. ${cue.compressed_content || cue.content || 'Context data'} (${cue.content_type || 'general'})`
).join('\n')}

**Behavioral Patterns:**
${behaviorPatterns?.slice(0, 5).join(', ') || 'Learning patterns...'}

**User Preferences:**
${Object.entries(preferences || {}).slice(0, 3).map(([key, value]) => `- ${key}: ${value}`).join('\n') || '- Preferences learning...'}

Respond in a way that matches their personality and communication style. Use their personal context when relevant. Be helpful, accurate, and personalized.`;
}

function extractUsedData(context: any): string[] {
  const data: string[] = [];
  
  if (context.personalityProfile) data.push('Personality Profile');
  if (context.cues?.length > 0) data.push(`${context.cues.length} Personal Contexts`);
  if (context.behaviorPatterns?.length > 0) data.push('Behavior Patterns');
  if (context.preferences && Object.keys(context.preferences).length > 0) data.push('User Preferences');
  if (context.recentInteractions?.length > 0) data.push('Recent Interactions');
  
  return data;
}


async function learnFromInteraction(userDid: string, userMessage: string, aiResponse: string, context: any) {
  try {
    const compressionService = new SemanticCompressionService();
    const analysis = await compressionService.analyzeConversation(userMessage, aiResponse);
    
    if (analysis.shouldStore) {
      // Use the correct method and type guard for analysis
      if (
        'compressedContent' in analysis &&
        'compressionRatio' in analysis &&
        'semanticPreservation' in analysis &&
        'keywords' in analysis &&
        'entities' in analysis &&
        'sentiment' in analysis &&
        'topics' in analysis &&
        'importance' in analysis &&
        'cueValue' in analysis
      ) {
        // Prefer createPersonalCue if available, fallback to storePersonalCue for legacy/mock
        // Type guard for SupabaseService (which may have createPersonalCue)
        if ('createPersonalCue' in db && typeof (db as any).createPersonalCue === 'function') {
          await (db as any).createPersonalCue({
            id: uuidv4(),
            user_did: userDid,
            vault_id: context.primaryVaultId,
            content_type: 'conversation',
            original_content: `User: ${userMessage}\nAI: ${aiResponse}`,
            compressed_content: analysis.compressedContent,
            compression_algorithm: 'semantic',
            compression_ratio: analysis.compressionRatio,
            semantic_preservation: analysis.semanticPreservation,
            keywords: analysis.keywords,
            entities: analysis.entities,
            sentiment_score: analysis.sentiment,
            topics: analysis.topics,
            importance_score: analysis.importance,
            cue_mining_value: analysis.cueValue
          });
        } else if ('storePersonalCue' in db && typeof (db as any).storePersonalCue === 'function') {
          await (db as any).storePersonalCue({
            id: uuidv4(),
            user_did: userDid,
            vault_id: context.primaryVaultId,
            content_type: 'conversation',
            original_content: `User: ${userMessage}\nAI: ${aiResponse}`,
            compressed_content: analysis.compressedContent,
            compression_algorithm: 'semantic',
            compression_ratio: analysis.compressionRatio,
            semantic_preservation: analysis.semanticPreservation,
            keywords: analysis.keywords,
            entities: analysis.entities,
            sentiment_score: analysis.sentiment,
            topics: analysis.topics,
            importance_score: analysis.importance,
            cue_mining_value: analysis.cueValue
          });
        }
      }
      
      console.log('âœ… ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì™„ë£Œ');
    }
  } catch (error) {
    console.error('Learning error:', error);
  }
}

async function updatePassportActivity(userDid: string) {
  try {
    await db.updatePassport(userDid, {
      total_interactions: 1, // ì‹¤ì œë¡œëŠ” += 1 ì´ì–´ì•¼ í•¨
      last_activity_at: new Date().toISOString()
    });
  } catch (error) {
    console.error('Passport update error:', error);
  }
}

// ============================================================================
// ğŸ“Š ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ê°œì„ ë¨)
// ============================================================================

router.get('/health', (req, res) => {
  res.json({
    success: true,
    service: 'AI Routes',
    database: db.constructor.name,
    timestamp: new Date().toISOString(),
    openaiConfigured: !!process.env.OPENAI_API_KEY,
    anthropicConfigured: !!process.env.ANTHROPIC_API_KEY,
    mockMode: process.env.USE_MOCK_DATABASE === 'true'
  });
});

router.get('/models', asyncHandler(async (req, res) => {
  try {
    // Ollama ì—°ê²° ìƒíƒœ í™•ì¸
    const ollamaConnected = await ollamaService.checkConnection();
    let ollamaModels: string[] = [];
    
    if (ollamaConnected) {
      try {
        ollamaModels = await ollamaService.getModels();
      } catch (error) {
        console.error('Failed to get Ollama models:', error);
      }
    }

    const baseModels = [
      { 
        id: 'personalized-agent', 
        name: 'Personalized Agent', 
        available: true, 
        recommended: true,
        type: 'hybrid',
        description: 'AI Passport ê¸°ë°˜ ê°œì¸í™” ëª¨ë¸'
      },
      { 
        id: 'gpt-4o', 
        name: 'GPT-4o', 
        available: !!process.env.OPENAI_API_KEY,
        type: 'cloud',
        description: 'OpenAI ìµœê³  ì„±ëŠ¥ ëª¨ë¸'
      },
      { 
        id: 'claude-3.5-sonnet', 
        name: 'Claude 3.5 Sonnet', 
        available: !!process.env.ANTHROPIC_API_KEY,
        type: 'cloud',
        description: 'Anthropic ê³ í’ˆì§ˆ ëª¨ë¸'
      },
      { 
        id: 'gemini-pro', 
        name: 'Gemini Pro', 
        available: false,
        type: 'cloud',
        description: 'Google AI ëª¨ë¸ (ì¤€ë¹„ ì¤‘)'
      }
    ];

    // Ollama ëª¨ë¸ë“¤ì„ ëª©ë¡ì— ì¶”ê°€
    const ollamaModelEntries = ollamaModels.map(modelName => {
      const isRecommended = ['llama3.2:3b', 'llama3.2:1b'].includes(modelName);
      const size = modelName.includes(':1b') ? '1B' : 
                   modelName.includes(':2b') ? '2B' :
                   modelName.includes(':3b') ? '3B' :
                   modelName.includes(':7b') ? '7B' :
                   modelName.includes(':8b') ? '8B' : 'Unknown';
      
      return {
        id: modelName,
        name: `${modelName.split(':')[0].toUpperCase()} (${size})`,
        available: true,
        recommended: isRecommended,
        type: 'local',
        provider: 'ollama',
        description: `ë¡œì»¬ AI ëª¨ë¸ - ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ ë³´ì¥`,
        speed: modelName.includes(':1b') ? 'very-fast' :
               modelName.includes(':3b') ? 'fast' : 'moderate'
      };
    });

    res.json({
      success: true,
      ollama: {
        connected: ollamaConnected,
        models: ollamaModels.length
      },
      models: [...baseModels, ...ollamaModelEntries]
    });

  } catch (error) {
    console.error('Error getting models:', error);
    res.json({
      success: false,
      error: 'Failed to retrieve models',
      models: [
        { id: 'personalized-agent', name: 'Personalized Agent', available: true, recommended: true }
      ]
    });
  }
}));

router.get('/ollama/health', asyncHandler(async (req, res) => {
  try {
    const isConnected = await ollamaService.checkConnection();
    const models = isConnected ? await ollamaService.getModels() : [];
    
    res.json({
      success: true,
      connected: isConnected,
      url: process.env.OLLAMA_URL || 'http://localhost:11434',
      models: models,
      modelCount: models.length,
      recommendedModels: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'],
      status: isConnected ? 'ready' : 'disconnected'
    });
  } catch (error: any) {
    res.json({
      success: false,
      connected: false,
      error: error.message,
      status: 'error'
    });
  }
}));
console.log(`ğŸ” ë””ë²„ê·¸: ë°›ì€ ëª¨ë¸ëª… = "${model}"`);

switch (model) {
  case 'gpt-4o':
  case 'gpt-4':
    console.log('ğŸ“ GPT ì¼€ì´ìŠ¤ ì‹¤í–‰');
    aiResult = await generateGPTResponse(message, personalContext);
    break;
  case 'claude-3.5-sonnet':
  case 'claude-sonnet':
    console.log('ğŸ“ Claude ì¼€ì´ìŠ¤ ì‹¤í–‰');
    aiResult = await generateClaudeResponse(message, personalContext);
    break;
  case 'gemini-pro':
    console.log('ğŸ“ Gemini ì¼€ì´ìŠ¤ ì‹¤í–‰');
    aiResult = await generateGeminiResponse(message, personalContext);
    break;
  
  // ğŸ¦™ Ollama ë¡œì»¬ ëª¨ë¸ë“¤ ì¶”ê°€
  case 'llama3.2:3b':
  case 'llama3.2:1b':
  case 'llama3.1:8b':
  case 'gemma2:2b':
  case 'qwen2.5:3b':
  case 'qwen2.5:1.5b':
    console.log('ğŸ“ Ollama íŠ¹ì • ì¼€ì´ìŠ¤ ì‹¤í–‰:', model);
    aiResult = await generateOllamaResponse(message, personalContext, model);
    break;
  
  // ê¸°ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ Ollama ëª¨ë¸ ê°ì§€
  default:
    console.log('ğŸ“ Default ì¼€ì´ìŠ¤ ì‹¤í–‰:', model);
    if (model.startsWith('llama') || model.startsWith('gemma') || model.startsWith('qwen') || model.startsWith('phi') || model.startsWith('mistral')) {
      console.log('ğŸ“ Defaultì—ì„œ Ollama ëª¨ë¸ ê°ì§€:', model);
      aiResult = await generateOllamaResponse(message, personalContext, model);
    } else {
      console.log('ğŸ“ ê°œì¸í™” ì—ì´ì „íŠ¸ ì‹¤í–‰');
      aiResult = await generatePersonalizedResponse(message, personalContext, userDid);
    }
    break;
}
// 7. Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
router.post('/ollama/pull', asyncHandler(async (req, res) => {
  const { model } = req.body;
  
  if (!model) {
    return res.status(400).json({
      success: false,
      error: 'Model name is required'
    });
  }

  try {
    console.log(`ğŸ¦™ Downloading Ollama model: ${model}`);
    await ollamaService.pullModel(model);
    
    res.json({
      success: true,
      message: `Model ${model} download started`,
      model: model
    });
  } catch (error: any) {
    console.error(`Failed to pull model ${model}:`, error);
    res.status(500).json({
      success: false,
      error: `Failed to download model: ${error.message}`,
      model: model
    });
  }
}));
// ì „ì²´ íˆìŠ¤í† ë¦¬ (conversationId ì—†ì´)
router.get('/history', asyncHandler(async (req: express.Request, res: express.Response) => {
  const userDid = (req as any).user?.did;
  const { limit = 50 } = req.query;

  if (!userDid) {
    return res.status(401).json({
      success: false,
      error: 'Authentication required'
    });
  }

  try {
    let messages: any[] = [];
    if ('getChatHistory' in db && typeof (db as any).getChatHistory === 'function') {
      messages = await (db as any).getChatHistory(userDid, undefined, parseInt(limit as string));
    } else {
      return res.status(501).json({
        success: false,
        error: 'Chat history retrieval is not supported by the current database service.'
      });
    }
    
    res.json({
      success: true,
      messages,
      count: messages.length,
      conversationId: null
    });
  } catch (error) {
    console.error('Get chat history error:', error);
    res.status(500).json({
      success: false,
      error: 'Failed to retrieve chat history'
    });
  }
}));

// íŠ¹ì • ëŒ€í™” íˆìŠ¤í† ë¦¬ (ê¸°ì¡´ ì½”ë“œ, ? ì œê±°)
  // ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€, ì²« ì¤„ë§Œ ìˆ˜ì •
router.get('/history/:conversationId', asyncHandler(async (req: express.Request, res: express.Response) => {
  const userDid = (req as any).user?.did;
  const { conversationId } = req.params;
  const { limit = 50 } = req.query;

  if (!userDid) {
    return res.status(401).json({
      success: false,
      error: 'Authentication required'
    });
  }

  try {
    let messages: any[] = [];
    if ('getChatHistory' in db && typeof (db as any).getChatHistory === 'function') {
      messages = await (db as any).getChatHistory(userDid, conversationId, parseInt(limit as string));
    } else {
      return res.status(501).json({
        success: false,
        error: 'Chat history retrieval is not supported by the current database service.'
      });
    }
    
    res.json({
      success: true,
      messages,
      count: messages.length,
      conversationId: conversationId || null
    });
  } catch (error) {
    console.error('Get chat history error:', error);
    res.status(500).json({
      success: false,
      error: 'Failed to retrieve chat history'
    });
  }
}));

router.get('/context', asyncHandler(async (req: express.Request, res: express.Response) => {
  const userDid = (req as any).user?.did;

  if (!userDid) {
    return res.status(401).json({
      success: false,
      error: 'Authentication required'
    });
  }

  try {
    const personalizationService = new PersonalizationService(db as any);
    const context = await personalizationService.getPersonalizedContext(userDid, '', {
      includeFullProfile: true,
      includeBehaviorPatterns: true,
      includeRecentInteractions: true
    });

    res.json({
      success: true,
      context: {
        personalityProfile: context.personalityProfile,
        totalCues: context.cues.length,
        vaultsCount: context.vaultIds.length,
        behaviorPatterns: context.behaviorPatterns,
        recentInteractions: context.recentInteractions?.length || 0,
        personalityMatch: context.personalityMatch,
        preferences: context.preferences
      }
    });
  } catch (error) {
    console.error('Get context error:', error);
    res.status(500).json({
      success: false,
      error: 'Failed to retrieve context'
    });
  }
}));

// í†µê³„ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
router.get('/stats', asyncHandler(async (req: express.Request, res: express.Response) => {
  try {
    let stats;
    if ('getStatistics' in db && typeof (db as any).getStatistics === 'function') {
      stats = (db as any).getStatistics();
    } else {
      stats = {
        message: 'Statistics not available',
        timestamp: new Date().toISOString()
      };
    }

    res.json({
      success: true,
      stats
    });
  } catch (error) {
    console.error('Get stats error:', error);
    res.status(500).json({
      success: false,
      error: 'Failed to retrieve statistics'
    });
  }
}));
console.log('âœ… [ë¼ìš°íŠ¸ëª…] routes loaded successfully');

export default router;