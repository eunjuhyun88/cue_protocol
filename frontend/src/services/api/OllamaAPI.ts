// ============================================================================
// ğŸ¦™ Enhanced Ollama API Client - í”„ë¡ íŠ¸ì—”ë“œ ì „ìš©
// ê²½ë¡œ: frontend/src/services/api/OllamaAPI.ts
// ============================================================================
// ë°±ì—”ë“œì˜ Ollama APIì™€ í†µì‹ í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸
// ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ ì²˜ë¦¬, ë¡œê¹…, ì¬ì‹œë„ ë¡œì§ í¬í•¨

export interface OllamaModel {
  id: string;
  name: string;
  displayName: string;
  size: string;
  specialty: string;
  available: boolean;
  color: string;
}

export interface OllamaChatRequest {
  model: string;
  message: string;
  passportData?: {
    did: string;
    personalityProfile?: any;
    preferences?: any;
    context?: string;
  };
  options?: {
    temperature?: number;
    maxTokens?: number;
    stream?: boolean;
  };
}

export interface OllamaChatResponse {
  success: boolean;
  response?: string;
  model: string;
  processingTime: number;
  tokensUsed?: number;
  conversationId?: string;
  cueEarned?: number;
  error?: string;
}

export interface OllamaHealthResponse {
  success: boolean;
  connected: boolean;
  modelCount: number;
  models: string[];
  timestamp: string;
  processingTime: number;
  error?: string;
}

export interface OllamaStreamChunk {
  content: string;
  done: boolean;
  model: string;
  tokensUsed?: number;
  processingTime?: number;
  cueEarned?: number;
  error?: string;
}

// ============================================================================
// ğŸ¯ Ollama API í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
// ============================================================================
export class OllamaAPIClient {
  private baseURL: string;
  private timeout: number;
  private maxRetries: number;

  constructor() {
    this.baseURL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:3001';
    this.timeout = 60000; // 60ì´ˆ
    this.maxRetries = 3;
    
    console.log(`ğŸ¦™ OllamaAPI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: ${this.baseURL}`);
  }

  // ============================================================================
  // ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
  // ============================================================================

  private async request<T>(
    endpoint: string, 
    options: RequestInit = {},
    retryCount = 0
  ): Promise<T> {
    const url = `${this.baseURL}${endpoint}`;
    const startTime = Date.now();

    try {
      console.log(`ğŸŒ Ollama API ìš”ì²­: ${options.method || 'GET'} ${endpoint}`);

      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), this.timeout);

      const response = await fetch(url, {
        ...options,
        headers: {
          'Content-Type': 'application/json',
          ...options.headers,
        },
        credentials: 'include',
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      const endTime = Date.now();
      const processingTime = endTime - startTime;

      console.log(`âš¡ Ollama API ì‘ë‹µ: ${response.status} (${processingTime}ms)`);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.message || `HTTP ${response.status}`);
      }

      return await response.json();
    } catch (error: any) {
      const endTime = Date.now();
      const processingTime = endTime - startTime;

      console.error(`âŒ Ollama API ì—ëŸ¬: ${endpoint} (${processingTime}ms)`, error.message);

      // ì¬ì‹œë„ ë¡œì§
      if (retryCount < this.maxRetries && this.shouldRetry(error)) {
        console.log(`ğŸ”„ ì¬ì‹œë„ ${retryCount + 1}/${this.maxRetries}: ${endpoint}`);
        await this.delay(Math.pow(2, retryCount) * 1000); // ì§€ìˆ˜ ë°±ì˜¤í”„
        return this.request<T>(endpoint, options, retryCount + 1);
      }

      throw error;
    }
  }

  private shouldRetry(error: any): boolean {
    // ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬, íƒ€ì„ì•„ì›ƒ, ì„œë²„ ì—ëŸ¬ ì‹œ ì¬ì‹œë„
    return (
      error.name === 'AbortError' ||
      error.code === 'ECONNREFUSED' ||
      error.message.includes('fetch')
    );
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  // ============================================================================
  // ğŸ“Š Ollama ìƒíƒœ ë° ëª¨ë¸ ê´€ë¦¬
  // ============================================================================

  /**
   * Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
   */
  async checkHealth(): Promise<OllamaHealthResponse> {
    try {
      const response = await this.request<OllamaHealthResponse>('/api/ollama/health');
      console.log(`ğŸ” Ollama ìƒíƒœ: ${response.connected ? 'Connected' : 'Disconnected'}`);
      return response;
    } catch (error: any) {
      console.error('âŒ Ollama ìƒíƒœ í™•ì¸ ì‹¤íŒ¨:', error.message);
      return {
        success: false,
        connected: false,
        modelCount: 0,
        models: [],
        timestamp: new Date().toISOString(),
        processingTime: 0,
        error: error.message
      };
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async getAvailableModels(): Promise<OllamaModel[]> {
    try {
      console.log('ğŸ“‹ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ');
      
      const response = await this.request<{
        success: boolean;
        models: string[];
        connected: boolean;
        modelCount: number;
      }>('/api/ollama/models');

      if (!response.success || !response.connected) {
        console.warn('âš ï¸ Ollama ì—°ê²°ë˜ì§€ ì•ŠìŒ - ë¹ˆ ëª¨ë¸ ëª©ë¡ ë°˜í™˜');
        return [];
      }

      // ëª¨ë¸ ì •ë³´ ë§¤í•‘
      const modelMap: Record<string, Omit<OllamaModel, 'id' | 'name' | 'available'>> = {
        'deepseek-coder:33b': { displayName: 'DeepSeek Coder 33B', size: '33B', specialty: 'ê³ ê¸‰ ì½”ë”©', color: 'bg-purple-500' },
        'deepseek-coder:6.7b': { displayName: 'DeepSeek Coder 6.7B', size: '6.7B', specialty: 'ë¹ ë¥¸ ì½”ë”©', color: 'bg-purple-400' },
        'codellama:13b': { displayName: 'Code Llama 13B', size: '13B', specialty: 'ì½”ë“œ ìƒì„±', color: 'bg-blue-500' },
        'codellama:7b': { displayName: 'Code Llama 7B', size: '7B', specialty: 'ì½”ë”© ì§€ì›', color: 'bg-blue-400' },
        'llama3.2:3b': { displayName: 'Llama 3.2 (3B)', size: '3B', specialty: 'ë¹ ë¥¸ ì‘ë‹µ', color: 'bg-green-500' },
        'llama3.2:1b': { displayName: 'Llama 3.2 (1B)', size: '1B', specialty: 'ì´ˆê³ ì†', color: 'bg-green-400' },
        'llama3.1:70b': { displayName: 'Llama 3.1 (70B)', size: '70B', specialty: 'ìµœê³  ì„±ëŠ¥', color: 'bg-green-600' },
        'llama3.1:8b': { displayName: 'Llama 3.1 (8B)', size: '8B', specialty: 'ê· í˜• ì¡íŒ', color: 'bg-green-500' },
        'mixtral:8x7b': { displayName: 'Mixtral 8x7B', size: '8x7B', specialty: 'ë³µì¡í•œ ì¶”ë¡ ', color: 'bg-orange-500' },
        'mistral:7b': { displayName: 'Mistral 7B', size: '7B', specialty: 'ì •í™•ì„±', color: 'bg-orange-400' },
        'phi3:mini': { displayName: 'Phi-3 Mini', size: '3B', specialty: 'íš¨ìœ¨ì„±', color: 'bg-cyan-500' },
      };

      const models: OllamaModel[] = response.models.map(modelName => ({
        id: modelName,
        name: modelName,
        available: true,
        ...modelMap[modelName] || {
          displayName: modelName,
          size: '?',
          specialty: 'ì¼ë°˜',
          color: 'bg-gray-500'
        }
      }));

      console.log(`âœ… ${models.length}ê°œ Ollama ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥:`, models.map(m => m.displayName));
      return models;

    } catch (error: any) {
      console.error('âŒ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', error.message);
      return [];
    }
  }

  // ============================================================================
  // ğŸ’¬ ì±„íŒ… ê¸°ëŠ¥
  // ============================================================================

  /**
   * ì¼ë°˜ ì±„íŒ… (ë™ê¸°ì‹)
   */
  async sendChatMessage(request: OllamaChatRequest): Promise<OllamaChatResponse> {
    try {
      console.log(`ğŸ’¬ Ollama ì±„íŒ… ì „ì†¡: ${request.model}`);
      console.log(`ğŸ“ ë©”ì‹œì§€: ${request.message.slice(0, 100)}...`);

      const response = await this.request<OllamaChatResponse>('/api/ollama/chat', {
        method: 'POST',
        body: JSON.stringify(request),
      });

      if (response.success) {
        console.log(`âœ… Ollama ì‘ë‹µ ë°›ìŒ: ${response.model} (${response.processingTime}ms)`);
        if (response.cueEarned) {
          console.log(`ğŸª™ CUE í† í° íšë“: +${response.cueEarned}`);
        }
      }

      return response;
    } catch (error: any) {
      console.error('âŒ Ollama ì±„íŒ… ì‹¤íŒ¨:', error.message);
      return {
        success: false,
        model: request.model,
        processingTime: 0,
        error: error.message
      };
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ì‹¤ì‹œê°„)
   */
  async sendStreamingMessage(
    request: OllamaChatRequest,
    onChunk: (chunk: OllamaStreamChunk) => void,
    onComplete?: (finalResponse: OllamaChatResponse) => void,
    onError?: (error: Error) => void
  ): Promise<void> {
    try {
      console.log(`ğŸ“¡ Ollama ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: ${request.model}`);

      const response = await fetch(`${this.baseURL}/api/ollama/chat/stream`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          ...request,
          options: {
            ...request.options,
            stream: true
          }
        }),
        credentials: 'include',
      });

      if (!response.ok) {
        throw new Error(`Ollama ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('ìŠ¤íŠ¸ë¦¼ ë¦¬ë” ìƒì„± ì‹¤íŒ¨');
      }

      let fullResponse = '';
      let totalTokens = 0;

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        const chunk = new TextDecoder().decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());

        for (const line of lines) {
          try {
            const data: OllamaStreamChunk = JSON.parse(line);
            
            if (data.content) {
              fullResponse += data.content;
              totalTokens += 1;
              onChunk(data);
            }

            if (data.done) {
              console.log(`âœ… Ollama ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ${request.model}`);
              
              const finalResponse: OllamaChatResponse = {
                success: true,
                response: fullResponse,
                model: data.model,
                processingTime: data.processingTime || 0,
                tokensUsed: data.tokensUsed || totalTokens,
                cueEarned: data.cueEarned
              };

              onComplete?.(finalResponse);
              return;
            }
          } catch (parseError) {
            console.warn('âš ï¸ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨:', line);
          }
        }
      }

    } catch (error: any) {
      console.error('âŒ Ollama ìŠ¤íŠ¸ë¦¬ë° ì—ëŸ¬:', error.message);
      onError?.(error);
    }
  }

  // ============================================================================
  // ğŸ¯ íŠ¹í™” ê¸°ëŠ¥
  // ============================================================================

  /**
   * ì½”ë”© ì „ìš© ì±„íŒ…
   */
  async sendCodeRequest(
    language: string,
    problem: string,
    context?: string
  ): Promise<OllamaChatResponse> {
    try {
      console.log(`ğŸ’» ì½”ë”© ìš”ì²­: ${language}`);

      const response = await this.request<OllamaChatResponse>('/api/ollama/code', {
        method: 'POST',
        body: JSON.stringify({
          language,
          problem,
          context
        }),
      });

      console.log(`âœ… ì½”ë”© ì‘ë‹µ ì™„ë£Œ: ${response.model}`);
      return response;
    } catch (error: any) {
      console.error('âŒ ì½”ë”© ìš”ì²­ ì‹¤íŒ¨:', error.message);
      return {
        success: false,
        model: 'coding-model',
        processingTime: 0,
        error: error.message
      };
    }
  }

  /**
   * ë¹ ë¥¸ ì‘ë‹µ ì±„íŒ… (ì‘ì€ ëª¨ë¸ ì‚¬ìš©)
   */
  async sendQuickMessage(message: string): Promise<OllamaChatResponse> {
    try {
      console.log(`âš¡ ë¹ ë¥¸ ì‘ë‹µ ìš”ì²­`);

      const response = await this.request<OllamaChatResponse>('/api/ollama/quick', {
        method: 'POST',
        body: JSON.stringify({ message }),
      });

      console.log(`âœ… ë¹ ë¥¸ ì‘ë‹µ ì™„ë£Œ: ${response.model}`);
      return response;
    } catch (error: any) {
      console.error('âŒ ë¹ ë¥¸ ì‘ë‹µ ì‹¤íŒ¨:', error.message);
      return {
        success: false,
        model: 'quick-model',
        processingTime: 0,
        error: error.message
      };
    }
  }

  // ============================================================================
  // ğŸ”§ ìœ í‹¸ë¦¬í‹° ê¸°ëŠ¥
  // ============================================================================

  /**
   * ì¶”ì²œ ëª¨ë¸ ì„ íƒ
   */
  getRecommendedModel(
    models: OllamaModel[],
    purpose: 'coding' | 'conversation' | 'quick' | 'complex'
  ): OllamaModel | null {
    const availableModels = models.filter(m => m.available);
    
    if (availableModels.length === 0) {
      console.warn('âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤');
      return null;
    }

    switch (purpose) {
      case 'coding':
        return availableModels.find(m => 
          m.name.includes('deepseek') || 
          m.name.includes('codellama') || 
          m.name.includes('magicoder')
        ) || availableModels[0];

      case 'quick':
        return availableModels.find(m => 
          m.name.includes('phi') || 
          m.name.includes('1b') ||
          m.size.includes('1B') ||
          m.size.includes('3B')
        ) || availableModels[0];

      case 'complex':
        return availableModels.find(m => 
          m.name.includes('70b') || 
          m.name.includes('mixtral') ||
          m.size.includes('70B')
        ) || availableModels[0];

      case 'conversation':
      default:
        return availableModels.find(m => 
          m.name.includes('llama3.2') || 
          m.name.includes('llama3.1')
        ) || availableModels[0];
    }
  }

  /**
   * ëª¨ë¸ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
   */
  categorizeModels(models: OllamaModel[]) {
    return {
      coding: models.filter(m => 
        m.name.includes('deepseek') || 
        m.name.includes('codellama') || 
        m.name.includes('starcoder') || 
        m.name.includes('magicoder')
      ),
      conversation: models.filter(m => 
        m.name.includes('llama') && !m.name.includes('codellama')
      ),
      specialized: models.filter(m => 
        m.name.includes('mixtral') || 
        m.name.includes('mistral') || 
        m.name.includes('qwen') || 
        m.name.includes('vicuna')
      ),
      fast: models.filter(m => 
        m.name.includes('phi') ||
        m.size.includes('1B') ||
        m.size.includes('2.7B')
      )
    };
  }
}

// ============================================================================
// ğŸ¯ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ export
// ============================================================================
export const ollamaAPI = new OllamaAPIClient();
export default ollamaAPI;