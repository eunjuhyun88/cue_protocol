// ============================================================================
// ğŸ“ src/services/api/ChatAPI.ts
// ğŸ’¬ ì±„íŒ… API í´ë¼ì´ì–¸íŠ¸ (ë³€ìˆ˜ëª… ì¶©ëŒ í•´ê²°)
// ============================================================================

import { PersistentDataAPIClient } from './PersistentDataAPIClient';

export interface ChatMessage {
  id: string;
  type: 'user' | 'ai';
  content: string;
  timestamp: string;
  model?: string;
  cueReward?: number;
  trustScore?: number;
  contextLearned?: boolean;
  qualityScore?: number;
  attachments?: File[];
  metadata?: any;
}

export interface ChatResponse {
  response: string;
  model: string;
  timestamp: string;
  cueReward?: number;
  trustScore?: number;
  contextLearned?: boolean;
  qualityScore?: number;
  processingTime?: number;
  tokensUsed?: number;
}

export class ChatAPI {
  private persistentClient: PersistentDataAPIClient;

  constructor() {
    this.persistentClient = new PersistentDataAPIClient();
  }

  // ê¸°ë³¸ ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡
  async sendMessage(message: string, model: string = 'gpt-4o', userDid?: string): Promise<ChatResponse> {
    return this.persistentClient.sendChatMessage(message, model, userDid);
  }

  // ê³ ê¸‰ ì±„íŒ… (íŒŒì¼ ì²¨ë¶€ ì§€ì›)
  async sendAdvancedMessage(
    message: string,
    options: {
      model?: string;
      userDid?: string;
      attachments?: File[];
      context?: any;
      personalizations?: string[];
    } = {}
  ): Promise<ChatResponse> {
    try {
      const {
        model = 'gpt-4o',
        userDid,
        attachments = [],
        context,
        personalizations = []
      } = options;

      // íŒŒì¼ ì²¨ë¶€ê°€ ìˆëŠ” ê²½ìš° ë¨¼ì € ì—…ë¡œë“œ
      let uploadedFiles = [];
      if (attachments.length > 0) {
        uploadedFiles = await this.uploadAttachments(attachments, userDid);
      }

      const requestData = {
        message,
        model,
        userDid,
        attachments: uploadedFiles,
        context,
        personalizations,
        timestamp: new Date().toISOString()
      };

      return await this.persistentClient.request('/api/ai/chat/advanced', {
        method: 'POST',
        body: JSON.stringify(requestData)
      });

    } catch (error) {
      console.warn('ê³ ê¸‰ ì±„íŒ… ì‹¤íŒ¨, ê¸°ë³¸ ì±„íŒ…ìœ¼ë¡œ fallback');
      return this.sendMessage(message, options.model, options.userDid);
    }
  }

  // ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ì‹¤ì‹œê°„ ì‘ë‹µ)
  async sendStreamingMessage(
    message: string,
    options: {
      model?: string;
      userDid?: string;
      onChunk?: (chunk: string) => void;
      onComplete?: (response: ChatResponse) => void;
    } = {}
  ): Promise<ChatResponse> {
    try {
      const { model = 'gpt-4o', userDid, onChunk, onComplete } = options;

      // WebSocket ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
      if (this.persistentClient.websocket?.readyState === WebSocket.OPEN) {
        return this.handleWebSocketStreaming(message, { model, userDid, onChunk, onComplete });
      }

      // HTTP ìŠ¤íŠ¸ë¦¬ë° fallback
      return this.handleHttpStreaming(message, { model, userDid, onChunk, onComplete });

    } catch (error) {
      console.warn('ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨, ê¸°ë³¸ ì±„íŒ…ìœ¼ë¡œ fallback');
      return this.sendMessage(message, options.model, options.userDid);
    }
  }

  // WebSocket ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
  private async handleWebSocketStreaming(
    message: string,
    options: any
  ): Promise<ChatResponse> {
    return new Promise((resolve, reject) => {
      const { model, userDid, onChunk, onComplete } = options;
      let fullResponse = '';
      let startTime = Date.now();

      const messageId = `stream_${Date.now()}`;
      
      // ë©”ì‹œì§€ ì „ì†¡
      this.persistentClient.websocket?.send(JSON.stringify({
        type: 'chat_stream',
        messageId,
        message,
        model,
        userDid
      }));

      // ì‘ë‹µ ë¦¬ìŠ¤ë„ˆ
      const listener = (data: any) => {
        if (data.messageId === messageId) {
          if (data.type === 'chunk') {
            fullResponse += data.content;
            onChunk?.(data.content);
          } else if (data.type === 'complete') {
            const chatResponse: ChatResponse = {
              response: fullResponse,
              model: data.model || model,
              timestamp: new Date().toISOString(),
              cueReward: data.cueReward,
              trustScore: data.trustScore,
              processingTime: Date.now() - startTime
            };
            
            onComplete?.(chatResponse);
            resolve(chatResponse);
          } else if (data.type === 'error') {
            reject(new Error(data.error));
          }
        }
      };

      // ë¦¬ìŠ¤ë„ˆ ë“±ë¡
      const unsubscribe = this.persistentClient.onRealtimeUpdate(listener);

      // íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
      setTimeout(() => {
        unsubscribe();
        if (fullResponse) {
          resolve({
            response: fullResponse,
            model,
            timestamp: new Date().toISOString(),
            processingTime: Date.now() - startTime
          });
        } else {
          reject(new Error('WebSocket streaming timeout'));
        }
      }, 30000);
    });
  }

  // HTTP ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë³€ìˆ˜ëª… ì¶©ëŒ í•´ê²°)
  private async handleHttpStreaming(
    message: string,
    options: any
  ): Promise<ChatResponse> {
    const { model, userDid, onChunk, onComplete } = options;
    
    try {
      const httpResponse = await fetch(`${this.persistentClient.baseURL}/api/ai/chat/stream`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${localStorage.getItem('cue_session_token')}`
        },
        body: JSON.stringify({ message, model, userDid })
      });

      if (!httpResponse.body) {
        throw new Error('ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—†ìŒ');
      }

      const reader = httpResponse.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      let startTime = Date.now();

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') {
              const finalResponse: ChatResponse = {
                response: fullResponse,
                model,
                timestamp: new Date().toISOString(),
                processingTime: Date.now() - startTime
              };
              onComplete?.(finalResponse);
              return finalResponse;
            }
            
            try {
              const parsed = JSON.parse(data);
              if (parsed.content) {
                fullResponse += parsed.content;
                onChunk?.(parsed.content);
              }
            } catch (e) {
              console.warn('ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹± ì˜¤ë¥˜:', e);
            }
          }
        }
      }

      const streamResponse: ChatResponse = {
        response: fullResponse,
        model,
        timestamp: new Date().toISOString(),
        processingTime: Date.now() - startTime
      };
      
      return streamResponse;

    } catch (error) {
      throw new Error(`HTTP ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: ${error}`);
    }
  }

  // íŒŒì¼ ì²¨ë¶€ ì—…ë¡œë“œ
  private async uploadAttachments(files: File[], userDid?: string): Promise<any[]> {
    try {
      const uploads = files.map(async (file) => {
        const formData = new FormData();
        formData.append('file', file);
        if (userDid) formData.append('userDid', userDid);

        const uploadResponse = await this.persistentClient.request('/api/files/upload', {
          method: 'POST',
          body: formData,
          headers: {} // FormDataëŠ” Content-Type ìë™ ì„¤ì •
        });

        return {
          name: file.name,
          type: file.type,
          size: file.size,
          url: uploadResponse.url,
          id: uploadResponse.id
        };
      });

      return await Promise.all(uploads);
    } catch (error) {
      console.warn('íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨:', error);
      return files.map(file => ({
        name: file.name,
        type: file.type,
        size: file.size,
        error: 'Upload failed'
      }));
    }
  }

  // ì±„íŒ… ê¸°ë¡ ì¡°íšŒ
  async getHistory(userDid: string, limit: number = 50, offset: number = 0): Promise<ChatMessage[]> {
    try {
      const historyResponse = await this.persistentClient.request(
        `/api/chat/history/${userDid}?limit=${limit}&offset=${offset}`
      );
      return historyResponse.messages || [];
    } catch (error) {
      // Mock ì±„íŒ… ê¸°ë¡
      return this.generateMockHistory(limit);
    }
  }

  // ì±„íŒ… ì €ì¥
  async saveMessage(userDid: string, message: ChatMessage): Promise<boolean> {
    try {
      const saveResponse = await this.persistentClient.request('/api/chat/save', {
        method: 'POST',
        body: JSON.stringify({
          userDid,
          ...message
        })
      });
      return saveResponse.success;
    } catch (error) {
      console.warn('ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  // ì±„íŒ… ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
  async analyzeContext(userDid: string, recentMessages: ChatMessage[]): Promise<any> {
    try {
      return await this.persistentClient.request('/api/chat/analyze', {
        method: 'POST',
        body: JSON.stringify({
          userDid,
          messages: recentMessages
        })
      });
    } catch (error) {
      return {
        topics: ['AI', 'Protocol', 'Web3'],
        sentiment: 'positive',
        complexity: 'medium',
        personalizations: ['ê¸°ìˆ ì  ê´€ì‹¬', 'í˜ì‹ ì  ì‚¬ê³ '],
        fallback: true
      };
    }
  }

  // ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
  async getAvailableModels(): Promise<any[]> {
    try {
      const modelsResponse = await this.persistentClient.request('/api/ai/models');
      return modelsResponse.models || [];
    } catch (error) {
      return [
        { id: 'personalized-agent', name: 'Personal Agent', description: 'AI Passport ê¸°ë°˜' },
        { id: 'llama3.2:3b', name: 'ğŸ¦™ Llama 3.2 (3B)', description: 'ë¡œì»¬ ê³ ì†' },
        { id: 'llama3.2:1b', name: 'ğŸ¦™ Llama 3.2 (1B)', description: 'ë¡œì»¬ ì´ˆê³ ì†' },
        { id: 'qwen2.5:3b', name: 'ğŸ‡°ğŸ‡· Qwen 2.5 (3B)', description: 'í•œêµ­ì–´ ìš°ìˆ˜' },
        { id: 'gemma2:2b', name: 'ğŸ¤– Gemma 2 (2B)', description: 'Google ë¡œì»¬' },
        { id: 'gpt-4o', name: 'â˜ï¸ GPT-4o', description: 'OpenAI í´ë¼ìš°ë“œ' },
        { id: 'claude-3.5-sonnet', name: 'â˜ï¸ Claude 3.5', description: 'Anthropic í´ë¼ìš°ë“œ' },
        { id: 'gemini-pro', name: 'â˜ï¸ Gemini Pro', description: 'Google í´ë¼ìš°ë“œ' }
      ];
    }
  }

  // Mock ì±„íŒ… ê¸°ë¡ ìƒì„±
  private generateMockHistory(limit: number): ChatMessage[] {
    const messages: ChatMessage[] = [];
    const sampleMessages = [
      { type: 'user', content: 'CUE Protocolì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜' },
      { type: 'ai', content: 'CUE Protocolì€ AI ê°œì¸í™”ë¥¼ ìœ„í•œ í˜ì‹ ì ì¸ ë¸”ë¡ì²´ì¸ í”Œë«í¼ì…ë‹ˆë‹¤...', cueReward: 5 },
      { type: 'user', content: 'RAG-DAGê°€ ë­ì•¼?' },
      { type: 'ai', content: 'RAG-DAGëŠ” Retrieval-Augmented Generation with Directed Acyclic Graphì˜ ì¤„ì„ë§ë¡œ...', cueReward: 8 }
    ];

    for (let i = 0; i < Math.min(limit, sampleMessages.length * 2); i++) {
      const sample = sampleMessages[i % sampleMessages.length];
      messages.push({
        id: `mock_${i}`,
        type: sample.type as 'user' | 'ai',
        content: sample.content,
        timestamp: new Date(Date.now() - (limit - i) * 60000).toISOString(),
        cueReward: sample.cueReward,
        trustScore: sample.type === 'ai' ? 0.85 + Math.random() * 0.15 : undefined
      });
    }

    return messages;
  }
}

export default ChatAPI;